{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/Cartpole-Deep-Q-Learning"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "PACKAGE_PARENT = '../../../../'\n",
    "import sys\n",
    "sys.path.append(PACKAGE_PARENT)\n",
    "\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "env_id = 'CartPole-v1'\n",
    "env_id = 'CartPole-v0'\n",
    "env = gym.make(env_id)\n",
    "n_actions = env.action_space.n\n",
    "len_obs_space = env.observation_space.shape[0]\n",
    "\n",
    "print('n_actions = {}'.format(n_actions))\n",
    "print('len_obs_space = {}'.format(len_obs_space))\n",
    "# torch.manual_seed(1423)\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_actions = 2\n",
      "len_obs_space = 4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from dqn import DQNv2 as DQN\n",
    "from dqn_agent2 import DQNAgent"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "learning_rate = 0.001\n",
    "EPISODES = 10000\n",
    "MINI_BATCH_SIZE = 64\n",
    "MEMORY_SIZE = 10000\n",
    "# EPISODES = 100\n",
    "rewards = []\n",
    "loss_list = []\n",
    "epsilon_list = []\n",
    "hidden_layer_size = 16\n",
    "# hidden_layer_size = 120\n",
    "layer_sizes = [len_obs_space, hidden_layer_size, n_actions]\n",
    "q_approx = DQN(lr=learning_rate, layer_sizes=layer_sizes, device=device).to(device)\n",
    "\n",
    "agent = DQNAgent(q_approx, env, epsilon, gamma, MINI_BATCH_SIZE,\n",
    "              MEMORY_SIZE, learning_rate)\n",
    "\n",
    "# reset replay memory\n",
    "\n",
    "avg_scores = agent.train(EPISODES=EPISODES, is_progress=True)\n",
    "# total_rewards = np.sum(rewards[0, 0,:])\n",
    "rewards = np.array(agent.rewards)\n",
    "print(rewards.shape)\n",
    "# print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DQNv2:\n\tMissing key(s) in state_dict: \"seq.1.weight\", \"seq.3.weight\", \"seq.4.weight\", \"seq.4.bias\", \"seq.5.weight\", \"seq.6.weight\", \"seq.6.bias\". \n\tsize mismatch for seq.2.weight: copying a param with shape torch.Size([2, 120]) from checkpoint, the shape in current model is torch.Size([120, 120]).\n\tsize mismatch for seq.2.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([120]).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_767302/3992265981.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# reset replay memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mavg_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# total_rewards = np.sum(rewards[0, 0,:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/skrip/other/cartpole/algs/qlearn/dqn_agent2.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, EPISODES, is_progress, threshold, running_avg_len)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTARGET_UPDATE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;31m# self.update_q_target()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q_target_no_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/skrip/other/cartpole/algs/qlearn/dqn_agent2.py\u001b[0m in \u001b[0;36mupdate_q_target_no_eval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         '''\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;31m# self.q_target.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/skrip/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DQNv2:\n\tMissing key(s) in state_dict: \"seq.1.weight\", \"seq.3.weight\", \"seq.4.weight\", \"seq.4.bias\", \"seq.5.weight\", \"seq.6.weight\", \"seq.6.bias\". \n\tsize mismatch for seq.2.weight: copying a param with shape torch.Size([2, 120]) from checkpoint, the shape in current model is torch.Size([120, 120]).\n\tsize mismatch for seq.2.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([120])."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('length of scores: ', len(rewards), ', len of avg_scores: ', len(avg_scores))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(rewards)+1), rewards, label=\"Rewards\")\n",
    "plt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg on 100 episodes\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1)) \n",
    "plt.ylabel('Total Reward')\n",
    "plt.xlabel('Episodes #')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(agent.epsilon_list)\n",
    "plt.ylabel('Epsilon')\n",
    "plt.xlabel('Episodes #')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BASE_PATH = \"./dqnv2_models/\"\n",
    "# save model\n",
    "model_info = str(learning_rate)+ '_' + str(gamma)\n",
    "path = BASE_PATH + 'model' + '_' + model_info + '.pt'\n",
    "q_name = 'q_approx_state_dict_' + model_info\n",
    "\n",
    "agent.save_q_model(path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "# load model\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "len_obs_space = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "layer_sizes = [len_obs_space, 16, n_actions]\n",
    "epsilon = 0\n",
    "q_approx = DQN(lr=learning_rate, layer_sizes=layer_sizes, device=device).to(device)\n",
    "agent = DQNAgent(q_approx, env, epsilon, gamma, MINI_BATCH_SIZE,\n",
    "              MEMORY_SIZE, learning_rate)\n",
    "\n",
    "PATH = './dqnv2_models/model_0.001_0.99.pt'\n",
    "agent.load_q_model(PATH)\n",
    "\n",
    "reward_arr = []\n",
    "for i in tqdm(range(100)):\n",
    "    obs, done, rew = env.reset(), False, 0\n",
    "    while not done:\n",
    "        A = agent.get_action(obs)\n",
    "        obs, reward, done, info = env.step(A.item())\n",
    "        rew += reward\n",
    "        # sleep(0.01)\n",
    "        # env.render()\n",
    "\n",
    "    reward_arr.append(rew)\n",
    "env.close()\n",
    "print(\"average reward per episode :\", sum(reward_arr) / len(reward_arr))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rewards = np.array(reward_arr)\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('reward')\n",
    "print('average reward per episode= {}'.format(np.mean(rewards)))\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "f41a9e18d32d699c7ebd9346171aa8606b8eaf6d2e7d29caa03f22c5e982b824"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}