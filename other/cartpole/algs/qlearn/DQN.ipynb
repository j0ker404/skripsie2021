{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "import gym\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import pickle\n",
                "from collections import deque"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "env_id = 'CartPole-v1'\n",
                "env = gym.make(env_id)\n",
                "n_actions = env.action_space.n\n",
                "len_obs_space = env.observation_space.shape[0]\n",
                "\n",
                "print('n_actions = {}'.format(n_actions))\n",
                "print('len_obs_space = {}'.format(len_obs_space))\n",
                "\n",
                "# if gpu is to be used\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288#:~:text=Cartpole%20%2D%20known%20also%20as%20an,forces%20to%20a%20pivot%20point.\n",
                "\n",
                "\n",
                "https://github.com/gsurma/cartpole"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "class DQN(nn.Module):\n",
                "\n",
                "    def __init__(self, lr, device) -> None:\n",
                "        super().__init__()\n",
                "        hidden_layer_size = 64\n",
                "        self.flatten = nn.Flatten()\n",
                "        # self.seq_relu = nn.Sequential(\n",
                "        #     nn.Linear(len_obs_space, hidden_layer_size),\n",
                "        #     nn.ReLU(),\n",
                "        #     nn.Linear(hidden_layer_size, hidden_layer_size),\n",
                "        #     nn.ReLU(),\n",
                "        #     nn.Linear(hidden_layer_size, n_actions)\n",
                "        # )\n",
                "        self.seq_relu = nn.Sequential(\n",
                "            nn.Linear(len_obs_space, hidden_layer_size),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(hidden_layer_size, n_actions),\n",
                "        )\n",
                "        self.learning_rate = lr\n",
                "        # self.optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
                "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
                "        self.loss = nn.MSELoss()\n",
                "        self.device = device\n",
                "\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.flatten(x)\n",
                "        logits = self.seq_relu(x)\n",
                "        return logits\n",
                "\n",
                "# learning_rate = 0.05\n",
                "# q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "# q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "# # load same weights as approx\n",
                "# q_target.load_state_dict(q_approx.state_dict())\n",
                "# q_target.eval()\n",
                "# print(q_approx)\n",
                "# print(q_target)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import random\n",
                "\n",
                "class Agent:\n",
                "    \n",
                "    def __init__(self, q_model, q_target, env, epsilon, gamma, batch_size) -> None:\n",
                "        self.q_model = q_model\n",
                "        self.q_target = q_target\n",
                "        self.env = env\n",
                "        self.n_actions = self.env.action_space.n\n",
                "        self.epsilon = epsilon\n",
                "        self.device = self.q_model.device\n",
                "        self.gamma = gamma\n",
                "        self.steps_done = 0\n",
                "        self.train_data = []\n",
                "        self.BATCH_SIZE = batch_size\n",
                "\n",
                "        # self.target_update = 5\n",
                "        # self.target_update_counter = 0\n",
                "        # self.index = 0\n",
                "\n",
                "    def train_episode(self):\n",
                "        done = False\n",
                "        obs = self.env.reset()\n",
                "        obs = torch.tensor([obs], device=self.device, dtype=torch.float64)\n",
                "        rewards = []\n",
                "\n",
                "        while not done:\n",
                "            # get action to execute based on state\n",
                "            action = self.get_action(obs.float())\n",
                "            #  take action, go to next time step\n",
                "            obs_next, reward, done, info = self.env.step(action.item())\n",
                "            rewards.append(reward)\n",
                "            # convert to tensors\n",
                "            obs_next = torch.tensor([obs_next], device=self.device, dtype=torch.float64)\n",
                "            reward = torch.tensor([reward,], device=self.device).reshape((-1,1))\n",
                "            # print(reward.shape)\n",
                "            done = torch.tensor(done, device=self.device, dtype=torch.bool)\n",
                "            \n",
                "            # create transitions\n",
                "            transition = (obs, action, reward, obs_next, done)\n",
                "            # store transitions\n",
                "            D.append(transition)\n",
                "            # self.index += 1\n",
                "\n",
                "            if len(D) >= self.BATCH_SIZE:\n",
                "                # self.index = 0\n",
                "                # for _ in range(4):\n",
                "                # update target\n",
                "                # if self.target_update_counter == self.target_update:\n",
                "                #     self.q_target.load_state_dict(self.q_model.state_dict())\n",
                "                #     self.q_target.eval()\n",
                "                #     self.target_update_counter = 0                        \n",
                "\n",
                "                # sample minibatch\n",
                "                mini_batch = random.choices(D, k=self.BATCH_SIZE)\n",
                "\n",
                "                obs_batch = tuple([tple[0] for tple in mini_batch])\n",
                "                obs_batch = torch.cat(obs_batch, dim=0)\n",
                "                # print('obs_batch = \\n{}'.format(obs_batch.shape))\n",
                "\n",
                "                action_batch = tuple([tple[1] for tple in mini_batch])\n",
                "                action_batch = torch.cat(action_batch, dim=0).reshape(-1,1)\n",
                "                # print('action_batch = \\n{}'.format(action_batch.shape))\n",
                "\n",
                "                reward_batch = torch.tensor([tple[2] for tple in mini_batch])\n",
                "                reward_batch = reward_batch.reshape((-1,1))\n",
                "                # print('reward_batch = \\n{}'.format(reward_batch.shape))\n",
                "\n",
                "                obs_next_batch = tuple([tple[3] for tple in mini_batch])\n",
                "                obs_next_batch = torch.cat(obs_next_batch, dim=0)\n",
                "                # print('obs_next_batch = \\n{}'.format(obs_next_batch.shape))\n",
                "            \n",
                "                done_batch = torch.tensor([tple[4] for tple in mini_batch])\n",
                "                done_batch = reward_batch.reshape((-1,1))\n",
                "                # print('done_batch = \\n{}'.format(done_batch.shape))\n",
                "                \n",
                "\n",
                "                # Compute prediction and loss\n",
                "                q_values = self.q_model(obs_batch.float()).gather(1, action_batch)\n",
                "                # print('q_values = {}'.format(q_values.shape))\n",
                "                target_q_values   = self.q_target(obs_batch.float())\n",
                "                # print('target_q_values = {}'.format(target_q_values.shape))\n",
                "                y = reward_batch + self.gamma*torch.max(target_q_values)*(1-done_batch)\n",
                "                loss = self.q_model.loss(q_values, y)\n",
                "                \n",
                "                # Backpropagation\n",
                "                self.q_model.optimizer.zero_grad()\n",
                "                loss.backward()\n",
                "                self.q_model.optimizer.step()\n",
                "\n",
                "                # self.target_update_counter += 1\n",
                "\n",
                "        rewards = np.array(rewards)\n",
                "        self.train_data.append(np.sum(rewards))\n",
                "        \n",
                "    def get_action(self, obs):\n",
                "        sample = random.random()\n",
                "        action = None\n",
                "        if sample < self.epsilon:\n",
                "            action = torch.tensor([random.choice(range(self.n_actions))])\n",
                "        else:\n",
                "            with torch.no_grad():\n",
                "                q_vals = self.q_model(obs)\n",
                "                # print(q_vals)\n",
                "                action = torch.argmax(q_vals)\n",
                "        return torch.tensor([action], device=self.device)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# -------------------------------\n",
                "# hyperparameters\n",
                "TARGET_UPDATE = 10\n",
                "\n",
                "EPSILON_MIN = 0.05\n",
                "EPSILON_START = 1\n",
                "EPS_DECAY = 0.001\n",
                "\n",
                "epsilon = EPSILON_START\n",
                "gamma = 0.999\n",
                "EPISODES = 10000\n",
                "\n",
                "MINI_BATCH_SIZE = 16\n",
                "MEMORY_SIZE = 256\n",
                "# -------------------------------"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "We need to tune the hyperparams\n",
                "- epsilon (epsilong_min)\n",
                "- gamma (discount rate)\n",
                "- learning rate\n",
                "- target update\n",
                "- mini batch size"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# create range for hyperparams\n",
                "learning_rates = np.arange(0.001, 0.002, 0.0005)\n",
                "gammas = np.arange(0.98, 1.001, 0.001)\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "rewards = np.zeros((len(learning_rates), len(gammas), EPISODES))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "BASE_PATH = \"./dqn_models/\"\n",
                "for index_lr, learning_rate in enumerate(learning_rates):\n",
                "    print('-'*5)\n",
                "    print('lr = {}'.format(learning_rate))\n",
                "    for index_gamma, gamma in enumerate(gammas):\n",
                "        q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "        q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "        # load same weights as approx\n",
                "        q_target.load_state_dict(q_approx.state_dict())\n",
                "        q_target.eval()\n",
                "        \n",
                "        # reset replay memory\n",
                "        D = deque(maxlen=MEMORY_SIZE)\n",
                "        agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE)\n",
                "        for episode in range(EPISODES):\n",
                "\n",
                "            # if episode % 100 == 0:\n",
                "            #     print('Episode = {}'.format(episode))\n",
                "            #     print('Epsilon = {}'.format(agent.epsilon))\n",
                "            # train one episode\n",
                "            agent.train_episode()\n",
                "\n",
                "            # update epsilon value\n",
                "            agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "\n",
                "            # update target paramters\n",
                "            if episode % TARGET_UPDATE == 0:\n",
                "                q_target.load_state_dict(q_approx.state_dict())\n",
                "                q_target.eval()\n",
                "        \n",
                "        # shallow copy\n",
                "        # rewards_per_episode = agent.train_data.copy()\n",
                "        # alias\n",
                "        rewards_per_episode = agent.train_data\n",
                "        rewards[index_lr, index_gamma] = rewards_per_episode \n",
                "\n",
                "        total_rewards = np.sum(rewards[index_lr, index_gamma,:])\n",
                "        print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\n",
                "        average_reward = total_rewards/EPISODES\n",
                "\n",
                "        # save models\n",
                "        model_info = str(learning_rate)+ '_' + str(gamma)\n",
                "        path = BASE_PATH + 'model' + '_' + model_info + '.pt'\n",
                "        q_name = 'q_approx_state_dict_' + model_info\n",
                "        optim_name = 'optim_state_dict_' + model_info\n",
                "        model = {\n",
                "            q_name: agent.q_model.state_dict(),\n",
                "            optim_name: agent.q_model.optimizer.state_dict()\n",
                "        }\n",
                "        torch.save(model, path)\n",
                "    print('-'*5)\n",
                "# save rewards to disk\n",
                "with open(\"./dqn_rewards/dqn_rewards.pkl\",'wb') as f:\n",
                "    pickle.dump(rewards, f)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# plot average reward data\n",
                "# print(rewards[:,:,:].shape)\n",
                "import pandas as pd\n",
                "\n",
                "d = pd.Series(rewards[0,0,:])\n",
                "plt.plot(np.arange(EPISODES),d.rolling(100).mean())\n",
                "plt.xlabel('episodes')\n",
                "plt.ylabel('average reward')\n",
                "# plt.plot(rewards[:,:])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# plt.plot(agent.train_data)\n",
                "# plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# for param in q_approx.parameters():\n",
                "#     print(param.size())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# RENDER = False\n",
                "# total_rewards = []\n",
                "# for episode in range(EPISODES):\n",
                "#     done = False\n",
                "#     obs = agent.env.reset()\n",
                "#     obs = torch.tensor([obs], device=agent.device, dtype=torch.float64)\n",
                "#     rewards = []\n",
                "#     while not done:\n",
                "#         # get action to execute based on state\n",
                "#         action = agent.get_action(obs.float())\n",
                "#         #  take action, go to next time step\n",
                "#         obs_next, reward, done, info = agent.env.step(action.item())\n",
                "#         obs_next = torch.tensor([obs_next], device=agent.device, dtype=torch.float64)\n",
                "#         obs = obs_next\n",
                "#         rewards.append(reward)\n",
                "#         if RENDER:\n",
                "#             env.render()\n",
                "\n",
                "#     rewards = np.array(rewards)\n",
                "#     total_rewards.append(np.sum(rewards))\n",
                "\n",
                "# plt.plot(total_rewards)\n",
                "# plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "'''\n",
                "\n",
                "learning_rate = 0.0001\n",
                "EPSILON_MIN = 0.05\n",
                "EPS_DECAY = 0.001\n",
                "gamma= 0.99\n",
                "TARGET_UPDATE = 10\n",
                "EPISODES = 10000\n",
                "\n",
                "MEMORY_SIZE = 256\n",
                "D = deque(maxlen=MEMORY_SIZE)\n",
                "MINI_BATCH_SIZE = 16\n",
                "epsilon=1\n",
                "\n",
                "\n",
                "\n",
                "q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "# load same weights as approx\n",
                "q_target.load_state_dict(q_approx.state_dict())\n",
                "q_target.eval()\n",
                "# reset replay memory\n",
                "D = deque(maxlen=MEMORY_SIZE)\n",
                "agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE)\n",
                "for episode in range(EPISODES):\n",
                "\n",
                "    if episode % 1000 == 0:\n",
                "        print('-'*5)\n",
                "        print('Episode = {}'.format(episode))\n",
                "        print('Epsilon = {}'.format(agent.epsilon))\n",
                "        print('-'*5)\n",
                "    # train one episode\n",
                "    agent.train_episode()\n",
                "\n",
                "    # update epsilon value\n",
                "    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "    if agent.epsilon > 0.05 :\n",
                "        agent.epsilon -= (1 / 5000)\n",
                "\n",
                "    # # update target paramters\n",
                "    if episode % TARGET_UPDATE == 0:\n",
                "        q_target.load_state_dict(q_approx.state_dict())\n",
                "        q_target.eval()\n",
                "\n",
                "# shallow copy\n",
                "# rewards_per_episode = agent.train_data.copy()\n",
                "# alias\n",
                "rewards_per_episode = agent.train_data\n",
                "\n",
                "total_rewards = np.sum(rewards_per_episode)\n",
                "print(\"Score over time: {}, gamma={}\".format( total_rewards/EPISODES, gamma))\n",
                "average_reward = total_rewards/EPISODES\n",
                "\n",
                "plt.plot(rewards_per_episode)\n",
                "\n",
                "\n",
                "'''"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "f41a9e18d32d699c7ebd9346171aa8606b8eaf6d2e7d29caa03f22c5e982b824"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}