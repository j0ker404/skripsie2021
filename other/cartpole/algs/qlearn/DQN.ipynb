{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "import gym\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import pickle\n",
                "from collections import deque"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "env_id = 'CartPole-v1'\n",
                "env_id = 'CartPole-v0'\n",
                "env = gym.make(env_id)\n",
                "n_actions = env.action_space.n\n",
                "len_obs_space = env.observation_space.shape[0]\n",
                "\n",
                "print('n_actions = {}'.format(n_actions))\n",
                "print('len_obs_space = {}'.format(len_obs_space))\n",
                "# torch.manual_seed(1423)\n",
                "# if gpu is to be used\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "n_actions = 2\n",
                        "len_obs_space = 4\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288#:~:text=Cartpole%20%2D%20known%20also%20as%20an,forces%20to%20a%20pivot%20point.\n",
                "\n",
                "\n",
                "https://github.com/gsurma/cartpole"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "class DQN(nn.Module):\n",
                "\n",
                "    def __init__(self, lr, device) -> None:\n",
                "        super().__init__()\n",
                "        torch.manual_seed(1423)\n",
                "        hidden_layer_size = 64\n",
                "        # self.flatten = nn.Flatten()\n",
                "        # self.seq_relu = nn.Sequential(\n",
                "        #     nn.Linear(len_obs_space, hidden_layer_size),\n",
                "        #     nn.ReLU(),\n",
                "        #     nn.Linear(hidden_layer_size, hidden_layer_size),\n",
                "        #     nn.ReLU(),\n",
                "        #     nn.Linear(hidden_layer_size, n_actions)\n",
                "        # )\n",
                "        # self.seq_relu = nn.Sequential(\n",
                "        #     nn.Linear(len_obs_space, hidden_layer_size),\n",
                "        #     nn.ReLU(),\n",
                "        #     nn.Linear(hidden_layer_size, n_actions),\n",
                "        # # )\n",
                "        self.seq = nn.Sequential(\n",
                "            nn.Linear(len_obs_space, hidden_layer_size),\n",
                "            nn.Tanh(),\n",
                "            nn.Linear(hidden_layer_size, n_actions),\n",
                "            nn.Identity()\n",
                "        )\n",
                "        # nn.Linear(len_obs_space, hidden_layer_size)\n",
                "        # nn.Tanh()\n",
                "        # nn.Linear(hidden_layer_size, n_actions)\n",
                "        # nn.Identity()\n",
                "\n",
                "        self.learning_rate = lr\n",
                "        self.device = device\n",
                "\n",
                "        # self.optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
                "        # self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
                "        # # self.optimizer = torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\n",
                "        # self.loss = nn.MSELoss()\n",
                "\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x = self.flatten(x)\n",
                "        logits = self.seq(x)\n",
                "        return logits\n",
                "\n",
                "\n",
                "# learning_rate = 0.05\n",
                "# q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "# q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "# # load same weights as approx\n",
                "# q_target.load_state_dict(q_approx.state_dict())\n",
                "# q_target.eval()\n",
                "# print(q_approx)\n",
                "# print(q_target)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "import random\n",
                "import copy\n",
                "import time\n",
                "class Agent:\n",
                "    '''\n",
                "        https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435\n",
                "    '''    \n",
                "    def __init__(self, q_model, env, epsilon, gamma, batch_size, exp_mem_size, lr) -> None:\n",
                "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "        torch.manual_seed(1423)\n",
                "        # torch.manual_seed(int(time.time()))\n",
                "        # torch.seed()\n",
                "        input_dim = env.observation_space.shape[0]\n",
                "        output_dim = env.action_space.n\n",
                "\n",
                "        # self.my_model = q_model\n",
                "        # self.q_model = self.build_nn([input_dim, 64, output_dim])\n",
                "\n",
                "        # self.my_model.load_state_dict(self.q_model.state_dict())\n",
                "\n",
                "        # print('My model params')        \n",
                "        # for index in range(len(self.my_model.state_dict())):\n",
                "        #     # print(param)\n",
                "        #     self.my_model.parameters()[index] =   self.q_model.parameters()[index]\n",
                "\n",
                "        # self.my_model.state_dict()['seq.0.weight'] = self.q_model.state_dict()['0.weight']\n",
                "        # print('Their model params')        \n",
                "        # print(self.q_model.state_dict()['0.weight'])\n",
                "        # for param in self.q_model.parameters():\n",
                "        #     print()\n",
                "\n",
                "        # print('My model params')        \n",
                "        # print(self.my_model.state_dict()['seq.0.weight'])\n",
                "        # for param in self.my_model.parameters():\n",
                "        #     print(param)        \n",
                "\n",
                "        # print('my_model = {}'.format(self.my_model))\n",
                "        # print('their model = {}'.format(self.q_model))\n",
                "\n",
                "        # load same weights as approx\n",
                "        # self.q_target = copy.deepcopy(self.q_model)\n",
                "        # self.q_target = self.build_nn([input_dim, 64, output_dim])\n",
                "\n",
                "        self.q_model = q_model\n",
                "        self.q_target = DQN(learning_rate, device=device)\n",
                "        self.q_target.load_state_dict(self.q_model.state_dict())\n",
                "        self.q_target.eval()\n",
                "        \n",
                "        self.env = env\n",
                "        self.n_actions = self.env.action_space.n\n",
                "        self.epsilon = epsilon\n",
                "        # self.device = self.q_model.device\n",
                "        # self.gamma = gamma\n",
                "        self.gamma = torch.tensor(gamma).float()\n",
                "        self.steps_done = 0\n",
                "        self.train_data = []\n",
                "\n",
                "        self.target_update = 5\n",
                "        self.target_update_counter = 0\n",
                "        self.train_step_count = 0\n",
                "\n",
                "        self.EXP_MEMORY_SIZE = exp_mem_size\n",
                "        self.BATCH_SIZE = batch_size\n",
                "\n",
                "        self.D = deque(maxlen=self.EXP_MEMORY_SIZE)\n",
                "        # self.clear_replay_memory()\n",
                "\n",
                "        self.loss = nn.MSELoss()\n",
                "        self.optimizer = torch.optim.Adam(self.q_model.parameters(), lr=lr)\n",
                "        # self.optimizer = torch.optim.Adam(self.q_model.parameters(), lr=self.q_model.learning_rate)\n",
                "        # self.optimizer = torch.optim.SGD(self.q_model.parameters(), lr=self.q_model.learning_rate)\n",
                "\n",
                "\n",
                "    def build_nn(self, layer_sizes):\n",
                "        assert len(layer_sizes) > 1\n",
                "        layers = []\n",
                "        for index in range(len(layer_sizes) - 1):\n",
                "            linear = nn.Linear(layer_sizes[index], layer_sizes[index + 1])\n",
                "            act = nn.Tanh() if index < len(layer_sizes) - 2 else nn.Identity()\n",
                "            # act = nn.ReLU() if index < len(layer_sizes) - 2 else nn.Identity()\n",
                "            layers += (linear, act)\n",
                "        return nn.Sequential(*layers)\n",
                "\n",
                "    def init_replay_memory(self):\n",
                "        \"\"\"\n",
                "            initialise replay memory\n",
                "        \"\"\"\n",
                "        self.clear_replay_memory()\n",
                "        is_mem_filled = False\n",
                "        counter = 0\n",
                "        for _ in range(self.EXP_MEMORY_SIZE):\n",
                "            # if is_mem_filled:\n",
                "            #     break\n",
                "            done = False\n",
                "            obs = self.env.reset()\n",
                "            # obs = torch.tensor([obs], device=self.device, dtype=torch.float64)\n",
                "            while not done:\n",
                "                # get action to execute based on state\n",
                "                action = self.get_action(obs, self.n_actions, epsilon=1)\n",
                "                \n",
                "                #  take action, go to next time step\n",
                "                obs_next, reward, done, info = self.env.step(action.item())\n",
                "                self.collect_experience([obs, action.item(), reward, obs_next, done])\n",
                "\n",
                "                # create transition\n",
                "                # transition = [obs, action, reward, obs_next, done]\n",
                "                # store transition\n",
                "                # self.D.append(transition)\n",
                "\n",
                "                obs = obs_next\n",
                "                counter += 1\n",
                "                if counter > self.EXP_MEMORY_SIZE:\n",
                "                    break\n",
                "                # is_mem_filled = True if counter > self.EXP_MEMORY_SIZE else False\n",
                "                # if is_mem_filled:\n",
                "                #     break\n",
                "        \n",
                "\n",
                "    def clear_replay_memory(self):\n",
                "        \"\"\"[summary]\n",
                "            \n",
                "            clear experience reply memory\n",
                "        \"\"\" \n",
                "        self.D = deque(maxlen=self.EXP_MEMORY_SIZE)\n",
                "\n",
                "    \n",
                "    def get_target_q_vals(self, obs):\n",
                "        '''\n",
                "            Return the target q-values for given observation\n",
                "\n",
                "            obs: torch shape(n_samples, n_features)\n",
                "        '''\n",
                "        # vals = None\n",
                "        # with torch.no_grad():\n",
                "        #     vals = self.q_target(obs.float()).amax(dim=1)\n",
                "\n",
                "        # return vals\n",
                "        with torch.no_grad():\n",
                "            qp = self.q_target(obs)\n",
                "        q, _ = torch.max(qp, axis=1)\n",
                "        return q\n",
                "\n",
                "    def train_episode(self):\n",
                "        done = False\n",
                "        obs = self.env.reset()\n",
                "        obs = torch.tensor([obs], device=self.device, dtype=torch.float64)\n",
                "        rewards = []\n",
                "        # frame_count = 0\n",
                "        # target_frame_update = 5\n",
                "        while not done:\n",
                "            # get action to execute based on state\n",
                "            action = self.get_action(obs.float())\n",
                "            #  take action, go to next time step\n",
                "            obs_next, reward, done, info = self.env.step(action.item())\n",
                "            rewards.append(reward)\n",
                "            # convert to tensors\n",
                "            obs_next = torch.tensor([obs_next], device=self.device, dtype=torch.float64)\n",
                "            reward = torch.tensor([reward,], device=self.device).reshape((-1,1))\n",
                "            # print(reward.shape)\n",
                "            done = torch.tensor(done, device=self.device, dtype=torch.bool)\n",
                "            \n",
                "            # create transitions\n",
                "            # transition = (obs, action, reward, obs_next, done)\n",
                "            # store transitions\n",
                "            # self.D.append(transition)\n",
                "            self.collect_experience([obs, action.item(), reward, obs_next, done])\n",
                "\n",
                "            self.train_step_count += 1\n",
                "\n",
                "            obs = obs_next\n",
                "\n",
                "            if self.train_step_count > 128:\n",
                "                # print('gf')\n",
                "                self.train_step_count = 0\n",
                "                for _ in range(4):\n",
                "                    # update target\n",
                "                    if self.target_update_counter == self.target_update:\n",
                "                        self.q_target.load_state_dict(self.q_model.state_dict())\n",
                "                        self.q_target.eval()\n",
                "                        self.target_update_counter = 0                        \n",
                "\n",
                "                    # sample minibatch\n",
                "                    # mini_batch = random.choices(self.D, k=self.BATCH_SIZE)\n",
                "                    mini_batch = random.sample(self.D, k=self.BATCH_SIZE)\n",
                "\n",
                "                    obs_batch = tuple([tple[0] for tple in mini_batch])\n",
                "                    # print('obsss')\n",
                "                    # print(' obs_batch = {}'.format(obs_batch))\n",
                "                    obs_batch = torch.cat(obs_batch, dim=0)\n",
                "                    # print('obs_batch = {}'.format(obs_batch.shape))\n",
                "\n",
                "                    action_batch = tuple([tple[1] for tple in mini_batch])\n",
                "                    # action_batch = torch.cat(action_batch, dim=0).reshape(-1,1)\n",
                "                    action_batch = torch.cat(action_batch, dim=0)\n",
                "                    # print('action_batch = \\n{}'.format(action_batch.shape))\n",
                "\n",
                "                    reward_batch = torch.tensor([tple[2] for tple in mini_batch])\n",
                "                    # reward_batch = reward_batch.reshape((-1,1))\n",
                "                    # print('reward_batch = \\n{}'.format(reward_batch.shape))\n",
                "\n",
                "                    obs_next_batch = tuple([tple[3] for tple in mini_batch])\n",
                "                    obs_next_batch = torch.cat(obs_next_batch, dim=0)\n",
                "                    # print('obs_next_batch = \\n{}'.format(obs_next_batch.shape))\n",
                "                \n",
                "                    done_batch = torch.tensor([tple[4] for tple in mini_batch])\n",
                "                    # done_batch = reward_batch.reshape((-1,1))\n",
                "                    # print('done_batch = \\n{}'.format(done_batch.int()))\n",
                "\n",
                "                    # print(action)\n",
                "                    # Compute prediction and loss\n",
                "                    # q_values = self.q_model(obs_batch.float()).gather(1, action_batch)\n",
                "                    q_values = self.q_model(obs_batch.float())\n",
                "                    # pred = q_values[torch.arange(16), action_batch]\n",
                "                    pred, _ = torch.max(q_values, axis=1)\n",
                "                    # target_q_values  = self.q_target(obs_batch.float())\n",
                "                    target_q_values = self.get_target_q_vals(obs_batch.float())\n",
                "                    # print('target_q_values = {}'.format(target_q_values.shape))\n",
                "                    # y = reward_batch + self.gamma*torch.max(target_q_values)*(1-done_batch.int())\n",
                "                    y = reward_batch + self.gamma*torch.max(target_q_values)\n",
                "                    # loss = self.q_model.loss(q_values, y)\n",
                "                    loss = self.q_model.loss(pred, y)\n",
                "                    \n",
                "                    \n",
                "                    \n",
                "                    # Backpropagation\n",
                "                    self.q_model.optimizer.zero_grad()\n",
                "                    loss.backward(retain_graph=True)\n",
                "                    self.q_model.optimizer.step()\n",
                "                    # self.optimizer.zero_grad()\n",
                "                    # loss.backward(retain_graph=True)\n",
                "                    # self.optimizer.step()\n",
                "\n",
                "                    self.target_update_counter += 1\n",
                "                    # print('-'*5)\n",
                "                    # print('obs_batch = {}'.format(obs_batch.shape))\n",
                "                    # print('action_batch = {}'.format(action_batch.shape))\n",
                "                    # print('reward_batch = {}'.format(reward_batch.shape))\n",
                "                    # print('obs_next_batch = {}'.format(obs_next_batch.shape))\n",
                "                    # print('q_values = {}'.format(q_values.shape))\n",
                "                    # print('pred = {}'.format(pred.shape))\n",
                "                    # print('target_q_values = {}'.format(target_q_values.shape))\n",
                "                    # print('y = {}'.format(y.shape))\n",
                "                    # print('loss = {}'.format(loss))\n",
                "                    # print('-'*5)\n",
                "\n",
                "        rewards = np.array(rewards)\n",
                "        self.train_data.append(np.sum(rewards))\n",
                "    \n",
                "    def sample_experience(self, sample_size):\n",
                "        # sample_size = self.BATCH_SIZE\n",
                "        if len(self.D) < sample_size:\n",
                "            sample_size = len(self.D)\n",
                "\n",
                "        mini_batch = random.sample(self.D, sample_size)\n",
                "\n",
                "        obs_batch = torch.tensor([tple[0] for tple in mini_batch]).float()\n",
                "        action_batch = torch.tensor([tple[1] for tple in mini_batch]).float()\n",
                "        reward_batch = torch.tensor([tple[2] for tple in mini_batch]).float()\n",
                "        obs_next_batch = torch.tensor([tple[3] for tple in mini_batch]).float()\n",
                "        done_batch = torch.tensor([tple[4] for tple in mini_batch])\n",
                "\n",
                "        # print('-'*5)\n",
                "        # print('obs_batch = {}'.format(obs_batch.shape))\n",
                "        # print('action_batch = {}'.format(action_batch.shape))\n",
                "        # print('reward_batch = {}'.format(reward_batch.shape))\n",
                "        # print('obs_next_batch = {}'.format(obs_next_batch.shape))\n",
                "        # print('-'*5)\n",
                "        return obs_batch, action_batch, reward_batch, obs_next_batch, done_batch\n",
                "        \n",
                "\n",
                "    def train(self, batch_size):\n",
                "        # sample minibatch\n",
                "        obs_batch, action_batch, reward_batch, obs_next_batch, done_batch = self.sample_experience(batch_size)\n",
                "        \n",
                "        if self.target_update_counter == self.target_update:\n",
                "            self.q_target.load_state_dict(self.q_model.state_dict())\n",
                "            self.q_target.eval()\n",
                "            self.target_update_counter = 0                        \n",
                "\n",
                "        # Compute prediction and loss\n",
                "        q_values = self.q_model(obs_batch)\n",
                "        pred, _ = torch.max(q_values, axis=1)\n",
                "        target_q_values = self.get_target_q_vals(obs_next_batch)\n",
                "        # y = reward_batch + self.gamma*torch.max(target_q_values)*(1-done_batch.int())\n",
                "        # y = reward_batch + self.gamma*torch.max(target_q_values)\n",
                "        y = reward_batch + self.gamma*target_q_values\n",
                "\n",
                "        # loss = self.q_model.loss(pred, y)\n",
                "        loss = self.loss(pred, y)\n",
                "        \n",
                "        \n",
                "        # Backpropagation\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward(retain_graph=True)\n",
                "        self.optimizer.step()\n",
                "\n",
                "        self.target_update_counter += 1\n",
                "\n",
                "        # print('-'*5)\n",
                "        # print('obs_batch = {}'.format(obs_batch.shape))\n",
                "        # print('action_batch = {}'.format(action_batch.shape))\n",
                "        # print('reward_batch = {}'.format(reward_batch.shape))\n",
                "        # print('obs_next_batch = {}'.format(obs_next_batch.shape))\n",
                "        # print('q_values = {}'.format(q_values.shape))\n",
                "        # print('pred = {}'.format(pred.shape))\n",
                "        # print('target_q_values = {}'.format(target_q_values.shape))\n",
                "        # print('y = {}'.format(y.shape))\n",
                "        # print('loss = {}'.format(loss))\n",
                "        # print('-'*5)\n",
                "        return loss.item()\n",
                "\n",
                "    def get_action(self, state, action_space_len, epsilon):\n",
                "        # We do not require gradient at this point, because this function will be used either\n",
                "        # during experience collection or during inference\n",
                "        with torch.no_grad():\n",
                "            # Qp = self.q_net(torch.from_numpy(state).float().cuda())\n",
                "            state_torch = torch.from_numpy(state)\n",
                "            # print(state_torch)\n",
                "            Qp = self.q_model(state_torch.float())\n",
                "            # print(Qp)\n",
                "        Q, A = torch.max(Qp, axis=0)\n",
                "        # print('Q, A = {}, {}'.format(Q, A))\n",
                "        A = A if torch.rand(1, ).item() > epsilon else torch.randint(0, action_space_len, (1,))\n",
                "        return A\n",
                "\n",
                "    def collect_experience(self, experience):\n",
                "        self.D.append(experience)\n",
                "        \n",
                "\n",
                "    # def get_action(self, obs):\n",
                "    #     # sample = random.random()\n",
                "    #     action = None\n",
                "    #     # if sample < self.epsilon:\n",
                "    #     #     action = torch.tensor([random.choice(range(self.n_actions))])\n",
                "    #     # else:\n",
                "    #     #     with torch.no_grad():\n",
                "    #     #         # print(obs)\n",
                "    #     #         state_torch = torch.from_numpy(obs).float()\n",
                "    #     #         q_vals = self.q_model(state_torch)\n",
                "    #     #         # print(q_vals)\n",
                "    #     #         action = torch.argmax(q_vals)\n",
                "    #     with torch.no_grad():\n",
                "    #         # print(obs)\n",
                "    #         state_torch = torch.from_numpy(obs).float()\n",
                "    #         q_vals = self.q_model(state_torch)\n",
                "    #         # print(q_vals)\n",
                "    #         action = torch.argmax(q_vals)\n",
                "        \n",
                "    #     action = torch.tensor([action], device=self.device)\n",
                "    #     action = action if torch.rand(1, ).item() > epsilon else torch.randint(0, self.n_actions, (1,))\n",
                "\n",
                "    #     # print(action.item())\n",
                "    #     return action\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "# -------------------------------\n",
                "# hyperparameters\n",
                "TARGET_UPDATE = 10\n",
                "\n",
                "EPSILON_MIN = 0.05\n",
                "EPSILON_START = 1\n",
                "EPS_DECAY = 0.001\n",
                "# EPS_DECAY = 0.0099\n",
                "\n",
                "epsilon = EPSILON_START\n",
                "gamma = 0.999\n",
                "EPISODES = 10000\n",
                "\n",
                "MINI_BATCH_SIZE = 16\n",
                "MEMORY_SIZE = 256\n",
                "# MINI_BATCH_SIZE = 10\n",
                "# MEMORY_SIZE = 100\n",
                "# -------------------------------"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "We need to tune the hyperparams\n",
                "- epsilon (epsilong_min)\n",
                "- gamma (discount rate)\n",
                "- learning rate\n",
                "- target update\n",
                "- mini batch size"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# create range for hyperparams\n",
                "# learning_rates = np.arange(0.0001, 0.0002, 0.00001)\n",
                "# gammas = np.arange(0.98, 1.001, 0.001)\n",
                "gammas = np.array([0.95])\n",
                "learning_rates = np.array([1e-3])\n",
                "\n",
                "print('lr.shape = {}'.format(learning_rates.shape))\n",
                "print('gammas.shape = {}'.format(gammas.shape))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "lr.shape = (1,)\n",
                        "gammas.shape = (1,)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "rewards = np.zeros((len(learning_rates), len(gammas), EPISODES))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "print(rewards.shape)\n",
                "\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(1, 1, 10000)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "BASE_PATH = \"./dqn_models/\"\n",
                "'''\n",
                "for index_lr, learning_rate in enumerate(learning_rates):\n",
                "    print('-'*5)\n",
                "    print('lr = {}'.format(learning_rate))\n",
                "    for index_gamma, gamma in enumerate(gammas):\n",
                "        q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "        q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "        # load same weights as approx\n",
                "        q_target.load_state_dict(q_approx.state_dict())\n",
                "        q_target.eval()\n",
                "\n",
                "        # print(q_approx)\n",
                "        # print(q_target)\n",
                "        # print(index_gamma)\n",
                "        \n",
                "        agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE, MEMORY_SIZE)\n",
                "\n",
                "        # reset replay memory\n",
                "        agent.init_replay_memory()\n",
                "\n",
                "        for episode in range(EPISODES):\n",
                "\n",
                "            # if episode % 100 == 0:\n",
                "            #     print('Episode = {}'.format(episode))\n",
                "            #     print('Epsilon = {}'.format(agent.epsilon))\n",
                "            # train one episode\n",
                "            agent.train_episode()\n",
                "\n",
                "            # update epsilon value\n",
                "            # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "            if agent.epsilon > 0.05:\n",
                "                agent.epsilon -= (1 / 5000)\n",
                "\n",
                "            # update target paramters\n",
                "            # if episode % TARGET_UPDATE == 0:\n",
                "            #     q_target.load_state_dict(q_approx.state_dict())\n",
                "            #     q_target.eval()\n",
                "        \n",
                "        # shallow copy\n",
                "        # rewards_per_episode = agent.train_data.copy()\n",
                "        # alias\n",
                "        rewards_per_episode = agent.train_data\n",
                "        rewards[index_lr, index_gamma] = rewards_per_episode \n",
                "\n",
                "        total_rewards = np.sum(rewards[index_lr, index_gamma,:])\n",
                "        print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\n",
                "        average_reward = total_rewards/EPISODES\n",
                "\n",
                "        # save models\n",
                "        model_info = str(learning_rate)+ '_' + str(gamma)\n",
                "        path = BASE_PATH + 'model' + '_' + model_info + '.pt'\n",
                "        q_name = 'q_approx_state_dict_' + model_info\n",
                "        optim_name = 'optim_state_dict_' + model_info\n",
                "        model = {\n",
                "            q_name: agent.q_model.state_dict(),\n",
                "            optim_name: agent.q_model.optimizer.state_dict()\n",
                "        }\n",
                "        # model = {\n",
                "        #     q_name: agent.q_model.state_dict(),\n",
                "        #     optim_name: agent.optimizer.state_dict()\n",
                "        # }\n",
                "        torch.save(model, path)\n",
                "    print('-'*5)\n",
                "# save rewards to disk\n",
                "with open(\"./dqn_rewards/dqn_rewards.pkl\",'wb') as f:\n",
                "    pickle.dump(rewards, f)\n",
                "'''"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'\\nfor index_lr, learning_rate in enumerate(learning_rates):\\n    print(\\'-\\'*5)\\n    print(\\'lr = {}\\'.format(learning_rate))\\n    for index_gamma, gamma in enumerate(gammas):\\n        q_approx = DQN(lr=learning_rate, device=device).to(device)\\n        q_target = DQN(lr=learning_rate, device=device).to(device)\\n        # load same weights as approx\\n        q_target.load_state_dict(q_approx.state_dict())\\n        q_target.eval()\\n\\n        # print(q_approx)\\n        # print(q_target)\\n        # print(index_gamma)\\n        \\n        agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE, MEMORY_SIZE)\\n\\n        # reset replay memory\\n        agent.init_replay_memory()\\n\\n        for episode in range(EPISODES):\\n\\n            # if episode % 100 == 0:\\n            #     print(\\'Episode = {}\\'.format(episode))\\n            #     print(\\'Epsilon = {}\\'.format(agent.epsilon))\\n            # train one episode\\n            agent.train_episode()\\n\\n            # update epsilon value\\n            # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\\n            if agent.epsilon > 0.05:\\n                agent.epsilon -= (1 / 5000)\\n\\n            # update target paramters\\n            # if episode % TARGET_UPDATE == 0:\\n            #     q_target.load_state_dict(q_approx.state_dict())\\n            #     q_target.eval()\\n        \\n        # shallow copy\\n        # rewards_per_episode = agent.train_data.copy()\\n        # alias\\n        rewards_per_episode = agent.train_data\\n        rewards[index_lr, index_gamma] = rewards_per_episode \\n\\n        total_rewards = np.sum(rewards[index_lr, index_gamma,:])\\n        print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\\n        average_reward = total_rewards/EPISODES\\n\\n        # save models\\n        model_info = str(learning_rate)+ \\'_\\' + str(gamma)\\n        path = BASE_PATH + \\'model\\' + \\'_\\' + model_info + \\'.pt\\'\\n        q_name = \\'q_approx_state_dict_\\' + model_info\\n        optim_name = \\'optim_state_dict_\\' + model_info\\n        model = {\\n            q_name: agent.q_model.state_dict(),\\n            optim_name: agent.q_model.optimizer.state_dict()\\n        }\\n        # model = {\\n        #     q_name: agent.q_model.state_dict(),\\n        #     optim_name: agent.optimizer.state_dict()\\n        # }\\n        torch.save(model, path)\\n    print(\\'-\\'*5)\\n# save rewards to disk\\nwith open(\"./dqn_rewards/dqn_rewards.pkl\",\\'wb\\') as f:\\n    pickle.dump(rewards, f)\\n'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 9
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "'''\n",
                "# plot average reward data\n",
                "# print(rewards[:,:,:].shape)\n",
                "import pandas as pd\n",
                "\n",
                "d = pd.Series(rewards[0,0,:])\n",
                "plt.plot(np.arange(EPISODES),d.rolling(100).mean())\n",
                "plt.xlabel('episodes')\n",
                "plt.ylabel('average reward')\n",
                "# plt.plot(rewards[:,:])\n",
                "'''"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "\"\\n# plot average reward data\\n# print(rewards[:,:,:].shape)\\nimport pandas as pd\\n\\nd = pd.Series(rewards[0,0,:])\\nplt.plot(np.arange(EPISODES),d.rolling(100).mean())\\nplt.xlabel('episodes')\\nplt.ylabel('average reward')\\n# plt.plot(rewards[:,:])\\n\""
                        ]
                    },
                    "metadata": {},
                    "execution_count": 10
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "from tqdm import tqdm\n",
                "gamma = 0.95\n",
                "epsilon = 1\n",
                "learning_rate = 1e-3\n",
                "EPISODES = 10000\n",
                "MINI_BATCH_SIZE = 16\n",
                "MEMORY_SIZE = 256\n",
                "# EPISODES = 100\n",
                "rewards = []\n",
                "loss_list = []\n",
                "epsilon_list = []\n",
                "q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "# q_target.eval()\n",
                "\n",
                "# print(q_approx)\n",
                "# print(q_target)\n",
                "# print(index_gamma)\n",
                "\n",
                "agent = Agent(q_approx, env, epsilon, gamma, MINI_BATCH_SIZE,\n",
                "              MEMORY_SIZE, learning_rate)\n",
                "\n",
                "# print(q_approx)\n",
                "# print(agent.q_target)\n",
                "# reset replay memory\n",
                "agent.init_replay_memory()\n",
                "agent.train_step_count = 128\n",
                "for episode in tqdm(range(EPISODES)):\n",
                "\n",
                "    # train one episode\n",
                "    # agent.train_episode()\n",
                "    done = False\n",
                "    obs = agent.env.reset()\n",
                "    rew = 0\n",
                "    losses = 0\n",
                "    while not done:\n",
                "        # get action to execute based on state\n",
                "\n",
                "        action = agent.get_action(obs, agent.n_actions, agent.epsilon)\n",
                "\n",
                "        #  take action, go to next time step\n",
                "        obs_next, reward, done, info = agent.env.step(action.item())\n",
                "\n",
                "        agent.collect_experience([obs, action.item(), reward, obs_next, done])\n",
                "\n",
                "        obs = obs_next\n",
                "        rew += reward\n",
                "        agent.train_step_count += 1\n",
                "\n",
                "        if agent.train_step_count > 128:\n",
                "            agent.train_step_count = 0\n",
                "            for _ in range(4):\n",
                "                loss = agent.train(agent.BATCH_SIZE)\n",
                "                losses += loss\n",
                "    # update epsilon value\n",
                "    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "    if agent.epsilon > 0.05:\n",
                "        agent.epsilon -= (1 / 5000)\n",
                "\n",
                "    rewards.append(rew)\n",
                "    loss_list.append(losses)\n",
                "    epsilon_list.append(agent.epsilon)\n",
                "\n",
                "# total_rewards = np.sum(rewards[0, 0,:])\n",
                "rewards = np.array(rewards)\n",
                "print(rewards.shape)\n",
                "# print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 10000/10000 [01:59<00:00, 83.53it/s]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(10000,)\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "# plot average reward data\n",
                "# print(rewards[:,:,:].shape)\n",
                "import pandas as pd\n",
                "\n",
                "# d = pd.Series(rewards[0,0,:])\n",
                "plt.plot(rewards)\n",
                "plt.xlabel('episodes')\n",
                "plt.ylabel('reward')\n",
                "# plt.plot(rewards[:,:])\n",
                "\n",
                "print('average reward per episode= {}'.format(np.mean(rewards, axis=0)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "average reward per episode= 100.4774\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "my_dict = {}\n",
                "# my_dict[\"data\"] = \"Their model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\nMy model params\\ntensor([[ 0.3041,  0.2882, -0.2917,  0.4162],\\n        [ 0.1855,  0.4892,  0.0079, -0.1485],\\n        [ 0.2827, -0.1978,  0.0915,  0.1492],\\n        [ 0.2694,  0.2406, -0.0709,  0.0753],\\n        [ 0.1781, -0.0996, -0.4768, -0.0902],\\n        [ 0.0527,  0.3042,  0.4053, -0.4673],\\n        [-0.3728,  0.4554,  0.1156,  0.1509],\\n        [-0.2538,  0.1942, -0.3371,  0.1100],\\n        [-0.3160, -0.2490, -0.2002,  0.1645],\\n        [-0.1380,  0.0902, -0.0432,  0.1049],\\n        [-0.1543,  0.4901,  0.0083,  0.0152],\\n        [ 0.1811, -0.4768,  0.0971,  0.1288],\\n        [ 0.2728,  0.4485, -0.2000,  0.1215],\\n        [ 0.1950, -0.4748,  0.2200,  0.2112],\\n        [-0.2967, -0.2873, -0.3639,  0.4410],\\n        [-0.1727, -0.3969, -0.0300,  0.4692],\\n        [-0.3265, -0.0512,  0.3419, -0.2583],\\n        [-0.0901, -0.2474, -0.0358, -0.0221],\\n        [-0.2907,  0.2076, -0.2116,  0.4516],\\n        [-0.4855, -0.2546, -0.3364, -0.3318],\\n        [-0.3998, -0.0230, -0.2552, -0.2768],\\n        [-0.3959,  0.0057,  0.4193,  0.2657],\\n        [-0.1441,  0.2253,  0.4854, -0.1122],\\n        [-0.1419,  0.1340, -0.2012,  0.2925],\\n        [ 0.3803,  0.3698, -0.4950, -0.1462],\\n        [-0.1645, -0.4683,  0.1710, -0.0956],\\n        [ 0.0400,  0.0339,  0.3003,  0.3645],\\n        [ 0.4606, -0.0104, -0.0227,  0.3237],\\n        [ 0.4904,  0.4335, -0.2649,  0.2615],\\n        [ 0.1789, -0.3907, -0.1692, -0.2415],\\n        [ 0.2577,  0.3993,  0.3816, -0.0371],\\n        [-0.0268,  0.4042,  0.2384,  0.4170],\\n        [ 0.3716, -0.1569, -0.2289, -0.4692],\\n        [-0.3748, -0.0156, -0.1977,  0.2151],\\n        [ 0.1638,  0.2755,  0.2921, -0.4573],\\n        [ 0.3901, -0.0634, -0.2933, -0.1968],\\n        [ 0.4970, -0.1262,  0.3650,  0.4007],\\n        [ 0.2788,  0.1445,  0.0942,  0.1048],\\n        [-0.3838, -0.4978, -0.4175, -0.1444],\\n        [-0.2990,  0.0109,  0.2287,  0.1491],\\n        [-0.2714,  0.1556, -0.0731,  0.4427],\\n        [-0.4720, -0.1670, -0.1322,  0.4997],\\n        [-0.0374, -0.2431, -0.4579, -0.1824],\\n        [ 0.2399, -0.1023, -0.0905, -0.0190],\\n        [-0.1580,  0.1126, -0.1087, -0.4724],\\n        [-0.0587, -0.2401,  0.3096,  0.0515],\\n        [-0.1888,  0.1881, -0.2436, -0.3915],\\n        [-0.0538, -0.3004,  0.4868,  0.4518],\\n        [ 0.4151,  0.3819, -0.4865, -0.2404],\\n        [-0.4273,  0.0289, -0.3914, -0.1450],\\n        [ 0.4119,  0.4608, -0.0953, -0.2822],\\n        [-0.2009, -0.2225,  0.3750,  0.1481],\\n        [ 0.4009,  0.2628, -0.4354,  0.4003],\\n        [ 0.1444,  0.0308,  0.4046,  0.0810],\\n        [ 0.0310,  0.0230, -0.3415, -0.2876],\\n        [ 0.0175, -0.4231, -0.2572, -0.4341],\\n        [ 0.1194, -0.0733, -0.1846,  0.3522],\\n        [-0.2700, -0.0280,  0.4058,  0.1455],\\n        [-0.1910, -0.2753, -0.4003, -0.1842],\\n        [-0.2268, -0.4600, -0.1236, -0.1261],\\n        [-0.3728, -0.3369,  0.4468,  0.1672],\\n        [ 0.4776, -0.4104,  0.1595,  0.0460],\\n        [-0.1318,  0.4914, -0.4923,  0.3753],\\n        [ 0.2644, -0.2238,  0.1891,  0.0529]])\\ntheir model = Sequential(\\n  (0): Linear(in_features=4, out_features=64, bias=True)\\n  (1): Tanh()\\n  (2): Linear(in_features=64, out_features=2, bias=True)\\n  (3): Identity()\\n)\\n\"\n",
                "# \"Their model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\nMy model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\n\"\n",
                "my_dict[\"data\"] = \"Their model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\nMy model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\n\"\n",
                "\n",
                "print(my_dict[\"data\"])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Their model params\n",
                        "tensor([[-0.0049,  0.4691,  0.0008, -0.2552],\n",
                        "        [-0.4204, -0.3378, -0.1222,  0.3769],\n",
                        "        [ 0.2397,  0.1964,  0.1816,  0.3778],\n",
                        "        [ 0.4347,  0.1436, -0.1596,  0.4537],\n",
                        "        [ 0.4348,  0.0355, -0.0083,  0.3024],\n",
                        "        [ 0.2166,  0.1349,  0.1098,  0.1224],\n",
                        "        [ 0.1775, -0.1645,  0.2105, -0.0397],\n",
                        "        [-0.0754, -0.4345,  0.3258,  0.1700],\n",
                        "        [-0.2496, -0.3918, -0.4072,  0.3070],\n",
                        "        [ 0.4155, -0.3877, -0.4982,  0.3630],\n",
                        "        [-0.2475,  0.3201,  0.1642, -0.3372],\n",
                        "        [ 0.2409,  0.1367, -0.3578,  0.0781],\n",
                        "        [-0.1473,  0.2909,  0.2483,  0.2926],\n",
                        "        [ 0.4901,  0.2115, -0.2738, -0.1703],\n",
                        "        [-0.0600,  0.2908,  0.4502,  0.4441],\n",
                        "        [ 0.4434,  0.4149, -0.2595,  0.1586],\n",
                        "        [ 0.3966,  0.1285, -0.2990,  0.1227],\n",
                        "        [-0.4980, -0.1935, -0.4164, -0.1403],\n",
                        "        [ 0.1850,  0.4681, -0.3764,  0.4686],\n",
                        "        [ 0.1104,  0.2758,  0.1343,  0.4484],\n",
                        "        [-0.3391, -0.1275, -0.4930,  0.2192],\n",
                        "        [ 0.1430, -0.1227, -0.2408, -0.2525],\n",
                        "        [-0.0177,  0.3471,  0.3640, -0.1564],\n",
                        "        [-0.0805, -0.4666,  0.4643, -0.3790],\n",
                        "        [ 0.0126, -0.1771,  0.0444,  0.0462],\n",
                        "        [ 0.2060,  0.0925,  0.3172, -0.2507],\n",
                        "        [-0.4180, -0.0289,  0.1603,  0.4187],\n",
                        "        [ 0.4140,  0.1766, -0.0715,  0.2054],\n",
                        "        [-0.1019, -0.1722, -0.4649,  0.0474],\n",
                        "        [ 0.0336, -0.4134, -0.4997, -0.2138],\n",
                        "        [-0.0089, -0.0535,  0.3089,  0.3632],\n",
                        "        [ 0.4706, -0.1999, -0.3329, -0.1394],\n",
                        "        [-0.2437,  0.0588, -0.4512, -0.4312],\n",
                        "        [-0.3709,  0.3943,  0.3293,  0.3457],\n",
                        "        [ 0.2464,  0.3609,  0.2065,  0.1736],\n",
                        "        [ 0.2262, -0.4726,  0.1455,  0.1766],\n",
                        "        [ 0.1093,  0.2545,  0.0765, -0.0834],\n",
                        "        [ 0.4327,  0.0756, -0.3747,  0.3781],\n",
                        "        [-0.0350, -0.4322,  0.4699,  0.4775],\n",
                        "        [ 0.1317, -0.0425,  0.4230, -0.3786],\n",
                        "        [ 0.3679, -0.4886, -0.2918, -0.2127],\n",
                        "        [ 0.3491, -0.1311, -0.0226,  0.1095],\n",
                        "        [ 0.0184, -0.3099,  0.1176,  0.1575],\n",
                        "        [ 0.0322, -0.0294, -0.2927, -0.3065],\n",
                        "        [-0.1197,  0.0516,  0.3122, -0.2274],\n",
                        "        [-0.0270,  0.1684,  0.2447, -0.0240],\n",
                        "        [-0.2449,  0.1924, -0.4391,  0.4411],\n",
                        "        [ 0.3282, -0.0320,  0.3797,  0.3175],\n",
                        "        [-0.0109,  0.0690, -0.2216,  0.2285],\n",
                        "        [ 0.1793, -0.1409, -0.4187, -0.0879],\n",
                        "        [-0.0542,  0.1776, -0.0465, -0.4142],\n",
                        "        [-0.2198, -0.0282,  0.0586, -0.1978],\n",
                        "        [ 0.4521, -0.1308,  0.4037, -0.3965],\n",
                        "        [ 0.4480,  0.3013, -0.3900, -0.2043],\n",
                        "        [ 0.3752, -0.2595,  0.1841, -0.4292],\n",
                        "        [ 0.3758,  0.0451,  0.2408,  0.4024],\n",
                        "        [-0.1107, -0.0551,  0.0391, -0.3562],\n",
                        "        [-0.3748, -0.2884, -0.2934,  0.3486],\n",
                        "        [-0.1138,  0.2532,  0.1133,  0.1165],\n",
                        "        [-0.3540, -0.1052,  0.2960,  0.2832],\n",
                        "        [-0.1219, -0.3756, -0.1850,  0.3641],\n",
                        "        [-0.1459,  0.4960,  0.2193, -0.4555],\n",
                        "        [-0.4368, -0.4323,  0.3171,  0.1576],\n",
                        "        [-0.4617,  0.4231,  0.1655,  0.1769]])\n",
                        "My model params\n",
                        "tensor([[-0.0049,  0.4691,  0.0008, -0.2552],\n",
                        "        [-0.4204, -0.3378, -0.1222,  0.3769],\n",
                        "        [ 0.2397,  0.1964,  0.1816,  0.3778],\n",
                        "        [ 0.4347,  0.1436, -0.1596,  0.4537],\n",
                        "        [ 0.4348,  0.0355, -0.0083,  0.3024],\n",
                        "        [ 0.2166,  0.1349,  0.1098,  0.1224],\n",
                        "        [ 0.1775, -0.1645,  0.2105, -0.0397],\n",
                        "        [-0.0754, -0.4345,  0.3258,  0.1700],\n",
                        "        [-0.2496, -0.3918, -0.4072,  0.3070],\n",
                        "        [ 0.4155, -0.3877, -0.4982,  0.3630],\n",
                        "        [-0.2475,  0.3201,  0.1642, -0.3372],\n",
                        "        [ 0.2409,  0.1367, -0.3578,  0.0781],\n",
                        "        [-0.1473,  0.2909,  0.2483,  0.2926],\n",
                        "        [ 0.4901,  0.2115, -0.2738, -0.1703],\n",
                        "        [-0.0600,  0.2908,  0.4502,  0.4441],\n",
                        "        [ 0.4434,  0.4149, -0.2595,  0.1586],\n",
                        "        [ 0.3966,  0.1285, -0.2990,  0.1227],\n",
                        "        [-0.4980, -0.1935, -0.4164, -0.1403],\n",
                        "        [ 0.1850,  0.4681, -0.3764,  0.4686],\n",
                        "        [ 0.1104,  0.2758,  0.1343,  0.4484],\n",
                        "        [-0.3391, -0.1275, -0.4930,  0.2192],\n",
                        "        [ 0.1430, -0.1227, -0.2408, -0.2525],\n",
                        "        [-0.0177,  0.3471,  0.3640, -0.1564],\n",
                        "        [-0.0805, -0.4666,  0.4643, -0.3790],\n",
                        "        [ 0.0126, -0.1771,  0.0444,  0.0462],\n",
                        "        [ 0.2060,  0.0925,  0.3172, -0.2507],\n",
                        "        [-0.4180, -0.0289,  0.1603,  0.4187],\n",
                        "        [ 0.4140,  0.1766, -0.0715,  0.2054],\n",
                        "        [-0.1019, -0.1722, -0.4649,  0.0474],\n",
                        "        [ 0.0336, -0.4134, -0.4997, -0.2138],\n",
                        "        [-0.0089, -0.0535,  0.3089,  0.3632],\n",
                        "        [ 0.4706, -0.1999, -0.3329, -0.1394],\n",
                        "        [-0.2437,  0.0588, -0.4512, -0.4312],\n",
                        "        [-0.3709,  0.3943,  0.3293,  0.3457],\n",
                        "        [ 0.2464,  0.3609,  0.2065,  0.1736],\n",
                        "        [ 0.2262, -0.4726,  0.1455,  0.1766],\n",
                        "        [ 0.1093,  0.2545,  0.0765, -0.0834],\n",
                        "        [ 0.4327,  0.0756, -0.3747,  0.3781],\n",
                        "        [-0.0350, -0.4322,  0.4699,  0.4775],\n",
                        "        [ 0.1317, -0.0425,  0.4230, -0.3786],\n",
                        "        [ 0.3679, -0.4886, -0.2918, -0.2127],\n",
                        "        [ 0.3491, -0.1311, -0.0226,  0.1095],\n",
                        "        [ 0.0184, -0.3099,  0.1176,  0.1575],\n",
                        "        [ 0.0322, -0.0294, -0.2927, -0.3065],\n",
                        "        [-0.1197,  0.0516,  0.3122, -0.2274],\n",
                        "        [-0.0270,  0.1684,  0.2447, -0.0240],\n",
                        "        [-0.2449,  0.1924, -0.4391,  0.4411],\n",
                        "        [ 0.3282, -0.0320,  0.3797,  0.3175],\n",
                        "        [-0.0109,  0.0690, -0.2216,  0.2285],\n",
                        "        [ 0.1793, -0.1409, -0.4187, -0.0879],\n",
                        "        [-0.0542,  0.1776, -0.0465, -0.4142],\n",
                        "        [-0.2198, -0.0282,  0.0586, -0.1978],\n",
                        "        [ 0.4521, -0.1308,  0.4037, -0.3965],\n",
                        "        [ 0.4480,  0.3013, -0.3900, -0.2043],\n",
                        "        [ 0.3752, -0.2595,  0.1841, -0.4292],\n",
                        "        [ 0.3758,  0.0451,  0.2408,  0.4024],\n",
                        "        [-0.1107, -0.0551,  0.0391, -0.3562],\n",
                        "        [-0.3748, -0.2884, -0.2934,  0.3486],\n",
                        "        [-0.1138,  0.2532,  0.1133,  0.1165],\n",
                        "        [-0.3540, -0.1052,  0.2960,  0.2832],\n",
                        "        [-0.1219, -0.3756, -0.1850,  0.3641],\n",
                        "        [-0.1459,  0.4960,  0.2193, -0.4555],\n",
                        "        [-0.4368, -0.4323,  0.3171,  0.1576],\n",
                        "        [-0.4617,  0.4231,  0.1655,  0.1769]])\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "# for param in q_approx.parameters():\n",
                "#     print(param.size())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "# RENDER = False\n",
                "# total_rewards = []\n",
                "# for episode in range(EPISODES):\n",
                "#     done = False\n",
                "#     obs = agent.env.reset()\n",
                "#     obs = torch.tensor([obs], device=agent.device, dtype=torch.float64)\n",
                "#     rewards = []\n",
                "#     while not done:\n",
                "#         # get action to execute based on state\n",
                "#         action = agent.get_action(obs.float())\n",
                "#         #  take action, go to next time step\n",
                "#         obs_next, reward, done, info = agent.env.step(action.item())\n",
                "#         obs_next = torch.tensor([obs_next], device=agent.device, dtype=torch.float64)\n",
                "#         obs = obs_next\n",
                "#         rewards.append(reward)\n",
                "#         if RENDER:\n",
                "#             env.render()\n",
                "\n",
                "#     rewards = np.array(rewards)\n",
                "#     total_rewards.append(np.sum(rewards))\n",
                "\n",
                "# plt.plot(total_rewards)\n",
                "# plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "'''\n",
                "\n",
                "learning_rate = 0.0001\n",
                "EPSILON_MIN = 0.05\n",
                "EPS_DECAY = 0.001\n",
                "gamma= 0.99\n",
                "TARGET_UPDATE = 10\n",
                "EPISODES = 10000\n",
                "\n",
                "MEMORY_SIZE = 256\n",
                "D = deque(maxlen=MEMORY_SIZE)\n",
                "MINI_BATCH_SIZE = 16\n",
                "epsilon=1\n",
                "\n",
                "\n",
                "\n",
                "q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "# load same weights as approx\n",
                "q_target.load_state_dict(q_approx.state_dict())\n",
                "q_target.eval()\n",
                "# reset replay memory\n",
                "D = deque(maxlen=MEMORY_SIZE)\n",
                "agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE)\n",
                "for episode in range(EPISODES):\n",
                "\n",
                "    if episode % 1000 == 0:\n",
                "        print('-'*5)\n",
                "        print('Episode = {}'.format(episode))\n",
                "        print('Epsilon = {}'.format(agent.epsilon))\n",
                "        print('-'*5)\n",
                "    # train one episode\n",
                "    agent.train_episode()\n",
                "\n",
                "    # update epsilon value\n",
                "    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "    if agent.epsilon > 0.05 :\n",
                "        agent.epsilon -= (1 / 5000)\n",
                "\n",
                "    # # update target paramters\n",
                "    if episode % TARGET_UPDATE == 0:\n",
                "        q_target.load_state_dict(q_approx.state_dict())\n",
                "        q_target.eval()\n",
                "\n",
                "# shallow copy\n",
                "# rewards_per_episode = agent.train_data.copy()\n",
                "# alias\n",
                "rewards_per_episode = agent.train_data\n",
                "\n",
                "total_rewards = np.sum(rewards_per_episode)\n",
                "print(\"Score over time: {}, gamma={}\".format( total_rewards/EPISODES, gamma))\n",
                "average_reward = total_rewards/EPISODES\n",
                "\n",
                "plt.plot(rewards_per_episode)\n",
                "\n",
                "\n",
                "'''"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'\\n\\nlearning_rate = 0.0001\\nEPSILON_MIN = 0.05\\nEPS_DECAY = 0.001\\ngamma= 0.99\\nTARGET_UPDATE = 10\\nEPISODES = 10000\\n\\nMEMORY_SIZE = 256\\nD = deque(maxlen=MEMORY_SIZE)\\nMINI_BATCH_SIZE = 16\\nepsilon=1\\n\\n\\n\\nq_approx = DQN(lr=learning_rate, device=device).to(device)\\nq_target = DQN(lr=learning_rate, device=device).to(device)\\n# load same weights as approx\\nq_target.load_state_dict(q_approx.state_dict())\\nq_target.eval()\\n# reset replay memory\\nD = deque(maxlen=MEMORY_SIZE)\\nagent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE)\\nfor episode in range(EPISODES):\\n\\n    if episode % 1000 == 0:\\n        print(\\'-\\'*5)\\n        print(\\'Episode = {}\\'.format(episode))\\n        print(\\'Epsilon = {}\\'.format(agent.epsilon))\\n        print(\\'-\\'*5)\\n    # train one episode\\n    agent.train_episode()\\n\\n    # update epsilon value\\n    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\\n    if agent.epsilon > 0.05 :\\n        agent.epsilon -= (1 / 5000)\\n\\n    # # update target paramters\\n    if episode % TARGET_UPDATE == 0:\\n        q_target.load_state_dict(q_approx.state_dict())\\n        q_target.eval()\\n\\n# shallow copy\\n# rewards_per_episode = agent.train_data.copy()\\n# alias\\nrewards_per_episode = agent.train_data\\n\\ntotal_rewards = np.sum(rewards_per_episode)\\nprint(\"Score over time: {}, gamma={}\".format( total_rewards/EPISODES, gamma))\\naverage_reward = total_rewards/EPISODES\\n\\nplt.plot(rewards_per_episode)\\n\\n\\n'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 16
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "f41a9e18d32d699c7ebd9346171aa8606b8eaf6d2e7d29caa03f22c5e982b824"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}