{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "import gym\n",
                "\n",
                "import numpy as np"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "env_id = 'CartPole-v1'\n",
                "env = gym.make(env_id)\n",
                "n_actions = env.action_space.n\n",
                "len_obs_space = env.observation_space.shape[0]\n",
                "\n",
                "print('n_actions = {}'.format(n_actions))\n",
                "print('len_obs_space = {}'.format(len_obs_space))\n",
                "\n",
                "# if gpu is to be used\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "n_actions = 2\n",
                        "len_obs_space = 4\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288#:~:text=Cartpole%20%2D%20known%20also%20as%20an,forces%20to%20a%20pivot%20point.\n",
                "\n",
                "\n",
                "https://github.com/gsurma/cartpole"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "class DQN(nn.Module):\n",
                "\n",
                "    def __init__(self, lr) -> None:\n",
                "        super().__init__()\n",
                "        self.flatten = nn.Flatten()\n",
                "        self.seq_relu = nn.Sequential(\n",
                "            nn.Linear(len_obs_space, len_obs_space),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(len_obs_space, len_obs_space),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(len_obs_space, n_actions)\n",
                "        )\n",
                "        self.learning_rate = lr\n",
                "        self.optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
                "        self.loss = nn.MSELoss()\n",
                "\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.flatten(x)\n",
                "        logits = self.seq_relu(x)\n",
                "        return logits\n",
                "\n",
                "learning_rate = 0.05\n",
                "q_approx = DQN(lr=learning_rate).to(device)\n",
                "q_target = DQN(lr=learning_rate).to(device)\n",
                "# load same weights as approx\n",
                "q_target.load_state_dict(q_approx.state_dict())\n",
                "q_target.eval()\n",
                "print(q_approx)\n",
                "print(q_target)\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "DQN(\n",
                        "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
                        "  (seq_relu): Sequential(\n",
                        "    (0): Linear(in_features=4, out_features=4, bias=True)\n",
                        "    (1): ReLU()\n",
                        "    (2): Linear(in_features=4, out_features=4, bias=True)\n",
                        "    (3): ReLU()\n",
                        "    (4): Linear(in_features=4, out_features=2, bias=True)\n",
                        "  )\n",
                        "  (loss): MSELoss()\n",
                        ")\n",
                        "DQN(\n",
                        "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
                        "  (seq_relu): Sequential(\n",
                        "    (0): Linear(in_features=4, out_features=4, bias=True)\n",
                        "    (1): ReLU()\n",
                        "    (2): Linear(in_features=4, out_features=4, bias=True)\n",
                        "    (3): ReLU()\n",
                        "    (4): Linear(in_features=4, out_features=2, bias=True)\n",
                        "  )\n",
                        "  (loss): MSELoss()\n",
                        ")\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "from collections import deque\n",
                "\n",
                "MEMORY_SIZE = 100\n",
                "D = deque(maxlen=MEMORY_SIZE)\n",
                "for i in range(MEMORY_SIZE):\n",
                "    D.append(0)\n",
                "\n",
                "\n",
                "MINI_BATCH_SIZE = 5\n",
                "EPISODES = 200\n",
                "\n",
                "EPSILON_MIN = 0.2\n",
                "EPSILON_START = 0.9\n",
                "EPISLON_BASE = 0.995\n",
                "epsilon = EPSILON_START"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "import random\n",
                "\n",
                "class Agent:\n",
                "    \n",
                "    def __init__(self, q_model, q_target, env, n_actions, epsilon) -> None:\n",
                "        self.q_model = q_model\n",
                "        self.q_target = q_target\n",
                "        self.env = env\n",
                "        self.n_actions = n_actions\n",
                "        self.epsilon = epsilon\n",
                "        self.device = self.q_model.device\n",
                "\n",
                "    def train_episode(self):\n",
                "        done = False\n",
                "        obs = self.env.reset()\n",
                "        obs = torch.tensor(obs, device=self.device)\n",
                "\n",
                "        while not done:\n",
                "            # get action to execute based on state\n",
                "            action = self.get_action(obs)\n",
                "            #  take action, go to next time step\n",
                "            obs_next, reward, done, info = self.env.step(action.item())\n",
                "\n",
                "            # convert to tensors\n",
                "            obs_next = torch.tensor(obs_next, device=self.device)\n",
                "            reward = torch.tensor(reward, device=self.device)\n",
                "            done = torch.tensor(done, device=self.device, dtype=torch.bool)\n",
                "            \n",
                "            # create transitions\n",
                "            transition = (obs, action, reward, obs_next, done)\n",
                "            # store transitions\n",
                "            D.append(transition)\n",
                "\n",
                "            # sample minibatch\n",
                "            minibatch = random.choices(D, k=MINI_BATCH_SIZE)\n",
                "            y_out = torch.zeros()\n",
                "            for sample in minibatch:\n",
                "                done_index = -1\n",
                "                if sample[done_index]:\n",
                "                    pass\n",
                "\n",
                "            # Compute prediction and loss\n",
                "            pred = model(X)\n",
                "            loss = self.q_model.loss(pred, y)\n",
                "            \n",
                "            # Backpropagation\n",
                "            self.q_model.optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            self.q_model.optimizer.step()\n",
                "                \n",
                "    def get_action(self, obs):\n",
                "        sample = random.random()\n",
                "        action = None\n",
                "        if sample < self.epsilon:\n",
                "            action = torch.tensor([random.choice(range(self.n_actions))])\n",
                "        else:\n",
                "            with torch.no_grad():\n",
                "                q_vals = self.q_model(obs)\n",
                "                print(q_vals)\n",
                "                action = torch.argmax(q_vals)\n",
                "        return torch.tensor([action], device=self.device)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "for episode in range(EPISODES):\n",
                "    # reset environment for new episode\n",
                "    obs = env.reset()\n",
                "    # get action to execute based on state\n",
                "    # action = q_approx.get_action(obs)\n",
                "    # update epsilon value"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "93b5180e63b11ee6de2da2bb211c0c987887a84125d818532d517fa521787195"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}