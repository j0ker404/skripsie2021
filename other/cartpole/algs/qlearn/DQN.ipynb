{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import torch\n",
                "import gym\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "env_id = 'CartPole-v1'\n",
                "env_id = 'CartPole-v0'\n",
                "env = gym.make(env_id)\n",
                "n_actions = env.action_space.n\n",
                "len_obs_space = env.observation_space.shape[0]\n",
                "\n",
                "print('n_actions = {}'.format(n_actions))\n",
                "print('len_obs_space = {}'.format(len_obs_space))\n",
                "# torch.manual_seed(1423)\n",
                "# if gpu is to be used\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "n_actions = 2\n",
                        "len_obs_space = 4\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288#:~:text=Cartpole%20%2D%20known%20also%20as%20an,forces%20to%20a%20pivot%20point.\n",
                "\n",
                "\n",
                "https://github.com/gsurma/cartpole"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "from dqn import DQN\n",
                "from dqn_agent import DQNAgent"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# -------------------------------\n",
                "# hyperparameters\n",
                "TARGET_UPDATE = 10\n",
                "\n",
                "EPSILON_MIN = 0.05\n",
                "EPSILON_START = 1\n",
                "EPS_DECAY = 0.001\n",
                "# EPS_DECAY = 0.0099\n",
                "\n",
                "epsilon = EPSILON_START\n",
                "gamma = 0.999\n",
                "EPISODES = 10000\n",
                "\n",
                "MINI_BATCH_SIZE = 16\n",
                "MEMORY_SIZE = 256\n",
                "# MINI_BATCH_SIZE = 10\n",
                "# MEMORY_SIZE = 100\n",
                "# -------------------------------"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "We need to tune the hyperparams\n",
                "- epsilon (epsilong_min)\n",
                "- gamma (discount rate)\n",
                "- learning rate\n",
                "- target update\n",
                "- mini batch size"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "# create range for hyperparams\n",
                "# learning_rates = np.arange(0.0001, 0.0002, 0.00001)\n",
                "# gammas = np.arange(0.98, 1.001, 0.001)\n",
                "gammas = np.array([0.95])\n",
                "learning_rates = np.array([1e-3])\n",
                "\n",
                "print('lr.shape = {}'.format(learning_rates.shape))\n",
                "print('gammas.shape = {}'.format(gammas.shape))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "lr.shape = (1,)\n",
                        "gammas.shape = (1,)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "rewards = np.zeros((len(learning_rates), len(gammas), EPISODES))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "print(rewards.shape)\n",
                "\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(1, 1, 10000)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "BASE_PATH = \"./dqn_models/\"\n",
                "'''\n",
                "for index_lr, learning_rate in enumerate(learning_rates):\n",
                "    print('-'*5)\n",
                "    print('lr = {}'.format(learning_rate))\n",
                "    for index_gamma, gamma in enumerate(gammas):\n",
                "        q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "        q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "        # load same weights as approx\n",
                "        q_target.load_state_dict(q_approx.state_dict())\n",
                "        q_target.eval()\n",
                "\n",
                "        # print(q_approx)\n",
                "        # print(q_target)\n",
                "        # print(index_gamma)\n",
                "        \n",
                "        agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE, MEMORY_SIZE)\n",
                "\n",
                "        # reset replay memory\n",
                "        agent.init_replay_memory()\n",
                "\n",
                "        for episode in range(EPISODES):\n",
                "\n",
                "            # if episode % 100 == 0:\n",
                "            #     print('Episode = {}'.format(episode))\n",
                "            #     print('Epsilon = {}'.format(agent.epsilon))\n",
                "            # train one episode\n",
                "            agent.train_episode()\n",
                "\n",
                "            # update epsilon value\n",
                "            # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "            if agent.epsilon > 0.05:\n",
                "                agent.epsilon -= (1 / 5000)\n",
                "\n",
                "            # update target paramters\n",
                "            # if episode % TARGET_UPDATE == 0:\n",
                "            #     q_target.load_state_dict(q_approx.state_dict())\n",
                "            #     q_target.eval()\n",
                "        \n",
                "        # shallow copy\n",
                "        # rewards_per_episode = agent.train_data.copy()\n",
                "        # alias\n",
                "        rewards_per_episode = agent.train_data\n",
                "        rewards[index_lr, index_gamma] = rewards_per_episode \n",
                "\n",
                "        total_rewards = np.sum(rewards[index_lr, index_gamma,:])\n",
                "        print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\n",
                "        average_reward = total_rewards/EPISODES\n",
                "\n",
                "        # save models\n",
                "        model_info = str(learning_rate)+ '_' + str(gamma)\n",
                "        path = BASE_PATH + 'model' + '_' + model_info + '.pt'\n",
                "        q_name = 'q_approx_state_dict_' + model_info\n",
                "        optim_name = 'optim_state_dict_' + model_info\n",
                "        model = {\n",
                "            q_name: agent.q_model.state_dict(),\n",
                "            optim_name: agent.q_model.optimizer.state_dict()\n",
                "        }\n",
                "        # model = {\n",
                "        #     q_name: agent.q_model.state_dict(),\n",
                "        #     optim_name: agent.optimizer.state_dict()\n",
                "        # }\n",
                "        torch.save(model, path)\n",
                "    print('-'*5)\n",
                "# save rewards to disk\n",
                "with open(\"./dqn_rewards/dqn_rewards.pkl\",'wb') as f:\n",
                "    pickle.dump(rewards, f)\n",
                "'''"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'\\nfor index_lr, learning_rate in enumerate(learning_rates):\\n    print(\\'-\\'*5)\\n    print(\\'lr = {}\\'.format(learning_rate))\\n    for index_gamma, gamma in enumerate(gammas):\\n        q_approx = DQN(lr=learning_rate, device=device).to(device)\\n        q_target = DQN(lr=learning_rate, device=device).to(device)\\n        # load same weights as approx\\n        q_target.load_state_dict(q_approx.state_dict())\\n        q_target.eval()\\n\\n        # print(q_approx)\\n        # print(q_target)\\n        # print(index_gamma)\\n        \\n        agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE, MEMORY_SIZE)\\n\\n        # reset replay memory\\n        agent.init_replay_memory()\\n\\n        for episode in range(EPISODES):\\n\\n            # if episode % 100 == 0:\\n            #     print(\\'Episode = {}\\'.format(episode))\\n            #     print(\\'Epsilon = {}\\'.format(agent.epsilon))\\n            # train one episode\\n            agent.train_episode()\\n\\n            # update epsilon value\\n            # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\\n            if agent.epsilon > 0.05:\\n                agent.epsilon -= (1 / 5000)\\n\\n            # update target paramters\\n            # if episode % TARGET_UPDATE == 0:\\n            #     q_target.load_state_dict(q_approx.state_dict())\\n            #     q_target.eval()\\n        \\n        # shallow copy\\n        # rewards_per_episode = agent.train_data.copy()\\n        # alias\\n        rewards_per_episode = agent.train_data\\n        rewards[index_lr, index_gamma] = rewards_per_episode \\n\\n        total_rewards = np.sum(rewards[index_lr, index_gamma,:])\\n        print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\\n        average_reward = total_rewards/EPISODES\\n\\n        # save models\\n        model_info = str(learning_rate)+ \\'_\\' + str(gamma)\\n        path = BASE_PATH + \\'model\\' + \\'_\\' + model_info + \\'.pt\\'\\n        q_name = \\'q_approx_state_dict_\\' + model_info\\n        optim_name = \\'optim_state_dict_\\' + model_info\\n        model = {\\n            q_name: agent.q_model.state_dict(),\\n            optim_name: agent.q_model.optimizer.state_dict()\\n        }\\n        # model = {\\n        #     q_name: agent.q_model.state_dict(),\\n        #     optim_name: agent.optimizer.state_dict()\\n        # }\\n        torch.save(model, path)\\n    print(\\'-\\'*5)\\n# save rewards to disk\\nwith open(\"./dqn_rewards/dqn_rewards.pkl\",\\'wb\\') as f:\\n    pickle.dump(rewards, f)\\n'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "'''\n",
                "# plot average reward data\n",
                "# print(rewards[:,:,:].shape)\n",
                "import pandas as pd\n",
                "\n",
                "d = pd.Series(rewards[0,0,:])\n",
                "plt.plot(np.arange(EPISODES),d.rolling(100).mean())\n",
                "plt.xlabel('episodes')\n",
                "plt.ylabel('average reward')\n",
                "# plt.plot(rewards[:,:])\n",
                "'''"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "\"\\n# plot average reward data\\n# print(rewards[:,:,:].shape)\\nimport pandas as pd\\n\\nd = pd.Series(rewards[0,0,:])\\nplt.plot(np.arange(EPISODES),d.rolling(100).mean())\\nplt.xlabel('episodes')\\nplt.ylabel('average reward')\\n# plt.plot(rewards[:,:])\\n\""
                        ]
                    },
                    "metadata": {},
                    "execution_count": 9
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "'''\n",
                "\n",
                "from tqdm import tqdm\n",
                "gamma = 0.95\n",
                "epsilon = 1\n",
                "learning_rate = 1e-3\n",
                "EPISODES = 10000\n",
                "MINI_BATCH_SIZE = 16\n",
                "MEMORY_SIZE = 256\n",
                "# EPISODES = 100\n",
                "rewards = []\n",
                "loss_list = []\n",
                "epsilon_list = []\n",
                "q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "# q_target.eval()\n",
                "\n",
                "# print(q_approx)\n",
                "# print(q_target)\n",
                "# print(index_gamma)\n",
                "\n",
                "agent = Agent(q_approx, env, epsilon, gamma, MINI_BATCH_SIZE,\n",
                "              MEMORY_SIZE, learning_rate)\n",
                "\n",
                "# print(q_approx)\n",
                "# print(agent.q_target)\n",
                "# reset replay memory\n",
                "agent.init_replay_memory()\n",
                "agent.train_step_count = 128\n",
                "for episode in tqdm(range(EPISODES)):\n",
                "\n",
                "    # train one episode\n",
                "    # agent.train_episode()\n",
                "    done = False\n",
                "    obs = agent.env.reset()\n",
                "    rew = 0\n",
                "    losses = 0\n",
                "    while not done:\n",
                "        # get action to execute based on state\n",
                "\n",
                "        action = agent.get_action(obs, agent.n_actions, agent.epsilon)\n",
                "\n",
                "        #  take action, go to next time step\n",
                "        obs_next, reward, done, info = agent.env.step(action.item())\n",
                "\n",
                "        agent.collect_experience([obs, action.item(), reward, obs_next, done])\n",
                "\n",
                "        obs = obs_next\n",
                "        rew += reward\n",
                "        agent.train_step_count += 1\n",
                "\n",
                "        if agent.train_step_count > 128:\n",
                "            agent.train_step_count = 0\n",
                "            for _ in range(4):\n",
                "                loss = agent.train(agent.BATCH_SIZE)\n",
                "                losses += loss\n",
                "    # update epsilon value\n",
                "    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "    if agent.epsilon > 0.05:\n",
                "        agent.epsilon -= (1 / 5000)\n",
                "\n",
                "    rewards.append(rew)\n",
                "    loss_list.append(losses)\n",
                "    epsilon_list.append(agent.epsilon)\n",
                "\n",
                "# total_rewards = np.sum(rewards[0, 0,:])\n",
                "rewards = np.array(rewards)\n",
                "print(rewards.shape)\n",
                "# print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\n",
                "\n",
                "'''"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'\\n\\nfrom tqdm import tqdm\\ngamma = 0.95\\nepsilon = 1\\nlearning_rate = 1e-3\\nEPISODES = 10000\\nMINI_BATCH_SIZE = 16\\nMEMORY_SIZE = 256\\n# EPISODES = 100\\nrewards = []\\nloss_list = []\\nepsilon_list = []\\nq_approx = DQN(lr=learning_rate, device=device).to(device)\\n# q_target.eval()\\n\\n# print(q_approx)\\n# print(q_target)\\n# print(index_gamma)\\n\\nagent = Agent(q_approx, env, epsilon, gamma, MINI_BATCH_SIZE,\\n              MEMORY_SIZE, learning_rate)\\n\\n# print(q_approx)\\n# print(agent.q_target)\\n# reset replay memory\\nagent.init_replay_memory()\\nagent.train_step_count = 128\\nfor episode in tqdm(range(EPISODES)):\\n\\n    # train one episode\\n    # agent.train_episode()\\n    done = False\\n    obs = agent.env.reset()\\n    rew = 0\\n    losses = 0\\n    while not done:\\n        # get action to execute based on state\\n\\n        action = agent.get_action(obs, agent.n_actions, agent.epsilon)\\n\\n        #  take action, go to next time step\\n        obs_next, reward, done, info = agent.env.step(action.item())\\n\\n        agent.collect_experience([obs, action.item(), reward, obs_next, done])\\n\\n        obs = obs_next\\n        rew += reward\\n        agent.train_step_count += 1\\n\\n        if agent.train_step_count > 128:\\n            agent.train_step_count = 0\\n            for _ in range(4):\\n                loss = agent.train(agent.BATCH_SIZE)\\n                losses += loss\\n    # update epsilon value\\n    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\\n    if agent.epsilon > 0.05:\\n        agent.epsilon -= (1 / 5000)\\n\\n    rewards.append(rew)\\n    loss_list.append(losses)\\n    epsilon_list.append(agent.epsilon)\\n\\n# total_rewards = np.sum(rewards[0, 0,:])\\nrewards = np.array(rewards)\\nprint(rewards.shape)\\n# print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\\n\\n'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 10
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "gamma = 0.95\n",
                "epsilon = 1\n",
                "learning_rate = 1e-3\n",
                "EPISODES = 10000\n",
                "MINI_BATCH_SIZE = 16\n",
                "MEMORY_SIZE = 256\n",
                "# EPISODES = 100\n",
                "rewards = []\n",
                "loss_list = []\n",
                "epsilon_list = []\n",
                "layer_sizes = [len_obs_space, 64, n_actions]\n",
                "q_approx = DQN(lr=learning_rate, layer_sizes=layer_sizes, device=device).to(device)\n",
                "\n",
                "agent = DQNAgent(q_approx, env, epsilon, gamma, MINI_BATCH_SIZE,\n",
                "              MEMORY_SIZE, learning_rate)\n",
                "\n",
                "# reset replay memory\n",
                "agent.init_replay_memory()\n",
                "agent.train_step_count = 128\n",
                "\n",
                "agent.train(EPISODES=EPISODES, is_progress=True)\n",
                "# total_rewards = np.sum(rewards[0, 0,:])\n",
                "rewards = np.array(agent.rewards)\n",
                "print(rewards.shape)\n",
                "# print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 10000/10000 [01:48<00:00, 92.04it/s]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(10000,)\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "# plot average reward data\n",
                "# print(rewards[:,:,:].shape)\n",
                "import pandas as pd\n",
                "\n",
                "# d = pd.Series(rewards[0,0,:])\n",
                "plt.plot(rewards)\n",
                "plt.xlabel('episodes')\n",
                "plt.ylabel('reward')\n",
                "# plt.plot(rewards[:,:])\n",
                "\n",
                "print('average reward per episode= {}'.format(np.mean(rewards, axis=0)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "average reward per episode= 106.0691\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "my_dict = {}\n",
                "# my_dict[\"data\"] = \"Their model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\nMy model params\\ntensor([[ 0.3041,  0.2882, -0.2917,  0.4162],\\n        [ 0.1855,  0.4892,  0.0079, -0.1485],\\n        [ 0.2827, -0.1978,  0.0915,  0.1492],\\n        [ 0.2694,  0.2406, -0.0709,  0.0753],\\n        [ 0.1781, -0.0996, -0.4768, -0.0902],\\n        [ 0.0527,  0.3042,  0.4053, -0.4673],\\n        [-0.3728,  0.4554,  0.1156,  0.1509],\\n        [-0.2538,  0.1942, -0.3371,  0.1100],\\n        [-0.3160, -0.2490, -0.2002,  0.1645],\\n        [-0.1380,  0.0902, -0.0432,  0.1049],\\n        [-0.1543,  0.4901,  0.0083,  0.0152],\\n        [ 0.1811, -0.4768,  0.0971,  0.1288],\\n        [ 0.2728,  0.4485, -0.2000,  0.1215],\\n        [ 0.1950, -0.4748,  0.2200,  0.2112],\\n        [-0.2967, -0.2873, -0.3639,  0.4410],\\n        [-0.1727, -0.3969, -0.0300,  0.4692],\\n        [-0.3265, -0.0512,  0.3419, -0.2583],\\n        [-0.0901, -0.2474, -0.0358, -0.0221],\\n        [-0.2907,  0.2076, -0.2116,  0.4516],\\n        [-0.4855, -0.2546, -0.3364, -0.3318],\\n        [-0.3998, -0.0230, -0.2552, -0.2768],\\n        [-0.3959,  0.0057,  0.4193,  0.2657],\\n        [-0.1441,  0.2253,  0.4854, -0.1122],\\n        [-0.1419,  0.1340, -0.2012,  0.2925],\\n        [ 0.3803,  0.3698, -0.4950, -0.1462],\\n        [-0.1645, -0.4683,  0.1710, -0.0956],\\n        [ 0.0400,  0.0339,  0.3003,  0.3645],\\n        [ 0.4606, -0.0104, -0.0227,  0.3237],\\n        [ 0.4904,  0.4335, -0.2649,  0.2615],\\n        [ 0.1789, -0.3907, -0.1692, -0.2415],\\n        [ 0.2577,  0.3993,  0.3816, -0.0371],\\n        [-0.0268,  0.4042,  0.2384,  0.4170],\\n        [ 0.3716, -0.1569, -0.2289, -0.4692],\\n        [-0.3748, -0.0156, -0.1977,  0.2151],\\n        [ 0.1638,  0.2755,  0.2921, -0.4573],\\n        [ 0.3901, -0.0634, -0.2933, -0.1968],\\n        [ 0.4970, -0.1262,  0.3650,  0.4007],\\n        [ 0.2788,  0.1445,  0.0942,  0.1048],\\n        [-0.3838, -0.4978, -0.4175, -0.1444],\\n        [-0.2990,  0.0109,  0.2287,  0.1491],\\n        [-0.2714,  0.1556, -0.0731,  0.4427],\\n        [-0.4720, -0.1670, -0.1322,  0.4997],\\n        [-0.0374, -0.2431, -0.4579, -0.1824],\\n        [ 0.2399, -0.1023, -0.0905, -0.0190],\\n        [-0.1580,  0.1126, -0.1087, -0.4724],\\n        [-0.0587, -0.2401,  0.3096,  0.0515],\\n        [-0.1888,  0.1881, -0.2436, -0.3915],\\n        [-0.0538, -0.3004,  0.4868,  0.4518],\\n        [ 0.4151,  0.3819, -0.4865, -0.2404],\\n        [-0.4273,  0.0289, -0.3914, -0.1450],\\n        [ 0.4119,  0.4608, -0.0953, -0.2822],\\n        [-0.2009, -0.2225,  0.3750,  0.1481],\\n        [ 0.4009,  0.2628, -0.4354,  0.4003],\\n        [ 0.1444,  0.0308,  0.4046,  0.0810],\\n        [ 0.0310,  0.0230, -0.3415, -0.2876],\\n        [ 0.0175, -0.4231, -0.2572, -0.4341],\\n        [ 0.1194, -0.0733, -0.1846,  0.3522],\\n        [-0.2700, -0.0280,  0.4058,  0.1455],\\n        [-0.1910, -0.2753, -0.4003, -0.1842],\\n        [-0.2268, -0.4600, -0.1236, -0.1261],\\n        [-0.3728, -0.3369,  0.4468,  0.1672],\\n        [ 0.4776, -0.4104,  0.1595,  0.0460],\\n        [-0.1318,  0.4914, -0.4923,  0.3753],\\n        [ 0.2644, -0.2238,  0.1891,  0.0529]])\\ntheir model = Sequential(\\n  (0): Linear(in_features=4, out_features=64, bias=True)\\n  (1): Tanh()\\n  (2): Linear(in_features=64, out_features=2, bias=True)\\n  (3): Identity()\\n)\\n\"\n",
                "# \"Their model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\nMy model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\n\"\n",
                "my_dict[\"data\"] = \"Their model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\nMy model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\n\"\n",
                "\n",
                "print(my_dict[\"data\"])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Their model params\n",
                        "tensor([[-0.0049,  0.4691,  0.0008, -0.2552],\n",
                        "        [-0.4204, -0.3378, -0.1222,  0.3769],\n",
                        "        [ 0.2397,  0.1964,  0.1816,  0.3778],\n",
                        "        [ 0.4347,  0.1436, -0.1596,  0.4537],\n",
                        "        [ 0.4348,  0.0355, -0.0083,  0.3024],\n",
                        "        [ 0.2166,  0.1349,  0.1098,  0.1224],\n",
                        "        [ 0.1775, -0.1645,  0.2105, -0.0397],\n",
                        "        [-0.0754, -0.4345,  0.3258,  0.1700],\n",
                        "        [-0.2496, -0.3918, -0.4072,  0.3070],\n",
                        "        [ 0.4155, -0.3877, -0.4982,  0.3630],\n",
                        "        [-0.2475,  0.3201,  0.1642, -0.3372],\n",
                        "        [ 0.2409,  0.1367, -0.3578,  0.0781],\n",
                        "        [-0.1473,  0.2909,  0.2483,  0.2926],\n",
                        "        [ 0.4901,  0.2115, -0.2738, -0.1703],\n",
                        "        [-0.0600,  0.2908,  0.4502,  0.4441],\n",
                        "        [ 0.4434,  0.4149, -0.2595,  0.1586],\n",
                        "        [ 0.3966,  0.1285, -0.2990,  0.1227],\n",
                        "        [-0.4980, -0.1935, -0.4164, -0.1403],\n",
                        "        [ 0.1850,  0.4681, -0.3764,  0.4686],\n",
                        "        [ 0.1104,  0.2758,  0.1343,  0.4484],\n",
                        "        [-0.3391, -0.1275, -0.4930,  0.2192],\n",
                        "        [ 0.1430, -0.1227, -0.2408, -0.2525],\n",
                        "        [-0.0177,  0.3471,  0.3640, -0.1564],\n",
                        "        [-0.0805, -0.4666,  0.4643, -0.3790],\n",
                        "        [ 0.0126, -0.1771,  0.0444,  0.0462],\n",
                        "        [ 0.2060,  0.0925,  0.3172, -0.2507],\n",
                        "        [-0.4180, -0.0289,  0.1603,  0.4187],\n",
                        "        [ 0.4140,  0.1766, -0.0715,  0.2054],\n",
                        "        [-0.1019, -0.1722, -0.4649,  0.0474],\n",
                        "        [ 0.0336, -0.4134, -0.4997, -0.2138],\n",
                        "        [-0.0089, -0.0535,  0.3089,  0.3632],\n",
                        "        [ 0.4706, -0.1999, -0.3329, -0.1394],\n",
                        "        [-0.2437,  0.0588, -0.4512, -0.4312],\n",
                        "        [-0.3709,  0.3943,  0.3293,  0.3457],\n",
                        "        [ 0.2464,  0.3609,  0.2065,  0.1736],\n",
                        "        [ 0.2262, -0.4726,  0.1455,  0.1766],\n",
                        "        [ 0.1093,  0.2545,  0.0765, -0.0834],\n",
                        "        [ 0.4327,  0.0756, -0.3747,  0.3781],\n",
                        "        [-0.0350, -0.4322,  0.4699,  0.4775],\n",
                        "        [ 0.1317, -0.0425,  0.4230, -0.3786],\n",
                        "        [ 0.3679, -0.4886, -0.2918, -0.2127],\n",
                        "        [ 0.3491, -0.1311, -0.0226,  0.1095],\n",
                        "        [ 0.0184, -0.3099,  0.1176,  0.1575],\n",
                        "        [ 0.0322, -0.0294, -0.2927, -0.3065],\n",
                        "        [-0.1197,  0.0516,  0.3122, -0.2274],\n",
                        "        [-0.0270,  0.1684,  0.2447, -0.0240],\n",
                        "        [-0.2449,  0.1924, -0.4391,  0.4411],\n",
                        "        [ 0.3282, -0.0320,  0.3797,  0.3175],\n",
                        "        [-0.0109,  0.0690, -0.2216,  0.2285],\n",
                        "        [ 0.1793, -0.1409, -0.4187, -0.0879],\n",
                        "        [-0.0542,  0.1776, -0.0465, -0.4142],\n",
                        "        [-0.2198, -0.0282,  0.0586, -0.1978],\n",
                        "        [ 0.4521, -0.1308,  0.4037, -0.3965],\n",
                        "        [ 0.4480,  0.3013, -0.3900, -0.2043],\n",
                        "        [ 0.3752, -0.2595,  0.1841, -0.4292],\n",
                        "        [ 0.3758,  0.0451,  0.2408,  0.4024],\n",
                        "        [-0.1107, -0.0551,  0.0391, -0.3562],\n",
                        "        [-0.3748, -0.2884, -0.2934,  0.3486],\n",
                        "        [-0.1138,  0.2532,  0.1133,  0.1165],\n",
                        "        [-0.3540, -0.1052,  0.2960,  0.2832],\n",
                        "        [-0.1219, -0.3756, -0.1850,  0.3641],\n",
                        "        [-0.1459,  0.4960,  0.2193, -0.4555],\n",
                        "        [-0.4368, -0.4323,  0.3171,  0.1576],\n",
                        "        [-0.4617,  0.4231,  0.1655,  0.1769]])\n",
                        "My model params\n",
                        "tensor([[-0.0049,  0.4691,  0.0008, -0.2552],\n",
                        "        [-0.4204, -0.3378, -0.1222,  0.3769],\n",
                        "        [ 0.2397,  0.1964,  0.1816,  0.3778],\n",
                        "        [ 0.4347,  0.1436, -0.1596,  0.4537],\n",
                        "        [ 0.4348,  0.0355, -0.0083,  0.3024],\n",
                        "        [ 0.2166,  0.1349,  0.1098,  0.1224],\n",
                        "        [ 0.1775, -0.1645,  0.2105, -0.0397],\n",
                        "        [-0.0754, -0.4345,  0.3258,  0.1700],\n",
                        "        [-0.2496, -0.3918, -0.4072,  0.3070],\n",
                        "        [ 0.4155, -0.3877, -0.4982,  0.3630],\n",
                        "        [-0.2475,  0.3201,  0.1642, -0.3372],\n",
                        "        [ 0.2409,  0.1367, -0.3578,  0.0781],\n",
                        "        [-0.1473,  0.2909,  0.2483,  0.2926],\n",
                        "        [ 0.4901,  0.2115, -0.2738, -0.1703],\n",
                        "        [-0.0600,  0.2908,  0.4502,  0.4441],\n",
                        "        [ 0.4434,  0.4149, -0.2595,  0.1586],\n",
                        "        [ 0.3966,  0.1285, -0.2990,  0.1227],\n",
                        "        [-0.4980, -0.1935, -0.4164, -0.1403],\n",
                        "        [ 0.1850,  0.4681, -0.3764,  0.4686],\n",
                        "        [ 0.1104,  0.2758,  0.1343,  0.4484],\n",
                        "        [-0.3391, -0.1275, -0.4930,  0.2192],\n",
                        "        [ 0.1430, -0.1227, -0.2408, -0.2525],\n",
                        "        [-0.0177,  0.3471,  0.3640, -0.1564],\n",
                        "        [-0.0805, -0.4666,  0.4643, -0.3790],\n",
                        "        [ 0.0126, -0.1771,  0.0444,  0.0462],\n",
                        "        [ 0.2060,  0.0925,  0.3172, -0.2507],\n",
                        "        [-0.4180, -0.0289,  0.1603,  0.4187],\n",
                        "        [ 0.4140,  0.1766, -0.0715,  0.2054],\n",
                        "        [-0.1019, -0.1722, -0.4649,  0.0474],\n",
                        "        [ 0.0336, -0.4134, -0.4997, -0.2138],\n",
                        "        [-0.0089, -0.0535,  0.3089,  0.3632],\n",
                        "        [ 0.4706, -0.1999, -0.3329, -0.1394],\n",
                        "        [-0.2437,  0.0588, -0.4512, -0.4312],\n",
                        "        [-0.3709,  0.3943,  0.3293,  0.3457],\n",
                        "        [ 0.2464,  0.3609,  0.2065,  0.1736],\n",
                        "        [ 0.2262, -0.4726,  0.1455,  0.1766],\n",
                        "        [ 0.1093,  0.2545,  0.0765, -0.0834],\n",
                        "        [ 0.4327,  0.0756, -0.3747,  0.3781],\n",
                        "        [-0.0350, -0.4322,  0.4699,  0.4775],\n",
                        "        [ 0.1317, -0.0425,  0.4230, -0.3786],\n",
                        "        [ 0.3679, -0.4886, -0.2918, -0.2127],\n",
                        "        [ 0.3491, -0.1311, -0.0226,  0.1095],\n",
                        "        [ 0.0184, -0.3099,  0.1176,  0.1575],\n",
                        "        [ 0.0322, -0.0294, -0.2927, -0.3065],\n",
                        "        [-0.1197,  0.0516,  0.3122, -0.2274],\n",
                        "        [-0.0270,  0.1684,  0.2447, -0.0240],\n",
                        "        [-0.2449,  0.1924, -0.4391,  0.4411],\n",
                        "        [ 0.3282, -0.0320,  0.3797,  0.3175],\n",
                        "        [-0.0109,  0.0690, -0.2216,  0.2285],\n",
                        "        [ 0.1793, -0.1409, -0.4187, -0.0879],\n",
                        "        [-0.0542,  0.1776, -0.0465, -0.4142],\n",
                        "        [-0.2198, -0.0282,  0.0586, -0.1978],\n",
                        "        [ 0.4521, -0.1308,  0.4037, -0.3965],\n",
                        "        [ 0.4480,  0.3013, -0.3900, -0.2043],\n",
                        "        [ 0.3752, -0.2595,  0.1841, -0.4292],\n",
                        "        [ 0.3758,  0.0451,  0.2408,  0.4024],\n",
                        "        [-0.1107, -0.0551,  0.0391, -0.3562],\n",
                        "        [-0.3748, -0.2884, -0.2934,  0.3486],\n",
                        "        [-0.1138,  0.2532,  0.1133,  0.1165],\n",
                        "        [-0.3540, -0.1052,  0.2960,  0.2832],\n",
                        "        [-0.1219, -0.3756, -0.1850,  0.3641],\n",
                        "        [-0.1459,  0.4960,  0.2193, -0.4555],\n",
                        "        [-0.4368, -0.4323,  0.3171,  0.1576],\n",
                        "        [-0.4617,  0.4231,  0.1655,  0.1769]])\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "# for param in q_approx.parameters():\n",
                "#     print(param.size())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "# RENDER = False\n",
                "# total_rewards = []\n",
                "# for episode in range(EPISODES):\n",
                "#     done = False\n",
                "#     obs = agent.env.reset()\n",
                "#     obs = torch.tensor([obs], device=agent.device, dtype=torch.float64)\n",
                "#     rewards = []\n",
                "#     while not done:\n",
                "#         # get action to execute based on state\n",
                "#         action = agent.get_action(obs.float())\n",
                "#         #  take action, go to next time step\n",
                "#         obs_next, reward, done, info = agent.env.step(action.item())\n",
                "#         obs_next = torch.tensor([obs_next], device=agent.device, dtype=torch.float64)\n",
                "#         obs = obs_next\n",
                "#         rewards.append(reward)\n",
                "#         if RENDER:\n",
                "#             env.render()\n",
                "\n",
                "#     rewards = np.array(rewards)\n",
                "#     total_rewards.append(np.sum(rewards))\n",
                "\n",
                "# plt.plot(total_rewards)\n",
                "# plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "'''\n",
                "\n",
                "learning_rate = 0.0001\n",
                "EPSILON_MIN = 0.05\n",
                "EPS_DECAY = 0.001\n",
                "gamma= 0.99\n",
                "TARGET_UPDATE = 10\n",
                "EPISODES = 10000\n",
                "\n",
                "MEMORY_SIZE = 256\n",
                "D = deque(maxlen=MEMORY_SIZE)\n",
                "MINI_BATCH_SIZE = 16\n",
                "epsilon=1\n",
                "\n",
                "\n",
                "\n",
                "q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "# load same weights as approx\n",
                "q_target.load_state_dict(q_approx.state_dict())\n",
                "q_target.eval()\n",
                "# reset replay memory\n",
                "D = deque(maxlen=MEMORY_SIZE)\n",
                "agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE)\n",
                "for episode in range(EPISODES):\n",
                "\n",
                "    if episode % 1000 == 0:\n",
                "        print('-'*5)\n",
                "        print('Episode = {}'.format(episode))\n",
                "        print('Epsilon = {}'.format(agent.epsilon))\n",
                "        print('-'*5)\n",
                "    # train one episode\n",
                "    agent.train_episode()\n",
                "\n",
                "    # update epsilon value\n",
                "    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "    if agent.epsilon > 0.05 :\n",
                "        agent.epsilon -= (1 / 5000)\n",
                "\n",
                "    # # update target paramters\n",
                "    if episode % TARGET_UPDATE == 0:\n",
                "        q_target.load_state_dict(q_approx.state_dict())\n",
                "        q_target.eval()\n",
                "\n",
                "# shallow copy\n",
                "# rewards_per_episode = agent.train_data.copy()\n",
                "# alias\n",
                "rewards_per_episode = agent.train_data\n",
                "\n",
                "total_rewards = np.sum(rewards_per_episode)\n",
                "print(\"Score over time: {}, gamma={}\".format( total_rewards/EPISODES, gamma))\n",
                "average_reward = total_rewards/EPISODES\n",
                "\n",
                "plt.plot(rewards_per_episode)\n",
                "\n",
                "\n",
                "'''"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'\\n\\nlearning_rate = 0.0001\\nEPSILON_MIN = 0.05\\nEPS_DECAY = 0.001\\ngamma= 0.99\\nTARGET_UPDATE = 10\\nEPISODES = 10000\\n\\nMEMORY_SIZE = 256\\nD = deque(maxlen=MEMORY_SIZE)\\nMINI_BATCH_SIZE = 16\\nepsilon=1\\n\\n\\n\\nq_approx = DQN(lr=learning_rate, device=device).to(device)\\nq_target = DQN(lr=learning_rate, device=device).to(device)\\n# load same weights as approx\\nq_target.load_state_dict(q_approx.state_dict())\\nq_target.eval()\\n# reset replay memory\\nD = deque(maxlen=MEMORY_SIZE)\\nagent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE)\\nfor episode in range(EPISODES):\\n\\n    if episode % 1000 == 0:\\n        print(\\'-\\'*5)\\n        print(\\'Episode = {}\\'.format(episode))\\n        print(\\'Epsilon = {}\\'.format(agent.epsilon))\\n        print(\\'-\\'*5)\\n    # train one episode\\n    agent.train_episode()\\n\\n    # update epsilon value\\n    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\\n    if agent.epsilon > 0.05 :\\n        agent.epsilon -= (1 / 5000)\\n\\n    # # update target paramters\\n    if episode % TARGET_UPDATE == 0:\\n        q_target.load_state_dict(q_approx.state_dict())\\n        q_target.eval()\\n\\n# shallow copy\\n# rewards_per_episode = agent.train_data.copy()\\n# alias\\nrewards_per_episode = agent.train_data\\n\\ntotal_rewards = np.sum(rewards_per_episode)\\nprint(\"Score over time: {}, gamma={}\".format( total_rewards/EPISODES, gamma))\\naverage_reward = total_rewards/EPISODES\\n\\nplt.plot(rewards_per_episode)\\n\\n\\n'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 16
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "f41a9e18d32d699c7ebd9346171aa8606b8eaf6d2e7d29caa03f22c5e982b824"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}