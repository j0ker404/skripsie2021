{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "import gym\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import pickle\n",
                "from collections import deque"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "env_id = 'CartPole-v1'\n",
                "env_id = 'CartPole-v0'\n",
                "env = gym.make(env_id)\n",
                "n_actions = env.action_space.n\n",
                "len_obs_space = env.observation_space.shape[0]\n",
                "\n",
                "print('n_actions = {}'.format(n_actions))\n",
                "print('len_obs_space = {}'.format(len_obs_space))\n",
                "# torch.manual_seed(1423)\n",
                "# if gpu is to be used\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288#:~:text=Cartpole%20%2D%20known%20also%20as%20an,forces%20to%20a%20pivot%20point.\n",
                "\n",
                "\n",
                "https://github.com/gsurma/cartpole"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "class DQN(nn.Module):\n",
                "\n",
                "    def __init__(self, lr, device) -> None:\n",
                "        super().__init__()\n",
                "        torch.manual_seed(1423)\n",
                "        hidden_layer_size = 64\n",
                "        # self.flatten = nn.Flatten()\n",
                "        # self.seq_relu = nn.Sequential(\n",
                "        #     nn.Linear(len_obs_space, hidden_layer_size),\n",
                "        #     nn.ReLU(),\n",
                "        #     nn.Linear(hidden_layer_size, hidden_layer_size),\n",
                "        #     nn.ReLU(),\n",
                "        #     nn.Linear(hidden_layer_size, n_actions)\n",
                "        # )\n",
                "        # self.seq_relu = nn.Sequential(\n",
                "        #     nn.Linear(len_obs_space, hidden_layer_size),\n",
                "        #     nn.ReLU(),\n",
                "        #     nn.Linear(hidden_layer_size, n_actions),\n",
                "        # # )\n",
                "        self.seq = nn.Sequential(\n",
                "            nn.Linear(len_obs_space, hidden_layer_size),\n",
                "            nn.Tanh(),\n",
                "            nn.Linear(hidden_layer_size, n_actions),\n",
                "            nn.Identity()\n",
                "        )\n",
                "        # nn.Linear(len_obs_space, hidden_layer_size)\n",
                "        # nn.Tanh()\n",
                "        # nn.Linear(hidden_layer_size, n_actions)\n",
                "        # nn.Identity()\n",
                "\n",
                "        self.learning_rate = lr\n",
                "        self.device = device\n",
                "\n",
                "        # self.optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
                "        # self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
                "        # # self.optimizer = torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\n",
                "        # self.loss = nn.MSELoss()\n",
                "\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x = self.flatten(x)\n",
                "        logits = self.seq(x)\n",
                "        return logits\n",
                "\n",
                "\n",
                "# learning_rate = 0.05\n",
                "# q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "# q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "# # load same weights as approx\n",
                "# q_target.load_state_dict(q_approx.state_dict())\n",
                "# q_target.eval()\n",
                "# print(q_approx)\n",
                "# print(q_target)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import random\n",
                "import copy\n",
                "import time\n",
                "class Agent:\n",
                "    '''\n",
                "        https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435\n",
                "    '''    \n",
                "    def __init__(self, q_model, env, epsilon, gamma, batch_size, exp_mem_size, lr) -> None:\n",
                "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "        torch.manual_seed(1423)\n",
                "        # torch.manual_seed(int(time.time()))\n",
                "        # torch.seed()\n",
                "        input_dim = env.observation_space.shape[0]\n",
                "        output_dim = env.action_space.n\n",
                "\n",
                "        # self.my_model = q_model\n",
                "        # self.q_model = self.build_nn([input_dim, 64, output_dim])\n",
                "\n",
                "        # self.my_model.load_state_dict(self.q_model.state_dict())\n",
                "\n",
                "        # print('My model params')        \n",
                "        # for index in range(len(self.my_model.state_dict())):\n",
                "        #     # print(param)\n",
                "        #     self.my_model.parameters()[index] =   self.q_model.parameters()[index]\n",
                "\n",
                "        # self.my_model.state_dict()['seq.0.weight'] = self.q_model.state_dict()['0.weight']\n",
                "        # print('Their model params')        \n",
                "        # print(self.q_model.state_dict()['0.weight'])\n",
                "        # for param in self.q_model.parameters():\n",
                "        #     print()\n",
                "\n",
                "        # print('My model params')        \n",
                "        # print(self.my_model.state_dict()['seq.0.weight'])\n",
                "        # for param in self.my_model.parameters():\n",
                "        #     print(param)        \n",
                "\n",
                "        # print('my_model = {}'.format(self.my_model))\n",
                "        # print('their model = {}'.format(self.q_model))\n",
                "\n",
                "        # load same weights as approx\n",
                "        # self.q_target = copy.deepcopy(self.q_model)\n",
                "        # self.q_target = self.build_nn([input_dim, 64, output_dim])\n",
                "\n",
                "        self.q_model = q_model\n",
                "        self.q_target = DQN(learning_rate, device=device)\n",
                "        self.q_target.load_state_dict(self.q_model.state_dict())\n",
                "        self.q_target.eval()\n",
                "        \n",
                "        self.env = env\n",
                "        self.n_actions = self.env.action_space.n\n",
                "        self.epsilon = epsilon\n",
                "        # self.device = self.q_model.device\n",
                "        # self.gamma = gamma\n",
                "        self.gamma = torch.tensor(gamma).float()\n",
                "        self.steps_done = 0\n",
                "        self.train_data = []\n",
                "\n",
                "        self.target_update = 5\n",
                "        self.target_update_counter = 0\n",
                "        self.train_step_count = 0\n",
                "\n",
                "        self.EXP_MEMORY_SIZE = exp_mem_size\n",
                "        self.BATCH_SIZE = batch_size\n",
                "\n",
                "        self.D = deque(maxlen=self.EXP_MEMORY_SIZE)\n",
                "        # self.clear_replay_memory()\n",
                "\n",
                "        self.loss = nn.MSELoss()\n",
                "        self.optimizer = torch.optim.Adam(self.q_model.parameters(), lr=lr)\n",
                "        # self.optimizer = torch.optim.Adam(self.q_model.parameters(), lr=self.q_model.learning_rate)\n",
                "        # self.optimizer = torch.optim.SGD(self.q_model.parameters(), lr=self.q_model.learning_rate)\n",
                "\n",
                "\n",
                "    def build_nn(self, layer_sizes):\n",
                "        assert len(layer_sizes) > 1\n",
                "        layers = []\n",
                "        for index in range(len(layer_sizes) - 1):\n",
                "            linear = nn.Linear(layer_sizes[index], layer_sizes[index + 1])\n",
                "            act = nn.Tanh() if index < len(layer_sizes) - 2 else nn.Identity()\n",
                "            # act = nn.ReLU() if index < len(layer_sizes) - 2 else nn.Identity()\n",
                "            layers += (linear, act)\n",
                "        return nn.Sequential(*layers)\n",
                "\n",
                "    def init_replay_memory(self):\n",
                "        \"\"\"\n",
                "            initialise replay memory\n",
                "        \"\"\"\n",
                "        self.clear_replay_memory()\n",
                "        is_mem_filled = False\n",
                "        counter = 0\n",
                "        for _ in range(self.EXP_MEMORY_SIZE):\n",
                "            # if is_mem_filled:\n",
                "            #     break\n",
                "            done = False\n",
                "            obs = self.env.reset()\n",
                "            # obs = torch.tensor([obs], device=self.device, dtype=torch.float64)\n",
                "            while not done:\n",
                "                # get action to execute based on state\n",
                "                action = self.get_action(obs, self.n_actions, epsilon=1)\n",
                "                \n",
                "                #  take action, go to next time step\n",
                "                obs_next, reward, done, info = self.env.step(action.item())\n",
                "                self.collect_experience([obs, action.item(), reward, obs_next, done])\n",
                "\n",
                "                # create transition\n",
                "                # transition = [obs, action, reward, obs_next, done]\n",
                "                # store transition\n",
                "                # self.D.append(transition)\n",
                "\n",
                "                obs = obs_next\n",
                "                counter += 1\n",
                "                if counter > self.EXP_MEMORY_SIZE:\n",
                "                    break\n",
                "                # is_mem_filled = True if counter > self.EXP_MEMORY_SIZE else False\n",
                "                # if is_mem_filled:\n",
                "                #     break\n",
                "        \n",
                "\n",
                "    def clear_replay_memory(self):\n",
                "        \"\"\"[summary]\n",
                "            \n",
                "            clear experience reply memory\n",
                "        \"\"\" \n",
                "        self.D = deque(maxlen=self.EXP_MEMORY_SIZE)\n",
                "\n",
                "    \n",
                "    def get_target_q_vals(self, obs):\n",
                "        '''\n",
                "            Return the target q-values for given observation\n",
                "\n",
                "            obs: torch shape(n_samples, n_features)\n",
                "        '''\n",
                "        # vals = None\n",
                "        # with torch.no_grad():\n",
                "        #     vals = self.q_target(obs.float()).amax(dim=1)\n",
                "\n",
                "        # return vals\n",
                "        with torch.no_grad():\n",
                "            qp = self.q_target(obs)\n",
                "        q, _ = torch.max(qp, axis=1)\n",
                "        return q\n",
                "\n",
                "    def train_episode(self):\n",
                "        done = False\n",
                "        obs = self.env.reset()\n",
                "        obs = torch.tensor([obs], device=self.device, dtype=torch.float64)\n",
                "        rewards = []\n",
                "        # frame_count = 0\n",
                "        # target_frame_update = 5\n",
                "        while not done:\n",
                "            # get action to execute based on state\n",
                "            action = self.get_action(obs.float())\n",
                "            #  take action, go to next time step\n",
                "            obs_next, reward, done, info = self.env.step(action.item())\n",
                "            rewards.append(reward)\n",
                "            # convert to tensors\n",
                "            obs_next = torch.tensor([obs_next], device=self.device, dtype=torch.float64)\n",
                "            reward = torch.tensor([reward,], device=self.device).reshape((-1,1))\n",
                "            # print(reward.shape)\n",
                "            done = torch.tensor(done, device=self.device, dtype=torch.bool)\n",
                "            \n",
                "            # create transitions\n",
                "            # transition = (obs, action, reward, obs_next, done)\n",
                "            # store transitions\n",
                "            # self.D.append(transition)\n",
                "            self.collect_experience([obs, action.item(), reward, obs_next, done])\n",
                "\n",
                "            self.train_step_count += 1\n",
                "\n",
                "            obs = obs_next\n",
                "\n",
                "            if self.train_step_count > 128:\n",
                "                # print('gf')\n",
                "                self.train_step_count = 0\n",
                "                for _ in range(4):\n",
                "                    # update target\n",
                "                    if self.target_update_counter == self.target_update:\n",
                "                        self.q_target.load_state_dict(self.q_model.state_dict())\n",
                "                        self.q_target.eval()\n",
                "                        self.target_update_counter = 0                        \n",
                "\n",
                "                    # sample minibatch\n",
                "                    # mini_batch = random.choices(self.D, k=self.BATCH_SIZE)\n",
                "                    mini_batch = random.sample(self.D, k=self.BATCH_SIZE)\n",
                "\n",
                "                    obs_batch = tuple([tple[0] for tple in mini_batch])\n",
                "                    # print('obsss')\n",
                "                    # print(' obs_batch = {}'.format(obs_batch))\n",
                "                    obs_batch = torch.cat(obs_batch, dim=0)\n",
                "                    # print('obs_batch = {}'.format(obs_batch.shape))\n",
                "\n",
                "                    action_batch = tuple([tple[1] for tple in mini_batch])\n",
                "                    # action_batch = torch.cat(action_batch, dim=0).reshape(-1,1)\n",
                "                    action_batch = torch.cat(action_batch, dim=0)\n",
                "                    # print('action_batch = \\n{}'.format(action_batch.shape))\n",
                "\n",
                "                    reward_batch = torch.tensor([tple[2] for tple in mini_batch])\n",
                "                    # reward_batch = reward_batch.reshape((-1,1))\n",
                "                    # print('reward_batch = \\n{}'.format(reward_batch.shape))\n",
                "\n",
                "                    obs_next_batch = tuple([tple[3] for tple in mini_batch])\n",
                "                    obs_next_batch = torch.cat(obs_next_batch, dim=0)\n",
                "                    # print('obs_next_batch = \\n{}'.format(obs_next_batch.shape))\n",
                "                \n",
                "                    done_batch = torch.tensor([tple[4] for tple in mini_batch])\n",
                "                    # done_batch = reward_batch.reshape((-1,1))\n",
                "                    # print('done_batch = \\n{}'.format(done_batch.int()))\n",
                "\n",
                "                    # print(action)\n",
                "                    # Compute prediction and loss\n",
                "                    # q_values = self.q_model(obs_batch.float()).gather(1, action_batch)\n",
                "                    q_values = self.q_model(obs_batch.float())\n",
                "                    # pred = q_values[torch.arange(16), action_batch]\n",
                "                    pred, _ = torch.max(q_values, axis=1)\n",
                "                    # target_q_values  = self.q_target(obs_batch.float())\n",
                "                    target_q_values = self.get_target_q_vals(obs_batch.float())\n",
                "                    # print('target_q_values = {}'.format(target_q_values.shape))\n",
                "                    # y = reward_batch + self.gamma*torch.max(target_q_values)*(1-done_batch.int())\n",
                "                    y = reward_batch + self.gamma*torch.max(target_q_values)\n",
                "                    # loss = self.q_model.loss(q_values, y)\n",
                "                    loss = self.q_model.loss(pred, y)\n",
                "                    \n",
                "                    \n",
                "                    \n",
                "                    # Backpropagation\n",
                "                    self.q_model.optimizer.zero_grad()\n",
                "                    loss.backward(retain_graph=True)\n",
                "                    self.q_model.optimizer.step()\n",
                "                    # self.optimizer.zero_grad()\n",
                "                    # loss.backward(retain_graph=True)\n",
                "                    # self.optimizer.step()\n",
                "\n",
                "                    self.target_update_counter += 1\n",
                "                    # print('-'*5)\n",
                "                    # print('obs_batch = {}'.format(obs_batch.shape))\n",
                "                    # print('action_batch = {}'.format(action_batch.shape))\n",
                "                    # print('reward_batch = {}'.format(reward_batch.shape))\n",
                "                    # print('obs_next_batch = {}'.format(obs_next_batch.shape))\n",
                "                    # print('q_values = {}'.format(q_values.shape))\n",
                "                    # print('pred = {}'.format(pred.shape))\n",
                "                    # print('target_q_values = {}'.format(target_q_values.shape))\n",
                "                    # print('y = {}'.format(y.shape))\n",
                "                    # print('loss = {}'.format(loss))\n",
                "                    # print('-'*5)\n",
                "\n",
                "        rewards = np.array(rewards)\n",
                "        self.train_data.append(np.sum(rewards))\n",
                "    \n",
                "    def sample_experience(self, sample_size):\n",
                "        # sample_size = self.BATCH_SIZE\n",
                "        if len(self.D) < sample_size:\n",
                "            sample_size = len(self.D)\n",
                "\n",
                "        mini_batch = random.sample(self.D, sample_size)\n",
                "\n",
                "        obs_batch = torch.tensor([tple[0] for tple in mini_batch]).float()\n",
                "        action_batch = torch.tensor([tple[1] for tple in mini_batch]).float()\n",
                "        reward_batch = torch.tensor([tple[2] for tple in mini_batch]).float()\n",
                "        obs_next_batch = torch.tensor([tple[3] for tple in mini_batch]).float()\n",
                "        done_batch = torch.tensor([tple[4] for tple in mini_batch])\n",
                "\n",
                "        # print('-'*5)\n",
                "        # print('obs_batch = {}'.format(obs_batch.shape))\n",
                "        # print('action_batch = {}'.format(action_batch.shape))\n",
                "        # print('reward_batch = {}'.format(reward_batch.shape))\n",
                "        # print('obs_next_batch = {}'.format(obs_next_batch.shape))\n",
                "        # print('-'*5)\n",
                "        return obs_batch, action_batch, reward_batch, obs_next_batch, done_batch\n",
                "        \n",
                "\n",
                "    def train(self, batch_size):\n",
                "        # sample minibatch\n",
                "        obs_batch, action_batch, reward_batch, obs_next_batch, done_batch = self.sample_experience(batch_size)\n",
                "        \n",
                "        if self.target_update_counter == self.target_update:\n",
                "            self.q_target.load_state_dict(self.q_model.state_dict())\n",
                "            self.q_target.eval()\n",
                "            self.target_update_counter = 0                        \n",
                "\n",
                "        # Compute prediction and loss\n",
                "        q_values = self.q_model(obs_batch)\n",
                "        pred, _ = torch.max(q_values, axis=1)\n",
                "        target_q_values = self.get_target_q_vals(obs_next_batch)\n",
                "        # y = reward_batch + self.gamma*torch.max(target_q_values)*(1-done_batch.int())\n",
                "        # y = reward_batch + self.gamma*torch.max(target_q_values)\n",
                "        y = reward_batch + self.gamma*target_q_values\n",
                "\n",
                "        # loss = self.q_model.loss(pred, y)\n",
                "        loss = self.loss(pred, y)\n",
                "        \n",
                "        \n",
                "        # Backpropagation\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward(retain_graph=True)\n",
                "        self.optimizer.step()\n",
                "\n",
                "        self.target_update_counter += 1\n",
                "\n",
                "        # print('-'*5)\n",
                "        # print('obs_batch = {}'.format(obs_batch.shape))\n",
                "        # print('action_batch = {}'.format(action_batch.shape))\n",
                "        # print('reward_batch = {}'.format(reward_batch.shape))\n",
                "        # print('obs_next_batch = {}'.format(obs_next_batch.shape))\n",
                "        # print('q_values = {}'.format(q_values.shape))\n",
                "        # print('pred = {}'.format(pred.shape))\n",
                "        # print('target_q_values = {}'.format(target_q_values.shape))\n",
                "        # print('y = {}'.format(y.shape))\n",
                "        # print('loss = {}'.format(loss))\n",
                "        # print('-'*5)\n",
                "        return loss.item()\n",
                "\n",
                "    def get_action(self, state, action_space_len, epsilon):\n",
                "        # We do not require gradient at this point, because this function will be used either\n",
                "        # during experience collection or during inference\n",
                "        with torch.no_grad():\n",
                "            # Qp = self.q_net(torch.from_numpy(state).float().cuda())\n",
                "            state_torch = torch.from_numpy(state)\n",
                "            # print(state_torch)\n",
                "            Qp = self.q_model(state_torch.float())\n",
                "            # print(Qp)\n",
                "        Q, A = torch.max(Qp, axis=0)\n",
                "        # print('Q, A = {}, {}'.format(Q, A))\n",
                "        A = A if torch.rand(1, ).item() > epsilon else torch.randint(0, action_space_len, (1,))\n",
                "        return A\n",
                "\n",
                "    def collect_experience(self, experience):\n",
                "        self.D.append(experience)\n",
                "        \n",
                "\n",
                "    # def get_action(self, obs):\n",
                "    #     # sample = random.random()\n",
                "    #     action = None\n",
                "    #     # if sample < self.epsilon:\n",
                "    #     #     action = torch.tensor([random.choice(range(self.n_actions))])\n",
                "    #     # else:\n",
                "    #     #     with torch.no_grad():\n",
                "    #     #         # print(obs)\n",
                "    #     #         state_torch = torch.from_numpy(obs).float()\n",
                "    #     #         q_vals = self.q_model(state_torch)\n",
                "    #     #         # print(q_vals)\n",
                "    #     #         action = torch.argmax(q_vals)\n",
                "    #     with torch.no_grad():\n",
                "    #         # print(obs)\n",
                "    #         state_torch = torch.from_numpy(obs).float()\n",
                "    #         q_vals = self.q_model(state_torch)\n",
                "    #         # print(q_vals)\n",
                "    #         action = torch.argmax(q_vals)\n",
                "        \n",
                "    #     action = torch.tensor([action], device=self.device)\n",
                "    #     action = action if torch.rand(1, ).item() > epsilon else torch.randint(0, self.n_actions, (1,))\n",
                "\n",
                "    #     # print(action.item())\n",
                "    #     return action\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# -------------------------------\n",
                "# hyperparameters\n",
                "TARGET_UPDATE = 10\n",
                "\n",
                "EPSILON_MIN = 0.05\n",
                "EPSILON_START = 1\n",
                "EPS_DECAY = 0.001\n",
                "# EPS_DECAY = 0.0099\n",
                "\n",
                "epsilon = EPSILON_START\n",
                "gamma = 0.999\n",
                "EPISODES = 10000\n",
                "\n",
                "MINI_BATCH_SIZE = 16\n",
                "MEMORY_SIZE = 256\n",
                "# MINI_BATCH_SIZE = 10\n",
                "# MEMORY_SIZE = 100\n",
                "# -------------------------------"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "We need to tune the hyperparams\n",
                "- epsilon (epsilong_min)\n",
                "- gamma (discount rate)\n",
                "- learning rate\n",
                "- target update\n",
                "- mini batch size"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# create range for hyperparams\n",
                "# learning_rates = np.arange(0.0001, 0.0002, 0.00001)\n",
                "# gammas = np.arange(0.98, 1.001, 0.001)\n",
                "gammas = np.array([0.95])\n",
                "learning_rates = np.array([1e-3])\n",
                "\n",
                "print('lr.shape = {}'.format(learning_rates.shape))\n",
                "print('gammas.shape = {}'.format(gammas.shape))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "rewards = np.zeros((len(learning_rates), len(gammas), EPISODES))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "print(rewards.shape)\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "BASE_PATH = \"./dqn_models/\"\n",
                "'''\n",
                "for index_lr, learning_rate in enumerate(learning_rates):\n",
                "    print('-'*5)\n",
                "    print('lr = {}'.format(learning_rate))\n",
                "    for index_gamma, gamma in enumerate(gammas):\n",
                "        q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "        q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "        # load same weights as approx\n",
                "        q_target.load_state_dict(q_approx.state_dict())\n",
                "        q_target.eval()\n",
                "\n",
                "        # print(q_approx)\n",
                "        # print(q_target)\n",
                "        # print(index_gamma)\n",
                "        \n",
                "        agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE, MEMORY_SIZE)\n",
                "\n",
                "        # reset replay memory\n",
                "        agent.init_replay_memory()\n",
                "\n",
                "        for episode in range(EPISODES):\n",
                "\n",
                "            # if episode % 100 == 0:\n",
                "            #     print('Episode = {}'.format(episode))\n",
                "            #     print('Epsilon = {}'.format(agent.epsilon))\n",
                "            # train one episode\n",
                "            agent.train_episode()\n",
                "\n",
                "            # update epsilon value\n",
                "            # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "            if agent.epsilon > 0.05:\n",
                "                agent.epsilon -= (1 / 5000)\n",
                "\n",
                "            # update target paramters\n",
                "            # if episode % TARGET_UPDATE == 0:\n",
                "            #     q_target.load_state_dict(q_approx.state_dict())\n",
                "            #     q_target.eval()\n",
                "        \n",
                "        # shallow copy\n",
                "        # rewards_per_episode = agent.train_data.copy()\n",
                "        # alias\n",
                "        rewards_per_episode = agent.train_data\n",
                "        rewards[index_lr, index_gamma] = rewards_per_episode \n",
                "\n",
                "        total_rewards = np.sum(rewards[index_lr, index_gamma,:])\n",
                "        print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\n",
                "        average_reward = total_rewards/EPISODES\n",
                "\n",
                "        # save models\n",
                "        model_info = str(learning_rate)+ '_' + str(gamma)\n",
                "        path = BASE_PATH + 'model' + '_' + model_info + '.pt'\n",
                "        q_name = 'q_approx_state_dict_' + model_info\n",
                "        optim_name = 'optim_state_dict_' + model_info\n",
                "        model = {\n",
                "            q_name: agent.q_model.state_dict(),\n",
                "            optim_name: agent.q_model.optimizer.state_dict()\n",
                "        }\n",
                "        # model = {\n",
                "        #     q_name: agent.q_model.state_dict(),\n",
                "        #     optim_name: agent.optimizer.state_dict()\n",
                "        # }\n",
                "        torch.save(model, path)\n",
                "    print('-'*5)\n",
                "# save rewards to disk\n",
                "with open(\"./dqn_rewards/dqn_rewards.pkl\",'wb') as f:\n",
                "    pickle.dump(rewards, f)\n",
                "'''"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "'''\n",
                "# plot average reward data\n",
                "# print(rewards[:,:,:].shape)\n",
                "import pandas as pd\n",
                "\n",
                "d = pd.Series(rewards[0,0,:])\n",
                "plt.plot(np.arange(EPISODES),d.rolling(100).mean())\n",
                "plt.xlabel('episodes')\n",
                "plt.ylabel('average reward')\n",
                "# plt.plot(rewards[:,:])\n",
                "'''"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from tqdm import tqdm\n",
                "gamma = 0.95\n",
                "epsilon = 1\n",
                "learning_rate = 1e-3\n",
                "EPISODES = 10000\n",
                "MINI_BATCH_SIZE = 16\n",
                "MEMORY_SIZE = 256\n",
                "# EPISODES = 100\n",
                "rewards = []\n",
                "loss_list = []\n",
                "epsilon_list = []\n",
                "q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "# q_target.eval()\n",
                "\n",
                "# print(q_approx)\n",
                "# print(q_target)\n",
                "# print(index_gamma)\n",
                "\n",
                "agent = Agent(q_approx, env, epsilon, gamma, MINI_BATCH_SIZE,\n",
                "              MEMORY_SIZE, learning_rate)\n",
                "\n",
                "# print(q_approx)\n",
                "# print(agent.q_target)\n",
                "# reset replay memory\n",
                "agent.init_replay_memory()\n",
                "agent.train_step_count = 128\n",
                "for episode in tqdm(range(EPISODES)):\n",
                "\n",
                "    # train one episode\n",
                "    # agent.train_episode()\n",
                "    done = False\n",
                "    obs = agent.env.reset()\n",
                "    rew = 0\n",
                "    losses = 0\n",
                "    while not done:\n",
                "        # get action to execute based on state\n",
                "\n",
                "        action = agent.get_action(obs, agent.n_actions, agent.epsilon)\n",
                "\n",
                "        #  take action, go to next time step\n",
                "        obs_next, reward, done, info = agent.env.step(action.item())\n",
                "\n",
                "        agent.collect_experience([obs, action.item(), reward, obs_next, done])\n",
                "\n",
                "        obs = obs_next\n",
                "        rew += reward\n",
                "        agent.train_step_count += 1\n",
                "\n",
                "        if agent.train_step_count > 128:\n",
                "            agent.train_step_count = 0\n",
                "            for _ in range(4):\n",
                "                loss = agent.train(agent.BATCH_SIZE)\n",
                "                losses += loss\n",
                "    # update epsilon value\n",
                "    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "    if agent.epsilon > 0.05:\n",
                "        agent.epsilon -= (1 / 5000)\n",
                "\n",
                "    rewards.append(rew)\n",
                "    loss_list.append(losses)\n",
                "    epsilon_list.append(agent.epsilon)\n",
                "\n",
                "# total_rewards = np.sum(rewards[0, 0,:])\n",
                "rewards = np.array(rewards)\n",
                "print(rewards.shape)\n",
                "# print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# plot average reward data\n",
                "# print(rewards[:,:,:].shape)\n",
                "import pandas as pd\n",
                "\n",
                "# d = pd.Series(rewards[0,0,:])\n",
                "plt.plot(rewards)\n",
                "plt.xlabel('episodes')\n",
                "plt.ylabel('reward')\n",
                "# plt.plot(rewards[:,:])\n",
                "\n",
                "print('average reward per episode= {}'.format(np.mean(rewards, axis=0)))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "my_dict = {}\n",
                "# my_dict[\"data\"] = \"Their model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\nMy model params\\ntensor([[ 0.3041,  0.2882, -0.2917,  0.4162],\\n        [ 0.1855,  0.4892,  0.0079, -0.1485],\\n        [ 0.2827, -0.1978,  0.0915,  0.1492],\\n        [ 0.2694,  0.2406, -0.0709,  0.0753],\\n        [ 0.1781, -0.0996, -0.4768, -0.0902],\\n        [ 0.0527,  0.3042,  0.4053, -0.4673],\\n        [-0.3728,  0.4554,  0.1156,  0.1509],\\n        [-0.2538,  0.1942, -0.3371,  0.1100],\\n        [-0.3160, -0.2490, -0.2002,  0.1645],\\n        [-0.1380,  0.0902, -0.0432,  0.1049],\\n        [-0.1543,  0.4901,  0.0083,  0.0152],\\n        [ 0.1811, -0.4768,  0.0971,  0.1288],\\n        [ 0.2728,  0.4485, -0.2000,  0.1215],\\n        [ 0.1950, -0.4748,  0.2200,  0.2112],\\n        [-0.2967, -0.2873, -0.3639,  0.4410],\\n        [-0.1727, -0.3969, -0.0300,  0.4692],\\n        [-0.3265, -0.0512,  0.3419, -0.2583],\\n        [-0.0901, -0.2474, -0.0358, -0.0221],\\n        [-0.2907,  0.2076, -0.2116,  0.4516],\\n        [-0.4855, -0.2546, -0.3364, -0.3318],\\n        [-0.3998, -0.0230, -0.2552, -0.2768],\\n        [-0.3959,  0.0057,  0.4193,  0.2657],\\n        [-0.1441,  0.2253,  0.4854, -0.1122],\\n        [-0.1419,  0.1340, -0.2012,  0.2925],\\n        [ 0.3803,  0.3698, -0.4950, -0.1462],\\n        [-0.1645, -0.4683,  0.1710, -0.0956],\\n        [ 0.0400,  0.0339,  0.3003,  0.3645],\\n        [ 0.4606, -0.0104, -0.0227,  0.3237],\\n        [ 0.4904,  0.4335, -0.2649,  0.2615],\\n        [ 0.1789, -0.3907, -0.1692, -0.2415],\\n        [ 0.2577,  0.3993,  0.3816, -0.0371],\\n        [-0.0268,  0.4042,  0.2384,  0.4170],\\n        [ 0.3716, -0.1569, -0.2289, -0.4692],\\n        [-0.3748, -0.0156, -0.1977,  0.2151],\\n        [ 0.1638,  0.2755,  0.2921, -0.4573],\\n        [ 0.3901, -0.0634, -0.2933, -0.1968],\\n        [ 0.4970, -0.1262,  0.3650,  0.4007],\\n        [ 0.2788,  0.1445,  0.0942,  0.1048],\\n        [-0.3838, -0.4978, -0.4175, -0.1444],\\n        [-0.2990,  0.0109,  0.2287,  0.1491],\\n        [-0.2714,  0.1556, -0.0731,  0.4427],\\n        [-0.4720, -0.1670, -0.1322,  0.4997],\\n        [-0.0374, -0.2431, -0.4579, -0.1824],\\n        [ 0.2399, -0.1023, -0.0905, -0.0190],\\n        [-0.1580,  0.1126, -0.1087, -0.4724],\\n        [-0.0587, -0.2401,  0.3096,  0.0515],\\n        [-0.1888,  0.1881, -0.2436, -0.3915],\\n        [-0.0538, -0.3004,  0.4868,  0.4518],\\n        [ 0.4151,  0.3819, -0.4865, -0.2404],\\n        [-0.4273,  0.0289, -0.3914, -0.1450],\\n        [ 0.4119,  0.4608, -0.0953, -0.2822],\\n        [-0.2009, -0.2225,  0.3750,  0.1481],\\n        [ 0.4009,  0.2628, -0.4354,  0.4003],\\n        [ 0.1444,  0.0308,  0.4046,  0.0810],\\n        [ 0.0310,  0.0230, -0.3415, -0.2876],\\n        [ 0.0175, -0.4231, -0.2572, -0.4341],\\n        [ 0.1194, -0.0733, -0.1846,  0.3522],\\n        [-0.2700, -0.0280,  0.4058,  0.1455],\\n        [-0.1910, -0.2753, -0.4003, -0.1842],\\n        [-0.2268, -0.4600, -0.1236, -0.1261],\\n        [-0.3728, -0.3369,  0.4468,  0.1672],\\n        [ 0.4776, -0.4104,  0.1595,  0.0460],\\n        [-0.1318,  0.4914, -0.4923,  0.3753],\\n        [ 0.2644, -0.2238,  0.1891,  0.0529]])\\ntheir model = Sequential(\\n  (0): Linear(in_features=4, out_features=64, bias=True)\\n  (1): Tanh()\\n  (2): Linear(in_features=64, out_features=2, bias=True)\\n  (3): Identity()\\n)\\n\"\n",
                "# \"Their model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\nMy model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\n\"\n",
                "my_dict[\"data\"] = \"Their model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\nMy model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\n\"\n",
                "\n",
                "print(my_dict[\"data\"])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# for param in q_approx.parameters():\n",
                "#     print(param.size())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# RENDER = False\n",
                "# total_rewards = []\n",
                "# for episode in range(EPISODES):\n",
                "#     done = False\n",
                "#     obs = agent.env.reset()\n",
                "#     obs = torch.tensor([obs], device=agent.device, dtype=torch.float64)\n",
                "#     rewards = []\n",
                "#     while not done:\n",
                "#         # get action to execute based on state\n",
                "#         action = agent.get_action(obs.float())\n",
                "#         #  take action, go to next time step\n",
                "#         obs_next, reward, done, info = agent.env.step(action.item())\n",
                "#         obs_next = torch.tensor([obs_next], device=agent.device, dtype=torch.float64)\n",
                "#         obs = obs_next\n",
                "#         rewards.append(reward)\n",
                "#         if RENDER:\n",
                "#             env.render()\n",
                "\n",
                "#     rewards = np.array(rewards)\n",
                "#     total_rewards.append(np.sum(rewards))\n",
                "\n",
                "# plt.plot(total_rewards)\n",
                "# plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "'''\n",
                "\n",
                "learning_rate = 0.0001\n",
                "EPSILON_MIN = 0.05\n",
                "EPS_DECAY = 0.001\n",
                "gamma= 0.99\n",
                "TARGET_UPDATE = 10\n",
                "EPISODES = 10000\n",
                "\n",
                "MEMORY_SIZE = 256\n",
                "D = deque(maxlen=MEMORY_SIZE)\n",
                "MINI_BATCH_SIZE = 16\n",
                "epsilon=1\n",
                "\n",
                "\n",
                "\n",
                "q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "# load same weights as approx\n",
                "q_target.load_state_dict(q_approx.state_dict())\n",
                "q_target.eval()\n",
                "# reset replay memory\n",
                "D = deque(maxlen=MEMORY_SIZE)\n",
                "agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE)\n",
                "for episode in range(EPISODES):\n",
                "\n",
                "    if episode % 1000 == 0:\n",
                "        print('-'*5)\n",
                "        print('Episode = {}'.format(episode))\n",
                "        print('Epsilon = {}'.format(agent.epsilon))\n",
                "        print('-'*5)\n",
                "    # train one episode\n",
                "    agent.train_episode()\n",
                "\n",
                "    # update epsilon value\n",
                "    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "    if agent.epsilon > 0.05 :\n",
                "        agent.epsilon -= (1 / 5000)\n",
                "\n",
                "    # # update target paramters\n",
                "    if episode % TARGET_UPDATE == 0:\n",
                "        q_target.load_state_dict(q_approx.state_dict())\n",
                "        q_target.eval()\n",
                "\n",
                "# shallow copy\n",
                "# rewards_per_episode = agent.train_data.copy()\n",
                "# alias\n",
                "rewards_per_episode = agent.train_data\n",
                "\n",
                "total_rewards = np.sum(rewards_per_episode)\n",
                "print(\"Score over time: {}, gamma={}\".format( total_rewards/EPISODES, gamma))\n",
                "average_reward = total_rewards/EPISODES\n",
                "\n",
                "plt.plot(rewards_per_episode)\n",
                "\n",
                "\n",
                "'''"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "f41a9e18d32d699c7ebd9346171aa8606b8eaf6d2e7d29caa03f22c5e982b824"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}