{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "import gym\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import pickle\n",
                "from collections import deque"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "env_id = 'CartPole-v1'\n",
                "env_id = 'CartPole-v0'\n",
                "env = gym.make(env_id)\n",
                "n_actions = env.action_space.n\n",
                "len_obs_space = env.observation_space.shape[0]\n",
                "\n",
                "print('n_actions = {}'.format(n_actions))\n",
                "print('len_obs_space = {}'.format(len_obs_space))\n",
                "# torch.manual_seed(1423)\n",
                "# if gpu is to be used\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "n_actions = 2\n",
                        "len_obs_space = 4\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288#:~:text=Cartpole%20%2D%20known%20also%20as%20an,forces%20to%20a%20pivot%20point.\n",
                "\n",
                "\n",
                "https://github.com/gsurma/cartpole"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "class DQN(nn.Module):\n",
                "\n",
                "    def __init__(self, lr, device, seed=1423) -> None:\n",
                "        super().__init__()\n",
                "        torch.manual_seed(seed)\n",
                "        hidden_layer_size = 64\n",
                "        # self.flatten = nn.Flatten()\n",
                "        # self.seq_relu = nn.Sequential(\n",
                "        #     nn.Linear(len_obs_space, hidden_layer_size),\n",
                "        #     nn.ReLU(),\n",
                "        #     nn.Linear(hidden_layer_size, hidden_layer_size),\n",
                "        #     nn.ReLU(),\n",
                "        #     nn.Linear(hidden_layer_size, n_actions)\n",
                "        # )\n",
                "        # self.seq_relu = nn.Sequential(\n",
                "        #     nn.Linear(len_obs_space, hidden_layer_size),\n",
                "        #     nn.ReLU(),\n",
                "        #     nn.Linear(hidden_layer_size, n_actions),\n",
                "        # # )\n",
                "        # self.seq = nn.Sequential(\n",
                "        #     nn.Linear(len_obs_space, hidden_layer_size),\n",
                "        #     nn.Tanh(),\n",
                "        #     nn.Linear(hidden_layer_size, n_actions),\n",
                "        #     nn.Identity()\n",
                "        # )\n",
                "        self.seq = nn.Sequential(\n",
                "            nn.Linear(len_obs_space, hidden_layer_size),\n",
                "            nn.Tanh(),\n",
                "            nn.Linear(hidden_layer_size, n_actions)\n",
                "        )\n",
                "        # nn.Linear(len_obs_space, hidden_layer_size)\n",
                "        # nn.Tanh()\n",
                "        # nn.Linear(hidden_layer_size, n_actions)\n",
                "        # nn.Identity()\n",
                "\n",
                "        self.learning_rate = lr\n",
                "        self.device = device\n",
                "\n",
                "        # self.optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
                "        # self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
                "        # # self.optimizer = torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\n",
                "        # self.loss = nn.MSELoss()\n",
                "\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x = self.flatten(x)\n",
                "        logits = self.seq(x)\n",
                "        return logits\n",
                "\n",
                "\n",
                "# learning_rate = 0.05\n",
                "# q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "# q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "# # load same weights as approx\n",
                "# q_target.load_state_dict(q_approx.state_dict())\n",
                "# q_target.eval()\n",
                "# print(q_approx)\n",
                "# print(q_target)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "import random\n",
                "import copy\n",
                "import time\n",
                "\n",
                "\n",
                "class Agent:\n",
                "    '''\n",
                "        https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435\n",
                "    '''\n",
                "\n",
                "    def __init__(self, q_model, env, epsilon, gamma, batch_size, exp_mem_size, lr, seed=1423) -> None:\n",
                "        # self.device = torch.device(\n",
                "        #     \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "        self.device = q_model.device\n",
                "\n",
                "        torch.manual_seed(seed)\n",
                "        # torch.manual_seed(int(time.time()))\n",
                "        # torch.seed()\n",
                "        input_dim = env.observation_space.shape[0]\n",
                "        output_dim = env.action_space.n\n",
                "\n",
                "        # self.my_model = q_model\n",
                "        # self.q_model = self.build_nn([input_dim, 64, output_dim])\n",
                "\n",
                "        # self.my_model.load_state_dict(self.q_model.state_dict())\n",
                "\n",
                "        # print('My model params')\n",
                "        # for index in range(len(self.my_model.state_dict())):\n",
                "        #     # print(param)\n",
                "        #     self.my_model.parameters()[index] =   self.q_model.parameters()[index]\n",
                "\n",
                "        # self.my_model.state_dict()['seq.0.weight'] = self.q_model.state_dict()['0.weight']\n",
                "        # print('Their model params')\n",
                "        # print(self.q_model.state_dict()['0.weight'])\n",
                "        # for param in self.q_model.parameters():\n",
                "        #     print()\n",
                "\n",
                "        # print('My model params')\n",
                "        # print(self.my_model.state_dict()['seq.0.weight'])\n",
                "        # for param in self.my_model.parameters():\n",
                "        #     print(param)\n",
                "\n",
                "        # print('my_model = {}'.format(self.my_model))\n",
                "        # print('their model = {}'.format(self.q_model))\n",
                "\n",
                "        # load same weights as approx\n",
                "        # self.q_target = copy.deepcopy(self.q_model)\n",
                "        # self.q_target = self.build_nn([input_dim, 64, output_dim])\n",
                "\n",
                "        self.q_model = q_model\n",
                "        self.q_target = DQN(learning_rate, device=device)\n",
                "        self.q_target.load_state_dict(self.q_model.state_dict())\n",
                "        self.q_target.eval()\n",
                "\n",
                "        self.env = env\n",
                "        self.n_actions = self.env.action_space.n\n",
                "        self.epsilon = epsilon\n",
                "        # self.device = self.q_model.device\n",
                "        # self.gamma = gamma\n",
                "        self.gamma = torch.tensor(gamma).float()\n",
                "        self.steps_done = 0\n",
                "        self.train_data = []\n",
                "\n",
                "        self.target_update = 5\n",
                "        self.target_update_counter = 0\n",
                "        self.train_step_count = 0\n",
                "        self.OPTIMIZE_COUNT = 4 \n",
                "\n",
                "        self.EXP_MEMORY_SIZE = exp_mem_size\n",
                "        self.BATCH_SIZE = batch_size\n",
                "\n",
                "        self.D = deque(maxlen=self.EXP_MEMORY_SIZE)\n",
                "        # self.clear_replay_memory()\n",
                "\n",
                "        self.loss = nn.MSELoss()\n",
                "        self.optimizer = torch.optim.Adam(self.q_model.parameters(), lr=lr)\n",
                "        # self.optimizer = torch.optim.Adam(self.q_model.parameters(), lr=self.q_model.learning_rate)\n",
                "        # self.optimizer = torch.optim.SGD(self.q_model.parameters(), lr=self.q_model.learning_rate)\n",
                "\n",
                "\n",
                "    def init_replay_memory(self):\n",
                "        \"\"\"\n",
                "            initialise replay memory\n",
                "        \"\"\"\n",
                "        self.clear_replay_memory()\n",
                "        counter = 0\n",
                "        for _ in range(self.EXP_MEMORY_SIZE):\n",
                "            # if is_mem_filled:\n",
                "            #     break\n",
                "            done = False\n",
                "            obs = self.env.reset()\n",
                "            # obs = torch.tensor([obs], device=self.device, dtype=torch.float64)\n",
                "            while not done:\n",
                "                # get action to execute based on state\n",
                "                action = self.get_action(obs)\n",
                "\n",
                "                #  take action, go to next time step\n",
                "                obs_next, reward, done, info = self.env.step(action.item())\n",
                "                self.collect_experience(\n",
                "                    [obs, action.item(), reward, obs_next, done])\n",
                "\n",
                "                obs = obs_next\n",
                "                counter += 1\n",
                "                if counter > self.EXP_MEMORY_SIZE:\n",
                "                    break\n",
                "\n",
                "    def clear_replay_memory(self):\n",
                "        \"\"\"[summary]\n",
                "\n",
                "            clear experience reply memory\n",
                "        \"\"\"\n",
                "        self.D = deque(maxlen=self.EXP_MEMORY_SIZE)\n",
                "\n",
                "    def get_target_q_vals(self, obs):\n",
                "        '''\n",
                "            Return the target  q-values for given observation\n",
                "\n",
                "            pause grad operation\n",
                "\n",
                "            obs: torch shape(n_samples, n_features)\n",
                "        '''\n",
                "\n",
                "        with torch.no_grad():\n",
                "            q_vals = self.q_target(obs)\n",
                "\n",
                "        return q_vals\n",
                "\n",
                "    def get_max_target_q_vals(self, obs):\n",
                "        \"\"\"return the maximum q_values from target model\n",
                "        Args:\n",
                "            obs (torch.Tensor): observed state\n",
                "        \"\"\"\n",
                "        q_vals = self.get_target_q_vals(obs)\n",
                "        q_max, max_indices = torch.max(q_vals, dim=1)\n",
                "\n",
                "        return q_max\n",
                "\n",
                "    def episode_train(self):\n",
                "\n",
                "        done = False\n",
                "        obs = self.env.reset()\n",
                "        rew = 0\n",
                "        losses = 0\n",
                "        # data: [total reward for episode , total loss for]\n",
                "        t = 0\n",
                "        action = self.get_action(obs)\n",
                "        while not done:\n",
                "\n",
                "            # perform one time step action\n",
                "            done, reward, obs_next, action_next, other_data =  self.forward(obs, action)\n",
                "            \n",
                "            # update next observation\n",
                "            obs = obs_next\n",
                "            # update next action\n",
                "            action = action_next\n",
                "\n",
                "            # update reward and loss data\n",
                "            rew += reward\n",
                "            losses += other_data['loss']\n",
                "        \n",
                "            # increment time step\n",
                "            t += 1\n",
                "\n",
                "        return t, [rew, losses]\n",
                "\n",
                "    def forward(self, obs, action):\n",
                "        '''\n",
                "            Perform one time step forward pass for \n",
                "            agent\n",
                "\n",
                "            return done, reward, obs_next, action_next, other_data\n",
                "\n",
                "            returns next action to be taken with obs_next\n",
                "        '''\n",
                "        done = None\n",
                "        reward = None\n",
                "        obs_next = None\n",
                "        action_next = None\n",
                "        other_data = {}\n",
                "        loss = 0\n",
                "\n",
                "        #  take action, go to next time step\n",
                "        obs_next, reward, done, info = self.env.step(action.item())\n",
                "\n",
                "        self.collect_experience([obs, action.item(), reward, obs_next, done])\n",
                "        \n",
                "        # get action to execute based on state\n",
                "        action_next = self.get_action(obs_next)\n",
                "\n",
                "        self.train_step_count += 1\n",
                "\n",
                "        if self.train_step_count > 128:\n",
                "            self.train_step_count = 0\n",
                "            for _ in range(self.OPTIMIZE_COUNT):\n",
                "                loss = self.optimize_model(self.BATCH_SIZE)\n",
                "\n",
                "        other_data['loss'] = loss\n",
                "        other_data['info'] = info\n",
                "\n",
                "        return done, reward, obs_next, action_next, other_data\n",
                "\n",
                "    \n",
                "\n",
                "    def sample_experience(self, sample_size):\n",
                "        # sample_size = self.BATCH_SIZE\n",
                "        if len(self.D) < sample_size:\n",
                "            sample_size = len(self.D)\n",
                "\n",
                "        mini_batch = random.sample(self.D, sample_size)\n",
                "\n",
                "        obs_batch = torch.tensor([tple[0] for tple in mini_batch]).float()\n",
                "        action_batch = torch.tensor([tple[1] for tple in mini_batch]).float()\n",
                "        reward_batch = torch.tensor([tple[2] for tple in mini_batch]).float()\n",
                "        obs_next_batch = torch.tensor([tple[3] for tple in mini_batch]).float()\n",
                "        done_batch = torch.tensor([tple[4] for tple in mini_batch])\n",
                "\n",
                "        return obs_batch, action_batch, reward_batch, obs_next_batch, done_batch\n",
                "\n",
                "    def optimize_model(self, batch_size):\n",
                "        # sample minibatch\n",
                "        obs_batch, action_batch, reward_batch, obs_next_batch, done_batch = self.sample_experience(\n",
                "            batch_size)\n",
                "\n",
                "        if self.target_update_counter == self.target_update:\n",
                "            self.q_target.load_state_dict(self.q_model.state_dict())\n",
                "            self.q_target.eval()\n",
                "            self.target_update_counter = 0\n",
                "\n",
                "        # Compute prediction and loss\n",
                "        q_values = self.q_model(obs_batch)\n",
                "        pred, _ = torch.max(q_values, axis=1)\n",
                "        target_q_values = self.get_target_q_vals(obs_next_batch)\n",
                "        target_max_q_values = self.get_max_target_q_vals(obs_next_batch)\n",
                "        # y = reward_batch + self.gamma*torch.max(target_q_values)*(1-done_batch.int())\n",
                "        # y = reward_batch + self.gamma*torch.max(target_q_values)\n",
                "        # y = reward_batch + self.gamma*target_q_values\n",
                "        y = reward_batch + self.gamma*target_max_q_values\n",
                "        # y = reward_batch + self.gamma*target_max_q_values*(1-done_batch.int())\n",
                "\n",
                "        # loss = self.q_model.loss(pred, y)\n",
                "        loss = self.loss(pred, y)\n",
                "\n",
                "        # Backpropagation\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward(retain_graph=True)\n",
                "        self.optimizer.step()\n",
                "\n",
                "        self.target_update_counter += 1\n",
                "\n",
                "\n",
                "        return loss.item()\n",
                "\n",
                "    # def get_action(self, state):\n",
                "    #     # We do not require gradient at this point, because this function will be used either\n",
                "    #     # during experience collection or during inference\n",
                "    #     with torch.no_grad():\n",
                "    #         # Qp = self.q_net(torch.from_numpy(state).float().cuda())\n",
                "    #         state_torch = torch.from_numpy(state)\n",
                "    #         # print(state_torch)\n",
                "    #         Qp = self.q_model(state_torch.float())\n",
                "    #         # print(Qp)\n",
                "    #     Q, A = torch.max(Qp, axis=0)\n",
                "    #     # print('Q, A = {}, {}'.format(Q, A))\n",
                "    #     A = A if torch.rand(1, ).item() > agent.epsilon else torch.randint(\n",
                "    #         0, agent.n_actions, (1,))\n",
                "    #     return A\n",
                "\n",
                "    def collect_experience(self, experience):\n",
                "        self.D.append(experience)\n",
                "\n",
                "    def get_action(self, obs):\n",
                "        sample = torch.rand((1,))\n",
                "        action = None\n",
                "        if sample.item() < self.epsilon:\n",
                "            action = torch.randint(0, self.n_actions, (1,))\n",
                "        else:\n",
                "            with torch.no_grad():\n",
                "                # print(obs)\n",
                "                state_torch = torch.from_numpy(obs).float()\n",
                "                q_vals = self.q_model(state_torch)\n",
                "                action = torch.argmax(q_vals)\n",
                "        return action\n",
                "\n",
                "\n",
                "\n",
                "# def train_episode(self):\n",
                "    #     done = False\n",
                "    #     obs = self.env.reset()\n",
                "    #     obs = torch.tensor([obs], device=self.device, dtype=torch.float64)\n",
                "    #     rewards = []\n",
                "    #     # frame_count = 0\n",
                "    #     # target_frame_update = 5\n",
                "    #     while not done:\n",
                "    #         # get action to execute based on state\n",
                "    #         action = self.get_action(obs.float())\n",
                "    #         #  take action, go to next time step\n",
                "    #         obs_next, reward, done, info = self.env.step(action.item())\n",
                "    #         rewards.append(reward)\n",
                "    #         # convert to tensors\n",
                "    #         obs_next = torch.tensor([obs_next], device=self.device, dtype=torch.float64)\n",
                "    #         reward = torch.tensor([reward,], device=self.device).reshape((-1,1))\n",
                "    #         # print(reward.shape)\n",
                "    #         done = torch.tensor(done, device=self.device, dtype=torch.bool)\n",
                "\n",
                "    #         # create transitions\n",
                "    #         # transition = (obs, action, reward, obs_next, done)\n",
                "    #         # store transitions\n",
                "    #         # self.D.append(transition)\n",
                "    #         self.collect_experience([obs, action.item(), reward, obs_next, done])\n",
                "\n",
                "    #         self.train_step_count += 1\n",
                "\n",
                "    #         obs = obs_next\n",
                "\n",
                "    #         if self.train_step_count > 128:\n",
                "    #             # print('gf')\n",
                "    #             self.train_step_count = 0\n",
                "    #             for _ in range(4):\n",
                "    #                 # update target\n",
                "    #                 if self.target_update_counter == self.target_update:\n",
                "    #                     self.q_target.load_state_dict(self.q_model.state_dict())\n",
                "    #                     self.q_target.eval()\n",
                "    #                     self.target_update_counter = 0\n",
                "\n",
                "    #                 # sample minibatch\n",
                "    #                 # mini_batch = random.choices(self.D, k=self.BATCH_SIZE)\n",
                "    #                 mini_batch = random.sample(self.D, k=self.BATCH_SIZE)\n",
                "\n",
                "    #                 obs_batch = tuple([tple[0] for tple in mini_batch])\n",
                "    #                 # print('obsss')\n",
                "    #                 # print(' obs_batch = {}'.format(obs_batch))\n",
                "    #                 obs_batch = torch.cat(obs_batch, dim=0)\n",
                "    #                 # print('obs_batch = {}'.format(obs_batch.shape))\n",
                "\n",
                "    #                 action_batch = tuple([tple[1] for tple in mini_batch])\n",
                "    #                 # action_batch = torch.cat(action_batch, dim=0).reshape(-1,1)\n",
                "    #                 action_batch = torch.cat(action_batch, dim=0)\n",
                "    #                 # print('action_batch = \\n{}'.format(action_batch.shape))\n",
                "\n",
                "    #                 reward_batch = torch.tensor([tple[2] for tple in mini_batch])\n",
                "    #                 # reward_batch = reward_batch.reshape((-1,1))\n",
                "    #                 # print('reward_batch = \\n{}'.format(reward_batch.shape))\n",
                "\n",
                "    #                 obs_next_batch = tuple([tple[3] for tple in mini_batch])\n",
                "    #                 obs_next_batch = torch.cat(obs_next_batch, dim=0)\n",
                "    #                 # print('obs_next_batch = \\n{}'.format(obs_next_batch.shape))\n",
                "\n",
                "    #                 done_batch = torch.tensor([tple[4] for tple in mini_batch])\n",
                "    #                 # done_batch = reward_batch.reshape((-1,1))\n",
                "    #                 # print('done_batch = \\n{}'.format(done_batch.int()))\n",
                "\n",
                "    #                 # print(action)\n",
                "    #                 # Compute prediction and loss\n",
                "    #                 # q_values = self.q_model(obs_batch.float()).gather(1, action_batch)\n",
                "    #                 q_values = self.q_model(obs_batch.float())\n",
                "    #                 # pred = q_values[torch.arange(16), action_batch]\n",
                "    #                 pred, _ = torch.max(q_values, axis=1)\n",
                "    #                 # target_q_values  = self.q_target(obs_batch.float())\n",
                "    #                 target_q_values = self.get_target_q_vals(obs_batch.float())\n",
                "    #                 # print('target_q_values = {}'.format(target_q_values.shape))\n",
                "    #                 # y = reward_batch + self.gamma*torch.max(target_q_values)*(1-done_batch.int())\n",
                "    #                 y = reward_batch + self.gamma*torch.max(target_q_values)\n",
                "    #                 # loss = self.q_model.loss(q_values, y)\n",
                "    #                 loss = self.q_model.loss(pred, y)\n",
                "\n",
                "    #                 # Backpropagation\n",
                "    #                 self.q_model.optimizer.zero_grad()\n",
                "    #                 loss.backward(retain_graph=True)\n",
                "    #                 self.q_model.optimizer.step()\n",
                "    #                 # self.optimizer.zero_grad()\n",
                "    #                 # loss.backward(retain_graph=True)\n",
                "    #                 # self.optimizer.step()\n",
                "\n",
                "    #                 self.target_update_counter += 1\n",
                "    #                 # print('-'*5)\n",
                "    #                 # print('obs_batch = {}'.format(obs_batch.shape))\n",
                "    #                 # print('action_batch = {}'.format(action_batch.shape))\n",
                "    #                 # print('reward_batch = {}'.format(reward_batch.shape))\n",
                "    #                 # print('obs_next_batch = {}'.format(obs_next_batch.shape))\n",
                "    #                 # print('q_values = {}'.format(q_values.shape))\n",
                "    #                 # print('pred = {}'.format(pred.shape))\n",
                "    #                 # print('target_q_values = {}'.format(target_q_values.shape))\n",
                "    #                 # print('y = {}'.format(y.shape))\n",
                "    #                 # print('loss = {}'.format(loss))\n",
                "    #                 # print('-'*5)\n",
                "\n",
                "    #     rewards = np.array(rewards)\n",
                "    #     self.train_data.append(np.sum(rewards))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "# -------------------------------\n",
                "# hyperparameters\n",
                "TARGET_UPDATE = 10\n",
                "\n",
                "EPSILON_MIN = 0.05\n",
                "EPSILON_START = 1\n",
                "EPS_DECAY = 0.001\n",
                "# EPS_DECAY = 0.0099\n",
                "\n",
                "epsilon = EPSILON_START\n",
                "gamma = 0.999\n",
                "EPISODES = 10000\n",
                "\n",
                "MINI_BATCH_SIZE = 16\n",
                "MEMORY_SIZE = 256\n",
                "# MINI_BATCH_SIZE = 10\n",
                "# MEMORY_SIZE = 100\n",
                "# -------------------------------"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "We need to tune the hyperparams\n",
                "- epsilon (epsilong_min)\n",
                "- gamma (discount rate)\n",
                "- learning rate\n",
                "- target update\n",
                "- mini batch size"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "source": [
                "# create range for hyperparams\n",
                "# learning_rates = np.arange(0.0001, 0.0002, 0.00001)\n",
                "# gammas = np.arange(0.98, 1.001, 0.001)\n",
                "gammas = np.array([0.95])\n",
                "learning_rates = np.array([1e-3])\n",
                "\n",
                "print('lr.shape = {}'.format(learning_rates.shape))\n",
                "print('gammas.shape = {}'.format(gammas.shape))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "lr.shape = (1,)\n",
                        "gammas.shape = (1,)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "rewards = np.zeros((len(learning_rates), len(gammas), EPISODES))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "source": [
                "print(rewards.shape)\n",
                "\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(1, 1, 10000)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "source": [
                "BASE_PATH = \"./dqn_models/\"\n",
                "'''\n",
                "for index_lr, learning_rate in enumerate(learning_rates):\n",
                "    print('-'*5)\n",
                "    print('lr = {}'.format(learning_rate))\n",
                "    for index_gamma, gamma in enumerate(gammas):\n",
                "        q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "        q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "        # load same weights as approx\n",
                "        q_target.load_state_dict(q_approx.state_dict())\n",
                "        q_target.eval()\n",
                "\n",
                "        # print(q_approx)\n",
                "        # print(q_target)\n",
                "        # print(index_gamma)\n",
                "        \n",
                "        agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE, MEMORY_SIZE)\n",
                "\n",
                "        # reset replay memory\n",
                "        agent.init_replay_memory()\n",
                "\n",
                "        for episode in range(EPISODES):\n",
                "\n",
                "            # if episode % 100 == 0:\n",
                "            #     print('Episode = {}'.format(episode))\n",
                "            #     print('Epsilon = {}'.format(agent.epsilon))\n",
                "            # train one episode\n",
                "            agent.train_episode()\n",
                "\n",
                "            # update epsilon value\n",
                "            # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "            if agent.epsilon > 0.05:\n",
                "                agent.epsilon -= (1 / 5000)\n",
                "\n",
                "            # update target paramters\n",
                "            # if episode % TARGET_UPDATE == 0:\n",
                "            #     q_target.load_state_dict(q_approx.state_dict())\n",
                "            #     q_target.eval()\n",
                "        \n",
                "        # shallow copy\n",
                "        # rewards_per_episode = agent.train_data.copy()\n",
                "        # alias\n",
                "        rewards_per_episode = agent.train_data\n",
                "        rewards[index_lr, index_gamma] = rewards_per_episode \n",
                "\n",
                "        total_rewards = np.sum(rewards[index_lr, index_gamma,:])\n",
                "        print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\n",
                "        average_reward = total_rewards/EPISODES\n",
                "\n",
                "        # save models\n",
                "        model_info = str(learning_rate)+ '_' + str(gamma)\n",
                "        path = BASE_PATH + 'model' + '_' + model_info + '.pt'\n",
                "        q_name = 'q_approx_state_dict_' + model_info\n",
                "        optim_name = 'optim_state_dict_' + model_info\n",
                "        model = {\n",
                "            q_name: agent.q_model.state_dict(),\n",
                "            optim_name: agent.q_model.optimizer.state_dict()\n",
                "        }\n",
                "        # model = {\n",
                "        #     q_name: agent.q_model.state_dict(),\n",
                "        #     optim_name: agent.optimizer.state_dict()\n",
                "        # }\n",
                "        torch.save(model, path)\n",
                "    print('-'*5)\n",
                "# save rewards to disk\n",
                "with open(\"./dqn_rewards/dqn_rewards.pkl\",'wb') as f:\n",
                "    pickle.dump(rewards, f)\n",
                "'''"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'\\nfor index_lr, learning_rate in enumerate(learning_rates):\\n    print(\\'-\\'*5)\\n    print(\\'lr = {}\\'.format(learning_rate))\\n    for index_gamma, gamma in enumerate(gammas):\\n        q_approx = DQN(lr=learning_rate, device=device).to(device)\\n        q_target = DQN(lr=learning_rate, device=device).to(device)\\n        # load same weights as approx\\n        q_target.load_state_dict(q_approx.state_dict())\\n        q_target.eval()\\n\\n        # print(q_approx)\\n        # print(q_target)\\n        # print(index_gamma)\\n        \\n        agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE, MEMORY_SIZE)\\n\\n        # reset replay memory\\n        agent.init_replay_memory()\\n\\n        for episode in range(EPISODES):\\n\\n            # if episode % 100 == 0:\\n            #     print(\\'Episode = {}\\'.format(episode))\\n            #     print(\\'Epsilon = {}\\'.format(agent.epsilon))\\n            # train one episode\\n            agent.train_episode()\\n\\n            # update epsilon value\\n            # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\\n            if agent.epsilon > 0.05:\\n                agent.epsilon -= (1 / 5000)\\n\\n            # update target paramters\\n            # if episode % TARGET_UPDATE == 0:\\n            #     q_target.load_state_dict(q_approx.state_dict())\\n            #     q_target.eval()\\n        \\n        # shallow copy\\n        # rewards_per_episode = agent.train_data.copy()\\n        # alias\\n        rewards_per_episode = agent.train_data\\n        rewards[index_lr, index_gamma] = rewards_per_episode \\n\\n        total_rewards = np.sum(rewards[index_lr, index_gamma,:])\\n        print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\\n        average_reward = total_rewards/EPISODES\\n\\n        # save models\\n        model_info = str(learning_rate)+ \\'_\\' + str(gamma)\\n        path = BASE_PATH + \\'model\\' + \\'_\\' + model_info + \\'.pt\\'\\n        q_name = \\'q_approx_state_dict_\\' + model_info\\n        optim_name = \\'optim_state_dict_\\' + model_info\\n        model = {\\n            q_name: agent.q_model.state_dict(),\\n            optim_name: agent.q_model.optimizer.state_dict()\\n        }\\n        # model = {\\n        #     q_name: agent.q_model.state_dict(),\\n        #     optim_name: agent.optimizer.state_dict()\\n        # }\\n        torch.save(model, path)\\n    print(\\'-\\'*5)\\n# save rewards to disk\\nwith open(\"./dqn_rewards/dqn_rewards.pkl\",\\'wb\\') as f:\\n    pickle.dump(rewards, f)\\n'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 43
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "'''\n",
                "# plot average reward data\n",
                "# print(rewards[:,:,:].shape)\n",
                "import pandas as pd\n",
                "\n",
                "d = pd.Series(rewards[0,0,:])\n",
                "plt.plot(np.arange(EPISODES),d.rolling(100).mean())\n",
                "plt.xlabel('episodes')\n",
                "plt.ylabel('average reward')\n",
                "# plt.plot(rewards[:,:])\n",
                "'''"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "\"\\n# plot average reward data\\n# print(rewards[:,:,:].shape)\\nimport pandas as pd\\n\\nd = pd.Series(rewards[0,0,:])\\nplt.plot(np.arange(EPISODES),d.rolling(100).mean())\\nplt.xlabel('episodes')\\nplt.ylabel('average reward')\\n# plt.plot(rewards[:,:])\\n\""
                        ]
                    },
                    "metadata": {},
                    "execution_count": 44
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "'''\n",
                "\n",
                "from tqdm import tqdm\n",
                "gamma = 0.95\n",
                "epsilon = 1\n",
                "learning_rate = 1e-3\n",
                "EPISODES = 10000\n",
                "MINI_BATCH_SIZE = 16\n",
                "MEMORY_SIZE = 256\n",
                "# EPISODES = 100\n",
                "rewards = []\n",
                "loss_list = []\n",
                "epsilon_list = []\n",
                "q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "# q_target.eval()\n",
                "\n",
                "# print(q_approx)\n",
                "# print(q_target)\n",
                "# print(index_gamma)\n",
                "\n",
                "agent = Agent(q_approx, env, epsilon, gamma, MINI_BATCH_SIZE,\n",
                "              MEMORY_SIZE, learning_rate)\n",
                "\n",
                "# print(q_approx)\n",
                "# print(agent.q_target)\n",
                "# reset replay memory\n",
                "agent.init_replay_memory()\n",
                "agent.train_step_count = 128\n",
                "for episode in tqdm(range(EPISODES)):\n",
                "\n",
                "    # train one episode\n",
                "    # agent.train_episode()\n",
                "    done = False\n",
                "    obs = agent.env.reset()\n",
                "    rew = 0\n",
                "    losses = 0\n",
                "    while not done:\n",
                "        # get action to execute based on state\n",
                "\n",
                "        action = agent.get_action(obs, agent.n_actions, agent.epsilon)\n",
                "\n",
                "        #  take action, go to next time step\n",
                "        obs_next, reward, done, info = agent.env.step(action.item())\n",
                "\n",
                "        agent.collect_experience([obs, action.item(), reward, obs_next, done])\n",
                "\n",
                "        obs = obs_next\n",
                "        rew += reward\n",
                "        agent.train_step_count += 1\n",
                "\n",
                "        if agent.train_step_count > 128:\n",
                "            agent.train_step_count = 0\n",
                "            for _ in range(4):\n",
                "                loss = agent.train(agent.BATCH_SIZE)\n",
                "                losses += loss\n",
                "    # update epsilon value\n",
                "    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "    if agent.epsilon > 0.05:\n",
                "        agent.epsilon -= (1 / 5000)\n",
                "\n",
                "    rewards.append(rew)\n",
                "    loss_list.append(losses)\n",
                "    epsilon_list.append(agent.epsilon)\n",
                "\n",
                "# total_rewards = np.sum(rewards[0, 0,:])\n",
                "rewards = np.array(rewards)\n",
                "print(rewards.shape)\n",
                "# print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\n",
                "\n",
                "'''"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'\\n\\nfrom tqdm import tqdm\\ngamma = 0.95\\nepsilon = 1\\nlearning_rate = 1e-3\\nEPISODES = 10000\\nMINI_BATCH_SIZE = 16\\nMEMORY_SIZE = 256\\n# EPISODES = 100\\nrewards = []\\nloss_list = []\\nepsilon_list = []\\nq_approx = DQN(lr=learning_rate, device=device).to(device)\\n# q_target.eval()\\n\\n# print(q_approx)\\n# print(q_target)\\n# print(index_gamma)\\n\\nagent = Agent(q_approx, env, epsilon, gamma, MINI_BATCH_SIZE,\\n              MEMORY_SIZE, learning_rate)\\n\\n# print(q_approx)\\n# print(agent.q_target)\\n# reset replay memory\\nagent.init_replay_memory()\\nagent.train_step_count = 128\\nfor episode in tqdm(range(EPISODES)):\\n\\n    # train one episode\\n    # agent.train_episode()\\n    done = False\\n    obs = agent.env.reset()\\n    rew = 0\\n    losses = 0\\n    while not done:\\n        # get action to execute based on state\\n\\n        action = agent.get_action(obs, agent.n_actions, agent.epsilon)\\n\\n        #  take action, go to next time step\\n        obs_next, reward, done, info = agent.env.step(action.item())\\n\\n        agent.collect_experience([obs, action.item(), reward, obs_next, done])\\n\\n        obs = obs_next\\n        rew += reward\\n        agent.train_step_count += 1\\n\\n        if agent.train_step_count > 128:\\n            agent.train_step_count = 0\\n            for _ in range(4):\\n                loss = agent.train(agent.BATCH_SIZE)\\n                losses += loss\\n    # update epsilon value\\n    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\\n    if agent.epsilon > 0.05:\\n        agent.epsilon -= (1 / 5000)\\n\\n    rewards.append(rew)\\n    loss_list.append(losses)\\n    epsilon_list.append(agent.epsilon)\\n\\n# total_rewards = np.sum(rewards[0, 0,:])\\nrewards = np.array(rewards)\\nprint(rewards.shape)\\n# print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\\n\\n'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 45
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "from tqdm import tqdm\n",
                "gamma = 0.95\n",
                "epsilon = 1\n",
                "learning_rate = 1e-3\n",
                "EPISODES = 10000\n",
                "MINI_BATCH_SIZE = 16\n",
                "MEMORY_SIZE = 256\n",
                "# EPISODES = 100\n",
                "rewards = []\n",
                "loss_list = []\n",
                "epsilon_list = []\n",
                "q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "# q_target.eval()\n",
                "\n",
                "# print(q_approx)\n",
                "# print(q_target)\n",
                "# print(index_gamma)\n",
                "\n",
                "agent = Agent(q_approx, env, epsilon, gamma, MINI_BATCH_SIZE,\n",
                "              MEMORY_SIZE, learning_rate)\n",
                "\n",
                "# print(q_approx)\n",
                "# print(agent.q_target)\n",
                "# reset replay memory\n",
                "agent.init_replay_memory()\n",
                "agent.train_step_count = 128\n",
                "for episode in tqdm(range(EPISODES)):\n",
                "\n",
                "    # train one episode\n",
                "    # done = False\n",
                "    # obs = agent.env.reset()\n",
                "    # # get action to execute based on state\n",
                "    # action = agent.get_action(obs)\n",
                "    # rew = 0\n",
                "    # losses = 0\n",
                "    # while not done:\n",
                "    #     # perform one time step action\n",
                "    #     done, reward, obs_next, \\\n",
                "    #         action_next, other_data = agent.forward(obs, action)\n",
                "        \n",
                "    #     # update next observation\n",
                "    #     obs = obs_next\n",
                "    #     # update next action\n",
                "    #     action = action_next\n",
                "\n",
                "    #     # update training data\n",
                "    #     rew += reward\n",
                "    #     losses += other_data['loss']\n",
                "\n",
                "    t, episode_data = agent.episode_train()\n",
                "    # update epsilon value\n",
                "    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "    if agent.epsilon > 0.05:\n",
                "        agent.epsilon -= (1 / 5000)\n",
                "\n",
                "    rewards.append(episode_data[0])\n",
                "    loss_list.append(episode_data[1])\n",
                "    epsilon_list.append(agent.epsilon)\n",
                "\n",
                "# total_rewards = np.sum(rewards[0, 0,:])\n",
                "rewards = np.array(rewards)\n",
                "print(rewards.shape)\n",
                "# print(\"Score over time: {}, gamma={}\".format(total_rewards/EPISODES, gamma))\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 10000/10000 [01:48<00:00, 92.18it/s]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(10000,)\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "source": [
                "# plot average reward data\n",
                "# print(rewards[:,:,:].shape)\n",
                "import pandas as pd\n",
                "\n",
                "# d = pd.Series(rewards[0,0,:])\n",
                "plt.plot(rewards)\n",
                "plt.xlabel('episodes')\n",
                "plt.ylabel('reward')\n",
                "# plt.plot(rewards[:,:])\n",
                "\n",
                "print('average reward per episode= {}'.format(np.mean(rewards, axis=0)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "average reward per episode= 101.919\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "my_dict = {}\n",
                "# my_dict[\"data\"] = \"Their model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\nMy model params\\ntensor([[ 0.3041,  0.2882, -0.2917,  0.4162],\\n        [ 0.1855,  0.4892,  0.0079, -0.1485],\\n        [ 0.2827, -0.1978,  0.0915,  0.1492],\\n        [ 0.2694,  0.2406, -0.0709,  0.0753],\\n        [ 0.1781, -0.0996, -0.4768, -0.0902],\\n        [ 0.0527,  0.3042,  0.4053, -0.4673],\\n        [-0.3728,  0.4554,  0.1156,  0.1509],\\n        [-0.2538,  0.1942, -0.3371,  0.1100],\\n        [-0.3160, -0.2490, -0.2002,  0.1645],\\n        [-0.1380,  0.0902, -0.0432,  0.1049],\\n        [-0.1543,  0.4901,  0.0083,  0.0152],\\n        [ 0.1811, -0.4768,  0.0971,  0.1288],\\n        [ 0.2728,  0.4485, -0.2000,  0.1215],\\n        [ 0.1950, -0.4748,  0.2200,  0.2112],\\n        [-0.2967, -0.2873, -0.3639,  0.4410],\\n        [-0.1727, -0.3969, -0.0300,  0.4692],\\n        [-0.3265, -0.0512,  0.3419, -0.2583],\\n        [-0.0901, -0.2474, -0.0358, -0.0221],\\n        [-0.2907,  0.2076, -0.2116,  0.4516],\\n        [-0.4855, -0.2546, -0.3364, -0.3318],\\n        [-0.3998, -0.0230, -0.2552, -0.2768],\\n        [-0.3959,  0.0057,  0.4193,  0.2657],\\n        [-0.1441,  0.2253,  0.4854, -0.1122],\\n        [-0.1419,  0.1340, -0.2012,  0.2925],\\n        [ 0.3803,  0.3698, -0.4950, -0.1462],\\n        [-0.1645, -0.4683,  0.1710, -0.0956],\\n        [ 0.0400,  0.0339,  0.3003,  0.3645],\\n        [ 0.4606, -0.0104, -0.0227,  0.3237],\\n        [ 0.4904,  0.4335, -0.2649,  0.2615],\\n        [ 0.1789, -0.3907, -0.1692, -0.2415],\\n        [ 0.2577,  0.3993,  0.3816, -0.0371],\\n        [-0.0268,  0.4042,  0.2384,  0.4170],\\n        [ 0.3716, -0.1569, -0.2289, -0.4692],\\n        [-0.3748, -0.0156, -0.1977,  0.2151],\\n        [ 0.1638,  0.2755,  0.2921, -0.4573],\\n        [ 0.3901, -0.0634, -0.2933, -0.1968],\\n        [ 0.4970, -0.1262,  0.3650,  0.4007],\\n        [ 0.2788,  0.1445,  0.0942,  0.1048],\\n        [-0.3838, -0.4978, -0.4175, -0.1444],\\n        [-0.2990,  0.0109,  0.2287,  0.1491],\\n        [-0.2714,  0.1556, -0.0731,  0.4427],\\n        [-0.4720, -0.1670, -0.1322,  0.4997],\\n        [-0.0374, -0.2431, -0.4579, -0.1824],\\n        [ 0.2399, -0.1023, -0.0905, -0.0190],\\n        [-0.1580,  0.1126, -0.1087, -0.4724],\\n        [-0.0587, -0.2401,  0.3096,  0.0515],\\n        [-0.1888,  0.1881, -0.2436, -0.3915],\\n        [-0.0538, -0.3004,  0.4868,  0.4518],\\n        [ 0.4151,  0.3819, -0.4865, -0.2404],\\n        [-0.4273,  0.0289, -0.3914, -0.1450],\\n        [ 0.4119,  0.4608, -0.0953, -0.2822],\\n        [-0.2009, -0.2225,  0.3750,  0.1481],\\n        [ 0.4009,  0.2628, -0.4354,  0.4003],\\n        [ 0.1444,  0.0308,  0.4046,  0.0810],\\n        [ 0.0310,  0.0230, -0.3415, -0.2876],\\n        [ 0.0175, -0.4231, -0.2572, -0.4341],\\n        [ 0.1194, -0.0733, -0.1846,  0.3522],\\n        [-0.2700, -0.0280,  0.4058,  0.1455],\\n        [-0.1910, -0.2753, -0.4003, -0.1842],\\n        [-0.2268, -0.4600, -0.1236, -0.1261],\\n        [-0.3728, -0.3369,  0.4468,  0.1672],\\n        [ 0.4776, -0.4104,  0.1595,  0.0460],\\n        [-0.1318,  0.4914, -0.4923,  0.3753],\\n        [ 0.2644, -0.2238,  0.1891,  0.0529]])\\ntheir model = Sequential(\\n  (0): Linear(in_features=4, out_features=64, bias=True)\\n  (1): Tanh()\\n  (2): Linear(in_features=64, out_features=2, bias=True)\\n  (3): Identity()\\n)\\n\"\n",
                "# \"Their model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\nMy model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\n\"\n",
                "my_dict[\"data\"] = \"Their model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\nMy model params\\ntensor([[-0.0049,  0.4691,  0.0008, -0.2552],\\n        [-0.4204, -0.3378, -0.1222,  0.3769],\\n        [ 0.2397,  0.1964,  0.1816,  0.3778],\\n        [ 0.4347,  0.1436, -0.1596,  0.4537],\\n        [ 0.4348,  0.0355, -0.0083,  0.3024],\\n        [ 0.2166,  0.1349,  0.1098,  0.1224],\\n        [ 0.1775, -0.1645,  0.2105, -0.0397],\\n        [-0.0754, -0.4345,  0.3258,  0.1700],\\n        [-0.2496, -0.3918, -0.4072,  0.3070],\\n        [ 0.4155, -0.3877, -0.4982,  0.3630],\\n        [-0.2475,  0.3201,  0.1642, -0.3372],\\n        [ 0.2409,  0.1367, -0.3578,  0.0781],\\n        [-0.1473,  0.2909,  0.2483,  0.2926],\\n        [ 0.4901,  0.2115, -0.2738, -0.1703],\\n        [-0.0600,  0.2908,  0.4502,  0.4441],\\n        [ 0.4434,  0.4149, -0.2595,  0.1586],\\n        [ 0.3966,  0.1285, -0.2990,  0.1227],\\n        [-0.4980, -0.1935, -0.4164, -0.1403],\\n        [ 0.1850,  0.4681, -0.3764,  0.4686],\\n        [ 0.1104,  0.2758,  0.1343,  0.4484],\\n        [-0.3391, -0.1275, -0.4930,  0.2192],\\n        [ 0.1430, -0.1227, -0.2408, -0.2525],\\n        [-0.0177,  0.3471,  0.3640, -0.1564],\\n        [-0.0805, -0.4666,  0.4643, -0.3790],\\n        [ 0.0126, -0.1771,  0.0444,  0.0462],\\n        [ 0.2060,  0.0925,  0.3172, -0.2507],\\n        [-0.4180, -0.0289,  0.1603,  0.4187],\\n        [ 0.4140,  0.1766, -0.0715,  0.2054],\\n        [-0.1019, -0.1722, -0.4649,  0.0474],\\n        [ 0.0336, -0.4134, -0.4997, -0.2138],\\n        [-0.0089, -0.0535,  0.3089,  0.3632],\\n        [ 0.4706, -0.1999, -0.3329, -0.1394],\\n        [-0.2437,  0.0588, -0.4512, -0.4312],\\n        [-0.3709,  0.3943,  0.3293,  0.3457],\\n        [ 0.2464,  0.3609,  0.2065,  0.1736],\\n        [ 0.2262, -0.4726,  0.1455,  0.1766],\\n        [ 0.1093,  0.2545,  0.0765, -0.0834],\\n        [ 0.4327,  0.0756, -0.3747,  0.3781],\\n        [-0.0350, -0.4322,  0.4699,  0.4775],\\n        [ 0.1317, -0.0425,  0.4230, -0.3786],\\n        [ 0.3679, -0.4886, -0.2918, -0.2127],\\n        [ 0.3491, -0.1311, -0.0226,  0.1095],\\n        [ 0.0184, -0.3099,  0.1176,  0.1575],\\n        [ 0.0322, -0.0294, -0.2927, -0.3065],\\n        [-0.1197,  0.0516,  0.3122, -0.2274],\\n        [-0.0270,  0.1684,  0.2447, -0.0240],\\n        [-0.2449,  0.1924, -0.4391,  0.4411],\\n        [ 0.3282, -0.0320,  0.3797,  0.3175],\\n        [-0.0109,  0.0690, -0.2216,  0.2285],\\n        [ 0.1793, -0.1409, -0.4187, -0.0879],\\n        [-0.0542,  0.1776, -0.0465, -0.4142],\\n        [-0.2198, -0.0282,  0.0586, -0.1978],\\n        [ 0.4521, -0.1308,  0.4037, -0.3965],\\n        [ 0.4480,  0.3013, -0.3900, -0.2043],\\n        [ 0.3752, -0.2595,  0.1841, -0.4292],\\n        [ 0.3758,  0.0451,  0.2408,  0.4024],\\n        [-0.1107, -0.0551,  0.0391, -0.3562],\\n        [-0.3748, -0.2884, -0.2934,  0.3486],\\n        [-0.1138,  0.2532,  0.1133,  0.1165],\\n        [-0.3540, -0.1052,  0.2960,  0.2832],\\n        [-0.1219, -0.3756, -0.1850,  0.3641],\\n        [-0.1459,  0.4960,  0.2193, -0.4555],\\n        [-0.4368, -0.4323,  0.3171,  0.1576],\\n        [-0.4617,  0.4231,  0.1655,  0.1769]])\\n\"\n",
                "\n",
                "print(my_dict[\"data\"])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Their model params\n",
                        "tensor([[-0.0049,  0.4691,  0.0008, -0.2552],\n",
                        "        [-0.4204, -0.3378, -0.1222,  0.3769],\n",
                        "        [ 0.2397,  0.1964,  0.1816,  0.3778],\n",
                        "        [ 0.4347,  0.1436, -0.1596,  0.4537],\n",
                        "        [ 0.4348,  0.0355, -0.0083,  0.3024],\n",
                        "        [ 0.2166,  0.1349,  0.1098,  0.1224],\n",
                        "        [ 0.1775, -0.1645,  0.2105, -0.0397],\n",
                        "        [-0.0754, -0.4345,  0.3258,  0.1700],\n",
                        "        [-0.2496, -0.3918, -0.4072,  0.3070],\n",
                        "        [ 0.4155, -0.3877, -0.4982,  0.3630],\n",
                        "        [-0.2475,  0.3201,  0.1642, -0.3372],\n",
                        "        [ 0.2409,  0.1367, -0.3578,  0.0781],\n",
                        "        [-0.1473,  0.2909,  0.2483,  0.2926],\n",
                        "        [ 0.4901,  0.2115, -0.2738, -0.1703],\n",
                        "        [-0.0600,  0.2908,  0.4502,  0.4441],\n",
                        "        [ 0.4434,  0.4149, -0.2595,  0.1586],\n",
                        "        [ 0.3966,  0.1285, -0.2990,  0.1227],\n",
                        "        [-0.4980, -0.1935, -0.4164, -0.1403],\n",
                        "        [ 0.1850,  0.4681, -0.3764,  0.4686],\n",
                        "        [ 0.1104,  0.2758,  0.1343,  0.4484],\n",
                        "        [-0.3391, -0.1275, -0.4930,  0.2192],\n",
                        "        [ 0.1430, -0.1227, -0.2408, -0.2525],\n",
                        "        [-0.0177,  0.3471,  0.3640, -0.1564],\n",
                        "        [-0.0805, -0.4666,  0.4643, -0.3790],\n",
                        "        [ 0.0126, -0.1771,  0.0444,  0.0462],\n",
                        "        [ 0.2060,  0.0925,  0.3172, -0.2507],\n",
                        "        [-0.4180, -0.0289,  0.1603,  0.4187],\n",
                        "        [ 0.4140,  0.1766, -0.0715,  0.2054],\n",
                        "        [-0.1019, -0.1722, -0.4649,  0.0474],\n",
                        "        [ 0.0336, -0.4134, -0.4997, -0.2138],\n",
                        "        [-0.0089, -0.0535,  0.3089,  0.3632],\n",
                        "        [ 0.4706, -0.1999, -0.3329, -0.1394],\n",
                        "        [-0.2437,  0.0588, -0.4512, -0.4312],\n",
                        "        [-0.3709,  0.3943,  0.3293,  0.3457],\n",
                        "        [ 0.2464,  0.3609,  0.2065,  0.1736],\n",
                        "        [ 0.2262, -0.4726,  0.1455,  0.1766],\n",
                        "        [ 0.1093,  0.2545,  0.0765, -0.0834],\n",
                        "        [ 0.4327,  0.0756, -0.3747,  0.3781],\n",
                        "        [-0.0350, -0.4322,  0.4699,  0.4775],\n",
                        "        [ 0.1317, -0.0425,  0.4230, -0.3786],\n",
                        "        [ 0.3679, -0.4886, -0.2918, -0.2127],\n",
                        "        [ 0.3491, -0.1311, -0.0226,  0.1095],\n",
                        "        [ 0.0184, -0.3099,  0.1176,  0.1575],\n",
                        "        [ 0.0322, -0.0294, -0.2927, -0.3065],\n",
                        "        [-0.1197,  0.0516,  0.3122, -0.2274],\n",
                        "        [-0.0270,  0.1684,  0.2447, -0.0240],\n",
                        "        [-0.2449,  0.1924, -0.4391,  0.4411],\n",
                        "        [ 0.3282, -0.0320,  0.3797,  0.3175],\n",
                        "        [-0.0109,  0.0690, -0.2216,  0.2285],\n",
                        "        [ 0.1793, -0.1409, -0.4187, -0.0879],\n",
                        "        [-0.0542,  0.1776, -0.0465, -0.4142],\n",
                        "        [-0.2198, -0.0282,  0.0586, -0.1978],\n",
                        "        [ 0.4521, -0.1308,  0.4037, -0.3965],\n",
                        "        [ 0.4480,  0.3013, -0.3900, -0.2043],\n",
                        "        [ 0.3752, -0.2595,  0.1841, -0.4292],\n",
                        "        [ 0.3758,  0.0451,  0.2408,  0.4024],\n",
                        "        [-0.1107, -0.0551,  0.0391, -0.3562],\n",
                        "        [-0.3748, -0.2884, -0.2934,  0.3486],\n",
                        "        [-0.1138,  0.2532,  0.1133,  0.1165],\n",
                        "        [-0.3540, -0.1052,  0.2960,  0.2832],\n",
                        "        [-0.1219, -0.3756, -0.1850,  0.3641],\n",
                        "        [-0.1459,  0.4960,  0.2193, -0.4555],\n",
                        "        [-0.4368, -0.4323,  0.3171,  0.1576],\n",
                        "        [-0.4617,  0.4231,  0.1655,  0.1769]])\n",
                        "My model params\n",
                        "tensor([[-0.0049,  0.4691,  0.0008, -0.2552],\n",
                        "        [-0.4204, -0.3378, -0.1222,  0.3769],\n",
                        "        [ 0.2397,  0.1964,  0.1816,  0.3778],\n",
                        "        [ 0.4347,  0.1436, -0.1596,  0.4537],\n",
                        "        [ 0.4348,  0.0355, -0.0083,  0.3024],\n",
                        "        [ 0.2166,  0.1349,  0.1098,  0.1224],\n",
                        "        [ 0.1775, -0.1645,  0.2105, -0.0397],\n",
                        "        [-0.0754, -0.4345,  0.3258,  0.1700],\n",
                        "        [-0.2496, -0.3918, -0.4072,  0.3070],\n",
                        "        [ 0.4155, -0.3877, -0.4982,  0.3630],\n",
                        "        [-0.2475,  0.3201,  0.1642, -0.3372],\n",
                        "        [ 0.2409,  0.1367, -0.3578,  0.0781],\n",
                        "        [-0.1473,  0.2909,  0.2483,  0.2926],\n",
                        "        [ 0.4901,  0.2115, -0.2738, -0.1703],\n",
                        "        [-0.0600,  0.2908,  0.4502,  0.4441],\n",
                        "        [ 0.4434,  0.4149, -0.2595,  0.1586],\n",
                        "        [ 0.3966,  0.1285, -0.2990,  0.1227],\n",
                        "        [-0.4980, -0.1935, -0.4164, -0.1403],\n",
                        "        [ 0.1850,  0.4681, -0.3764,  0.4686],\n",
                        "        [ 0.1104,  0.2758,  0.1343,  0.4484],\n",
                        "        [-0.3391, -0.1275, -0.4930,  0.2192],\n",
                        "        [ 0.1430, -0.1227, -0.2408, -0.2525],\n",
                        "        [-0.0177,  0.3471,  0.3640, -0.1564],\n",
                        "        [-0.0805, -0.4666,  0.4643, -0.3790],\n",
                        "        [ 0.0126, -0.1771,  0.0444,  0.0462],\n",
                        "        [ 0.2060,  0.0925,  0.3172, -0.2507],\n",
                        "        [-0.4180, -0.0289,  0.1603,  0.4187],\n",
                        "        [ 0.4140,  0.1766, -0.0715,  0.2054],\n",
                        "        [-0.1019, -0.1722, -0.4649,  0.0474],\n",
                        "        [ 0.0336, -0.4134, -0.4997, -0.2138],\n",
                        "        [-0.0089, -0.0535,  0.3089,  0.3632],\n",
                        "        [ 0.4706, -0.1999, -0.3329, -0.1394],\n",
                        "        [-0.2437,  0.0588, -0.4512, -0.4312],\n",
                        "        [-0.3709,  0.3943,  0.3293,  0.3457],\n",
                        "        [ 0.2464,  0.3609,  0.2065,  0.1736],\n",
                        "        [ 0.2262, -0.4726,  0.1455,  0.1766],\n",
                        "        [ 0.1093,  0.2545,  0.0765, -0.0834],\n",
                        "        [ 0.4327,  0.0756, -0.3747,  0.3781],\n",
                        "        [-0.0350, -0.4322,  0.4699,  0.4775],\n",
                        "        [ 0.1317, -0.0425,  0.4230, -0.3786],\n",
                        "        [ 0.3679, -0.4886, -0.2918, -0.2127],\n",
                        "        [ 0.3491, -0.1311, -0.0226,  0.1095],\n",
                        "        [ 0.0184, -0.3099,  0.1176,  0.1575],\n",
                        "        [ 0.0322, -0.0294, -0.2927, -0.3065],\n",
                        "        [-0.1197,  0.0516,  0.3122, -0.2274],\n",
                        "        [-0.0270,  0.1684,  0.2447, -0.0240],\n",
                        "        [-0.2449,  0.1924, -0.4391,  0.4411],\n",
                        "        [ 0.3282, -0.0320,  0.3797,  0.3175],\n",
                        "        [-0.0109,  0.0690, -0.2216,  0.2285],\n",
                        "        [ 0.1793, -0.1409, -0.4187, -0.0879],\n",
                        "        [-0.0542,  0.1776, -0.0465, -0.4142],\n",
                        "        [-0.2198, -0.0282,  0.0586, -0.1978],\n",
                        "        [ 0.4521, -0.1308,  0.4037, -0.3965],\n",
                        "        [ 0.4480,  0.3013, -0.3900, -0.2043],\n",
                        "        [ 0.3752, -0.2595,  0.1841, -0.4292],\n",
                        "        [ 0.3758,  0.0451,  0.2408,  0.4024],\n",
                        "        [-0.1107, -0.0551,  0.0391, -0.3562],\n",
                        "        [-0.3748, -0.2884, -0.2934,  0.3486],\n",
                        "        [-0.1138,  0.2532,  0.1133,  0.1165],\n",
                        "        [-0.3540, -0.1052,  0.2960,  0.2832],\n",
                        "        [-0.1219, -0.3756, -0.1850,  0.3641],\n",
                        "        [-0.1459,  0.4960,  0.2193, -0.4555],\n",
                        "        [-0.4368, -0.4323,  0.3171,  0.1576],\n",
                        "        [-0.4617,  0.4231,  0.1655,  0.1769]])\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "# for param in q_approx.parameters():\n",
                "#     print(param.size())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "# RENDER = False\n",
                "# total_rewards = []\n",
                "# for episode in range(EPISODES):\n",
                "#     done = False\n",
                "#     obs = agent.env.reset()\n",
                "#     obs = torch.tensor([obs], device=agent.device, dtype=torch.float64)\n",
                "#     rewards = []\n",
                "#     while not done:\n",
                "#         # get action to execute based on state\n",
                "#         action = agent.get_action(obs.float())\n",
                "#         #  take action, go to next time step\n",
                "#         obs_next, reward, done, info = agent.env.step(action.item())\n",
                "#         obs_next = torch.tensor([obs_next], device=agent.device, dtype=torch.float64)\n",
                "#         obs = obs_next\n",
                "#         rewards.append(reward)\n",
                "#         if RENDER:\n",
                "#             env.render()\n",
                "\n",
                "#     rewards = np.array(rewards)\n",
                "#     total_rewards.append(np.sum(rewards))\n",
                "\n",
                "# plt.plot(total_rewards)\n",
                "# plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "'''\n",
                "\n",
                "learning_rate = 0.0001\n",
                "EPSILON_MIN = 0.05\n",
                "EPS_DECAY = 0.001\n",
                "gamma= 0.99\n",
                "TARGET_UPDATE = 10\n",
                "EPISODES = 10000\n",
                "\n",
                "MEMORY_SIZE = 256\n",
                "D = deque(maxlen=MEMORY_SIZE)\n",
                "MINI_BATCH_SIZE = 16\n",
                "epsilon=1\n",
                "\n",
                "\n",
                "\n",
                "q_approx = DQN(lr=learning_rate, device=device).to(device)\n",
                "q_target = DQN(lr=learning_rate, device=device).to(device)\n",
                "# load same weights as approx\n",
                "q_target.load_state_dict(q_approx.state_dict())\n",
                "q_target.eval()\n",
                "# reset replay memory\n",
                "D = deque(maxlen=MEMORY_SIZE)\n",
                "agent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE)\n",
                "for episode in range(EPISODES):\n",
                "\n",
                "    if episode % 1000 == 0:\n",
                "        print('-'*5)\n",
                "        print('Episode = {}'.format(episode))\n",
                "        print('Epsilon = {}'.format(agent.epsilon))\n",
                "        print('-'*5)\n",
                "    # train one episode\n",
                "    agent.train_episode()\n",
                "\n",
                "    # update epsilon value\n",
                "    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\n",
                "    if agent.epsilon > 0.05 :\n",
                "        agent.epsilon -= (1 / 5000)\n",
                "\n",
                "    # # update target paramters\n",
                "    if episode % TARGET_UPDATE == 0:\n",
                "        q_target.load_state_dict(q_approx.state_dict())\n",
                "        q_target.eval()\n",
                "\n",
                "# shallow copy\n",
                "# rewards_per_episode = agent.train_data.copy()\n",
                "# alias\n",
                "rewards_per_episode = agent.train_data\n",
                "\n",
                "total_rewards = np.sum(rewards_per_episode)\n",
                "print(\"Score over time: {}, gamma={}\".format( total_rewards/EPISODES, gamma))\n",
                "average_reward = total_rewards/EPISODES\n",
                "\n",
                "plt.plot(rewards_per_episode)\n",
                "\n",
                "\n",
                "'''"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'\\n\\nlearning_rate = 0.0001\\nEPSILON_MIN = 0.05\\nEPS_DECAY = 0.001\\ngamma= 0.99\\nTARGET_UPDATE = 10\\nEPISODES = 10000\\n\\nMEMORY_SIZE = 256\\nD = deque(maxlen=MEMORY_SIZE)\\nMINI_BATCH_SIZE = 16\\nepsilon=1\\n\\n\\n\\nq_approx = DQN(lr=learning_rate, device=device).to(device)\\nq_target = DQN(lr=learning_rate, device=device).to(device)\\n# load same weights as approx\\nq_target.load_state_dict(q_approx.state_dict())\\nq_target.eval()\\n# reset replay memory\\nD = deque(maxlen=MEMORY_SIZE)\\nagent = Agent(q_approx, q_target, env, epsilon, gamma, MINI_BATCH_SIZE)\\nfor episode in range(EPISODES):\\n\\n    if episode % 1000 == 0:\\n        print(\\'-\\'*5)\\n        print(\\'Episode = {}\\'.format(episode))\\n        print(\\'Epsilon = {}\\'.format(agent.epsilon))\\n        print(\\'-\\'*5)\\n    # train one episode\\n    agent.train_episode()\\n\\n    # update epsilon value\\n    # agent.epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN)*np.exp(-EPS_DECAY*episode)\\n    if agent.epsilon > 0.05 :\\n        agent.epsilon -= (1 / 5000)\\n\\n    # # update target paramters\\n    if episode % TARGET_UPDATE == 0:\\n        q_target.load_state_dict(q_approx.state_dict())\\n        q_target.eval()\\n\\n# shallow copy\\n# rewards_per_episode = agent.train_data.copy()\\n# alias\\nrewards_per_episode = agent.train_data\\n\\ntotal_rewards = np.sum(rewards_per_episode)\\nprint(\"Score over time: {}, gamma={}\".format( total_rewards/EPISODES, gamma))\\naverage_reward = total_rewards/EPISODES\\n\\nplt.plot(rewards_per_episode)\\n\\n\\n'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 51
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "f41a9e18d32d699c7ebd9346171aa8606b8eaf6d2e7d29caa03f22c5e982b824"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}