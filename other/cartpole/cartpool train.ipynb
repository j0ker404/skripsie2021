{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "PACKAGE_PARENT = '../../'\n",
    "sys.path.append(PACKAGE_PARENT)\n",
    "\n",
    "from alphaslime.evaluate.eval_agents import EvaluateGameSA\n",
    "from alphaslime.agents.other.semiGradSarsa import SemiGradSarsa\n",
    "from alphaslime.approx.linearq import LinearQApprox\n",
    "\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import csv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# config intial properties\n",
    "\n",
    "env_id = 'CartPole-v1'\n",
    "env = gym.make(env_id)\n",
    "# seed the random numbers and the gym environment\n",
    "seed = 42\n",
    "np.random.seed(seed)    \n",
    "env.seed(seed)\n",
    "\n",
    "# agent properties\n",
    "alpha = 0.1 # step size\n",
    "epsilon = 1\n",
    "gamma = 0.95\n",
    "training_episodes = 1000\n",
    "observation_dimension=4\n",
    "action_table = [0, 1]\n",
    "\n",
    "\n",
    "# q function approximator\n",
    "q_hat = LinearQApprox()\n",
    "\n",
    "# set config file for agent\n",
    "config = {\n",
    "    'alpha': None,\n",
    "    'gamma': gamma,\n",
    "    'epsilon': epsilon,\n",
    "    'action_table': action_table,\n",
    "    'd': observation_dimension,\n",
    "    't_max': 500,\n",
    "    'max_score': 500,\n",
    "    'episode_printer': 100,\n",
    "    'env': env,\n",
    "    'weights':None,\n",
    "    'q_hat': q_hat\n",
    "}\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# we want to determine the best alpha value\n",
    "# so iterate alpha\n",
    "# then train, then determine average score per alpha\n",
    "\n",
    "# function for saving weights to disk\n",
    "def save_weight(alpha, weights):\n",
    "    path = './train/sarsa/'\n",
    "    file_name = 'weights_alpha' + \".csv\"\n",
    "    path += file_name\n",
    "    with open(path, 'a', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        # write the header\n",
    "        writer.writerow(str(alpha))\n",
    "\n",
    "        # write multiple rows\n",
    "        writer.writerows(weights)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# create alpha list\n",
    "alphas = np.arange(0.1,1,0.05)\n",
    "\n",
    "# testing trails\n",
    "trails = 1\n",
    "\n",
    "average_rewards = []\n",
    "\n",
    "# configure evaluation test\n",
    "base_dir = './'\n",
    "RENDER = False\n",
    "\n",
    "agent_rewards = np.zeros((trails,))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# train for different alpha values\n",
    "for alpha in alphas:\n",
    "    print('Training agent for alpha = {}'.format(alpha))\n",
    "\n",
    "    # set config file\n",
    "    config['alpha'] = alpha\n",
    "    config['epsilon'] = epsilon\n",
    "\n",
    "    # init agent\n",
    "    agent = SemiGradSarsa(config)\n",
    "\n",
    "    # update alpha value\n",
    "    agent.alpha = alpha \n",
    "    # reset episolon value\n",
    "    agent.epsilon = epsilon\n",
    "    # train agent\n",
    "    agent.train(episodes=training_episodes)\n",
    "\n",
    "    # get trained weight value\n",
    "    weights = agent.w\n",
    "\n",
    "    # save weights\n",
    "    # TODO: use a thread for write operation\n",
    "    save_weight(alpha, weights)\n",
    "\n",
    "    # determine average score per episode\n",
    "    eval_game = EvaluateGameSA(agent=agent, base_dir_path=base_dir, render=RENDER, env=env)\n",
    "    agent.epsilon = 0 # make greedy actions\n",
    "    for episode in range(trails):\n",
    "        reward = eval_game.evaluate_episode()\n",
    "        agent_rewards[episode] = reward\n",
    "    \n",
    "    # store average reward\n",
    "    average_rewards.append(np.mean(agent_rewards))\n",
    "\n",
    "average_rewards = np.array(average_rewards)\n",
    "\n",
    "    \n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training agent for alpha = 0.1\n",
      "Completed Episodes = 0\n",
      "Completed Episodes = 100\n",
      "Completed Episodes = 200\n",
      "Completed Episodes = 300\n",
      "Completed Episodes = 400\n",
      "Completed Episodes = 500\n",
      "Completed Episodes = 600\n",
      "Completed Episodes = 700\n",
      "Completed Episodes = 800\n",
      "Completed Episodes = 900\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "iterable expected, not numpy.float64",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_78220/2871743323.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# save weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# TODO: use a thread for write operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msave_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# determine average score per episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_78220/4182971027.py\u001b[0m in \u001b[0;36msave_weight\u001b[0;34m(alpha, weights)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# write the header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# write multiple rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: iterable expected, not numpy.float64"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot average reward per alpha\n",
    "plt.plot(alphas, average_rewards)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "f41a9e18d32d699c7ebd9346171aa8606b8eaf6d2e7d29caa03f22c5e982b824"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}