{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PACKAGE_PARENT = '../../../'\n",
    "sys.path.append(PACKAGE_PARENT)\n",
    "\n",
    "from alphaslime.agents.RL.policygrad.torch.ppo import PPOAgent\n",
    "\n",
    "from alphaslime.trainer.trainerSA import TrainerSA\n",
    "from alphaslime.store.constantConfig import Constants\n",
    "from alphaslime.store.config import Config\n",
    "\n",
    "\n",
    "\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import slimevolleygym\n",
    "import gym\n",
    "import csv\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create directory if not present\n",
    "if not os.path.exists('tmp/ppo/'):\n",
    "    os.makedirs('tmp/ppo/')\n",
    "\n",
    "if not os.path.exists('plots/'):\n",
    "    os.makedirs('plots/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# constant config\n",
    "env_id = \"SlimeVolley-v0\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# actions for slimeball\n",
    "action_table = [[0, 0, 0], # NOOP\n",
    "                [1, 0, 0], # LEFT (forward)\n",
    "                [1, 0, 1], # UPLEFT (forward jump)\n",
    "                [0, 0, 1], # UP (jump)\n",
    "                [0, 1, 1], # UPRIGHT (backward jump)\n",
    "                [0, 1, 0]] # RIGHT (backward)\n",
    "\n",
    "# env.seed(256)\n",
    "# random.seed(256)\n",
    "\n",
    "\n",
    "# agent config\n",
    "STEP_UPDATE = 4096\n",
    "input_dims = env.observation_space.shape\n",
    "gamma = 0.99\n",
    "alpha = 0.0003\n",
    "gae_lambda = 0.95\n",
    "policy_clip = 0.2\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "\n",
    "\n",
    "# training config\n",
    "threshold = 195\n",
    "is_threshold_stop = False\n",
    "running_avg_len = 100\n",
    "is_progress = True\n",
    "EPISODES = 300\n",
    "EPISODES = 1000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "const = {\n",
    "    'env': env,\n",
    "    'action_table': action_table\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "agent_config = {\n",
    "    'input_dims': input_dims,\n",
    "    'gamma': gamma,\n",
    "    'alpha':alpha,\n",
    "    'gae_lambda': gae_lambda,\n",
    "    'policy_clip': policy_clip,\n",
    "    'batch_size': batch_size,\n",
    "    'n_epochs': n_epochs,\n",
    "    'STEP_UPDATE': STEP_UPDATE,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "\n",
    "training_configs = {\n",
    "    'agent_type': PPOAgent,\n",
    "    'EPISODES': EPISODES,\n",
    "    'is_progress': is_progress,\n",
    "    'threshold': threshold, \n",
    "    'is_threshold_stop': is_threshold_stop,\n",
    "    'running_avg_len': running_avg_len\n",
    "}\n",
    "\n",
    "\n",
    "CONST = Constants(const)\n",
    "agent_hyper = Config(agent_config)\n",
    "agent_training_configs = Config(training_configs)\n",
    "\n",
    "figure_file = 'plots/cartpole.png'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_learning_curve_plus_score(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.plot(scores)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "agent = PPOAgent(CONSTANTS=CONST, config=agent_config)\n",
    "# train agent\n",
    "agent.train(agent_training_configs)\n",
    "\n",
    "training_data = agent.get_training_data()\n",
    "\n",
    "score_history = training_data['rewards']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = [i+1 for i in range(len(score_history))]\n",
    "# plot_learning_curve(x, score_history, figure_file)\n",
    "plot_learning_curve_plus_score(x, score_history, figure_file)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "f41a9e18d32d699c7ebd9346171aa8606b8eaf6d2e7d29caa03f22c5e982b824"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}