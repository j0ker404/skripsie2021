{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "PACKAGE_PARENT = '../../'\n",
    "sys.path.append(PACKAGE_PARENT)\n",
    "\n",
    "from alphaslime.evaluate.eval_agents import EvaluateGameSA\n",
    "from alphaslime.agents.other.semiGradSarsa import SemiGradSarsa\n",
    "from alphaslime.agents.other.dqnAgent import DQNAgent\n",
    "from alphaslime.approx.linearq import LinearQApprox\n",
    "from alphaslime.approx.dqn import DQN\n",
    "from alphaslime.envgame.slenv import SLenv\n",
    "from alphaslime.agents.baseline import BaselineAgent\n",
    "from alphaslime.epsilon.exp_epsilon import ExponentialDecay\n",
    "from alphaslime.epsilon.linear_epsilon import LinearDecay \n",
    "\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import csv\n",
    "\n",
    "import time\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# config intial properties\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "learning_rate = 0.001\n",
    "EPISODES = 10000\n",
    "MINI_BATCH_SIZE = 64\n",
    "MEMORY_SIZE = 10000\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# env_id = 'CartPole-v1'\n",
    "# env_id = 'CartPole-v0'\n",
    "# action_table = [0, 1]\n",
    "\n",
    "env_id = \"SlimeVolley-v0\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# actions for slimeball\n",
    "action_table = [[0, 0, 0], # NOOP\n",
    "                [1, 0, 0], # LEFT (forward)\n",
    "                [1, 0, 1], # UPLEFT (forward jump)\n",
    "                [0, 0, 1], # UP (jump)\n",
    "                [0, 1, 1], # UPRIGHT (backward jump)\n",
    "                [0, 1, 0]] # RIGHT (backward)\n",
    "\n",
    "config = {}\n",
    "config['t_max'] = 200\n",
    "config['max_score'] = 200\n",
    "config['episode_printer'] = 100\n",
    "config['env'] = None\n",
    "config['action_table'] =  action_table\n",
    "\n",
    "# set opponent agent\n",
    "opponent = BaselineAgent(config)\n",
    "\n",
    "# create multi agent wrapper\n",
    "# env = SLenv(env=env, opponent=opponent)\n",
    "\n",
    "\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "len_obs_space = env.observation_space.shape[0]\n",
    "\n",
    "print('n_actions = {}'.format(n_actions))\n",
    "print('len_obs_space = {}'.format(len_obs_space))\n",
    "# q function approximator\n",
    "hidden_layer_size = 16\n",
    "layer_sizes = [len_obs_space, hidden_layer_size, n_actions]\n",
    "q_hat = DQN(lr=learning_rate, layer_sizes=layer_sizes, device=device).to(device)\n",
    "\n",
    "# ***************************************************\n",
    "# set epsilon decay model\n",
    "min_epsilon = 0.2 \n",
    "max_epsilon = 1\n",
    "decay_rate = 0.97\n",
    "eps_decay_exp = ExponentialDecay(min_epsilon, max_epsilon, decay_rate)\n",
    "\n",
    "min_epsilon = 0.01\n",
    "max_episode = 50\n",
    "eps_decay_lin = LinearDecay(min_epsilon, max_episode)\n",
    "\n",
    "decay_model = eps_decay_exp \n",
    "# ***************************************************\n",
    "\n",
    "# set config file for agent\n",
    "config = {\n",
    "    'lr': learning_rate,\n",
    "    'gamma': gamma,\n",
    "    'epsilon': epsilon,\n",
    "    'action_table': action_table,\n",
    "    't_max': None,\n",
    "    'max_score': None,\n",
    "    'episode_printer': 100,\n",
    "    'env': env,\n",
    "    'q_hat': q_hat,\n",
    "    'batch_size': MINI_BATCH_SIZE,\n",
    "    'exp_mem_size': MEMORY_SIZE,\n",
    "    'TARGET_UPDATE': TARGET_UPDATE,\n",
    "    'epsilon_decay': decay_model\n",
    "}\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "agent = DQNAgent(config)\n",
    "\n",
    "# train agent\n",
    "avg_scores = agent.train(EPISODES=EPISODES, is_progress=True, threshold=3)\n",
    "# total_rewards = np.sum(rewards[0, 0,:])\n",
    "rewards = np.array(agent.rewards)\n",
    "print(rewards.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('length of scores: ', len(rewards), ', len of avg_scores: ', len(avg_scores))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(rewards)+1), rewards, label=\"Rewards\")\n",
    "plt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg on 100 episodes\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1)) \n",
    "plt.ylabel('Total Reward')\n",
    "plt.xlabel('Episodes #')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(agent.epsilon_list)\n",
    "plt.ylabel('Epsilon')\n",
    "plt.xlabel('Episodes #')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BASE_PATH = \"./models/\"\n",
    "# save model\n",
    "model_info = str(learning_rate)+ '_' + str(gamma)\n",
    "path = BASE_PATH + 'model' + '_' + model_info + '.pt'\n",
    "q_name = 'q_approx_state_dict_' + model_info\n",
    "\n",
    "agent.save_q_model(path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "# load model\n",
    "env = gym.make(env_id)\n",
    "\n",
    "\n",
    "\n",
    "len_obs_space = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "\n",
    "epsilon = 0\n",
    "hidden_layer_size = 16\n",
    "layer_sizes = [len_obs_space, hidden_layer_size, n_actions]\n",
    "q_hat = DQN(lr=learning_rate, layer_sizes=layer_sizes, device=device).to(device)\n",
    "\n",
    "# set config file for agent\n",
    "config = {\n",
    "    'lr': learning_rate,\n",
    "    'gamma': gamma,\n",
    "    'epsilon': epsilon,\n",
    "    'action_table': action_table,\n",
    "    't_max': None,\n",
    "    'max_score': None,\n",
    "    'episode_printer': 100,\n",
    "    'env': env,\n",
    "    'q_hat': q_hat,\n",
    "    'batch_size': MINI_BATCH_SIZE,\n",
    "    'exp_mem_size': MEMORY_SIZE,\n",
    "    'TARGET_UPDATE': TARGET_UPDATE,\n",
    "    'epsilon_decay': decay_model\n",
    "}\n",
    "agent = DQNAgent(config)\n",
    "\n",
    "PATH = './models/model_0.001_0.99.pt'\n",
    "agent.load_q_model(PATH)\n",
    "\n",
    "reward_arr = []\n",
    "for i in tqdm(range(100)):\n",
    "    obs, done, rew = env.reset(), False, 0\n",
    "    while not done:\n",
    "        A = agent.get_action(obs)\n",
    "        act = agent.action_table[A.item()]\n",
    "        obs, reward, done, info = env.step(act)\n",
    "        rew += reward\n",
    "        # sleep(0.01)\n",
    "        # env.render()\n",
    "\n",
    "    reward_arr.append(rew)\n",
    "env.close()\n",
    "print(\"average reward per episode :\", sum(reward_arr) / len(reward_arr))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rewards = np.array(reward_arr)\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('reward')\n",
    "print('average reward per episode= {}'.format(np.mean(rewards)))\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "f41a9e18d32d699c7ebd9346171aa8606b8eaf6d2e7d29caa03f22c5e982b824"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}