{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "PACKAGE_PARENT = '../../'\n",
    "sys.path.append(PACKAGE_PARENT)\n",
    "\n",
    "from alphaslime.evaluate.eval_agents import EvaluateGameSA\n",
    "from alphaslime.agents.other.semiGradSarsa import SemiGradSarsa\n",
    "from alphaslime.agents.other.dqnAgent import DQNAgent\n",
    "from alphaslime.approx.linearq import LinearQApprox\n",
    "from alphaslime.approx.dqn import DQN\n",
    "from alphaslime.envgame.slenv import SLenv\n",
    "from alphaslime.agents.baseline import BaselineAgent\n",
    "from alphaslime.epsilon.exp_epsilon import ExponentialDecay\n",
    "from alphaslime.epsilon.linear_epsilon import LinearDecay \n",
    "\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import csv\n",
    "\n",
    "import time\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# config intial properties\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "learning_rate = 0.001\n",
    "EPISODES = 10000\n",
    "MINI_BATCH_SIZE = 64\n",
    "MEMORY_SIZE = 10000\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# env_id = 'CartPole-v1'\n",
    "# env_id = 'CartPole-v0'\n",
    "# action_table = [0, 1]\n",
    "\n",
    "env_id = \"SlimeVolley-v0\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# actions for slimeball\n",
    "action_table = [[0, 0, 0], # NOOP\n",
    "                [1, 0, 0], # LEFT (forward)\n",
    "                [1, 0, 1], # UPLEFT (forward jump)\n",
    "                [0, 0, 1], # UP (jump)\n",
    "                [0, 1, 1], # UPRIGHT (backward jump)\n",
    "                [0, 1, 0]] # RIGHT (backward)\n",
    "\n",
    "config = {}\n",
    "config['t_max'] = 200\n",
    "config['max_score'] = 200\n",
    "config['episode_printer'] = 100\n",
    "config['env'] = None\n",
    "config['action_table'] =  action_table\n",
    "\n",
    "# set opponent agent\n",
    "opponent = BaselineAgent(config)\n",
    "\n",
    "# create multi agent wrapper\n",
    "env = SLenv(env=env, opponent=opponent)\n",
    "\n",
    "\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "len_obs_space = env.observation_space.shape[0]\n",
    "\n",
    "print('n_actions = {}'.format(n_actions))\n",
    "print('len_obs_space = {}'.format(len_obs_space))\n",
    "# q function approximator\n",
    "hidden_layer_size = 64\n",
    "layer_sizes = [len_obs_space, hidden_layer_size, n_actions]\n",
    "q_hat = DQN(lr=learning_rate, layer_sizes=layer_sizes, device=device).to(device)\n",
    "\n",
    "# ***************************************************\n",
    "# set epsilon decay model\n",
    "min_epsilon = 0.2 \n",
    "max_epsilon = 1\n",
    "decay_rate = 0.001\n",
    "eps_decay_exp = ExponentialDecay(min_epsilon, max_epsilon, decay_rate)\n",
    "\n",
    "min_epsilon = 0.01\n",
    "max_episode = 50\n",
    "eps_decay_lin = LinearDecay(min_epsilon, max_episode)\n",
    "\n",
    "decay_model = eps_decay_exp \n",
    "# ***************************************************\n",
    "\n",
    "# set config file for agent\n",
    "config = {\n",
    "    'lr': learning_rate,\n",
    "    'gamma': gamma,\n",
    "    'epsilon': epsilon,\n",
    "    'action_table': action_table,\n",
    "    'env': env,\n",
    "    'q_hat': q_hat,\n",
    "    'batch_size': MINI_BATCH_SIZE,\n",
    "    'exp_mem_size': MEMORY_SIZE,\n",
    "    'TARGET_UPDATE': TARGET_UPDATE,\n",
    "    'epsilon_decay': decay_model\n",
    "}\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "agent = DQNAgent(config)\n",
    "\n",
    "# train agent\n",
    "avg_scores = agent.train(EPISODES, is_progress=True, threshold=3)\n",
    "# total_rewards = np.sum(rewards[0, 0,:])\n",
    "rewards = np.array(agent.rewards)\n",
    "print(rewards.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# do hyper param sweep"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('length of scores: ', len(rewards), ', len of avg_scores: ', len(avg_scores))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(rewards)+1), rewards, label=\"Rewards\")\n",
    "plt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg on 100 episodes\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1)) \n",
    "plt.ylabel('Total Reward')\n",
    "plt.xlabel('Episodes #')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(agent.epsilon_list)\n",
    "plt.ylabel('Epsilon')\n",
    "plt.xlabel('Episodes #')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BASE_PATH = \"./models/\"\n",
    "# save model\n",
    "model_info = str(learning_rate)+ '_' + str(gamma)\n",
    "path = BASE_PATH + 'model' + '_' + model_info + '.pt'\n",
    "q_name = 'q_approx_state_dict_' + model_info\n",
    "\n",
    "agent.save_q_model(path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "# load model\n",
    "env = gym.make(env_id)\n",
    "\n",
    "\n",
    "\n",
    "len_obs_space = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "\n",
    "epsilon = 0\n",
    "hidden_layer_size = 64\n",
    "layer_sizes = [len_obs_space, hidden_layer_size, n_actions]\n",
    "q_hat = DQN(lr=learning_rate, layer_sizes=layer_sizes, device=device).to(device)\n",
    "\n",
    "# set config file for agent\n",
    "config = {\n",
    "    'lr': learning_rate,\n",
    "    'gamma': gamma,\n",
    "    'epsilon': epsilon,\n",
    "    'action_table': action_table,\n",
    "    't_max': None,\n",
    "    'max_score': None,\n",
    "    'episode_printer': 100,\n",
    "    'env': env,\n",
    "    'q_hat': q_hat,\n",
    "    'batch_size': MINI_BATCH_SIZE,\n",
    "    'exp_mem_size': MEMORY_SIZE,\n",
    "    'TARGET_UPDATE': TARGET_UPDATE,\n",
    "    'epsilon_decay': decay_model\n",
    "}\n",
    "agent = DQNAgent(config)\n",
    "\n",
    "PATH = './models/model_0.001_0.99.pt'\n",
    "agent.load_q_model(PATH)\n",
    "\n",
    "reward_arr = []\n",
    "eps = 1\n",
    "for i in tqdm(range(eps)):\n",
    "    obs, done, rew = env.reset(), False, 0\n",
    "    while not done:\n",
    "        A = agent.get_action(obs)\n",
    "        act = agent.action_table[A.item()]\n",
    "        obs, reward, done, info = env.step(act)\n",
    "        rew += reward\n",
    "        # sleep(0.01)\n",
    "        env.render()\n",
    "\n",
    "    reward_arr.append(rew)\n",
    "env.close()\n",
    "print(\"average reward per episode :\", sum(reward_arr) / len(reward_arr))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rewards = np.array(reward_arr)\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('reward')\n",
    "print('average reward per episode= {}'.format(np.mean(rewards)))\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.45s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "average reward per episode : -5.0\n",
      "average reward per episode= -5.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "f41a9e18d32d699c7ebd9346171aa8606b8eaf6d2e7d29caa03f22c5e982b824"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}